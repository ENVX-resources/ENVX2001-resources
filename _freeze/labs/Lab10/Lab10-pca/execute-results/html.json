{
  "hash": "d57a1950206034a3195758bc3e431882",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Lab 10\n# revised-by: alex\nsubtitle: ENVX2001 Applied Statistical Methods\ndescription: The University of Sydney\ntoc: true\nresources:\n  - data/water.xlsx\n---\n\n## Learning Outcomes\n\nIn this lab, we will learn how to:\n\n1.  Perform Principal Component Analysis (PCA) on various datasets\n2.  Interpret the output of PCA and Factor Analysis (FA) -- Loadings, biplot, screeplot, rotations\n\n## Specific goals\n\nBy the end of this lab, you should be able to:\n\n-   [ ] Decide if a dataset is suitable for multivariate analysis\n-   [ ] Perform a PCA in R\n-   [ ] Make a scree plot\n-   [ ] Interpret loadings\n\n## Buckle Up!\n\nWelcome to week 10. We hope you've had fun so far in ENVX2001 :) From now until the end of the course, we will dive into the world of multivariate statistics.\n\nWe've saved the best for last. Out of all the topics in this unit, multivariate statistics might just be the coolest one. In the beginning, you may find some aspects of this topic a little confusing; if that's the case, please approach one of your demonstrators -- they will happily help you out.\n\nBy the end of this lab, you will know how to explore multivariate datasets in a simple and intuitive way. \n\nIn particular, we will learn about a very common multivariate analysis technique: Principle Component Analysis, or PCA.\n\n## Preparation\n\nTo prepare for this lab, please activate the following packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(ggplot2)\n```\n:::\n\n\nAnd download these files: [bumpus_sparrows_clean.csv](data/bumpus_sparrows_clean.csv), [FactBeer.csv](data/FactBeer.csv), [kangaroo.csv](data/kangaroo.csv)\n\nIf you are a fan of learning from textbooks, here are some we recommend:\n\n-   Quinn, G. P. and Keough, M. J. (2002, 2024) *Experimental Design and Data Analysis for Biologists*. Cambridge University Press.\n\n-   Han, S. Y., Filippi, P., Román Dobarco, M., Harianto, J., Crowther M. S., and Bishop, T. F. A. (2023). Multivariate analysis for soil science. In *'Encyclopedia of Soils in the Environment (Second Edition)'*. (Ed. M. J. Goss and M. Oliver) pp. 499-508. Academic Press: Oxford.\n\nIt is also a good idea to have pen and paper (or stylus and ipad) in hand while you go through this lab. One of the best ways to make sense of multivariate statistics is to draw lots of pictures.\n\nReady? Let's get started!\n\n## Part 1: Why PCA? When PCA? Who PCA?\n\nIn the last 9 weeks of ENVX2001, we learned about t-tests, ANOVAs, simple linear regressions, multiple linear regressions, interactions, blocking terms ...\n\nThat's a lot of statistical tools already. Do we really need more?\n\nWell, let's see.\n\n### Exercise: Irises\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/a/af/Iris_versicolor_SCA-0769.jpg)\n:::\n:::\n\n\n*The beautiful* Iris versicolor *, also called the harlequin blueflag, a species of wildflower native to the marshes of North America. From Wikimedia Commons (2016), by R. A. Nonenmacher.*\n\nOne of my personal favourite datasets to explore, for almost any kind of analysis, is the iris dataset, which contains floral measurements from three different species of irises: *Iris versicolour*, *Iris setosa*, and *Iris virginica*.\n\nThis dataset actually has a rather shady past; it was originally published in the 'Annals of Eugenics' by Ronald Fisher, one of the most influential figures in 20th Century Statistics, and a pretty controversial person. [^1]\n\n[^1]: FISHER, R. A. (1936). THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS. *Annals of Eugenics, 7*(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x\n\nFisher invented brilliant statisical concepts such as the F-Distribution (F for Fisher, if you were wondering) and the Analysis of Variance (ANOVA), but he also endorsed eugenics, feuded with his fellow academics, and refused to believe that smoking could increase the risk of lung cancer. [^2]\n\n[^2]: FISHER, R. A. (1958). Cancer and Smoking. *Nature, 182*(4635), 596–596. https://doi.org/10.1038/182596a0\n\nA good example of how complex people can be, I suppose.\n\nFortunately for us, the 'Annals of Eugenics' was brought down a long time ago, but the iris dataset survived as a training tool for statisticians.\n\nThe iris dataset is built into R itself, and conveniently named `iris`. We can summon it like this:\n\n::: {.callout-note collapse=\"true\"}\n#### The iris dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris # It's LONG\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.7         4.4          1.5         0.4     setosa\n17           5.4         3.9          1.3         0.4     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n22           5.1         3.7          1.5         0.4     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n28           5.2         3.5          1.5         0.2     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n32           5.4         3.4          1.5         0.4     setosa\n33           5.2         4.1          1.5         0.1     setosa\n34           5.5         4.2          1.4         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n36           5.0         3.2          1.2         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n41           5.0         3.5          1.3         0.3     setosa\n42           4.5         2.3          1.3         0.3     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n47           5.1         3.8          1.6         0.2     setosa\n48           4.6         3.2          1.4         0.2     setosa\n49           5.3         3.7          1.5         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n55           6.5         2.8          4.6         1.5 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n58           4.9         2.4          3.3         1.0 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n60           5.2         2.7          3.9         1.4 versicolor\n61           5.0         2.0          3.5         1.0 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n65           5.6         2.9          3.6         1.3 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n69           6.2         2.2          4.5         1.5 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n72           6.1         2.8          4.0         1.3 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n79           6.0         2.9          4.5         1.5 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n83           5.8         2.7          3.9         1.2 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n98           6.2         2.9          4.3         1.3 versicolor\n99           5.1         2.5          3.0         1.1 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n101          6.3         3.3          6.0         2.5  virginica\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n106          7.6         3.0          6.6         2.1  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n112          6.4         2.7          5.3         1.9  virginica\n113          6.8         3.0          5.5         2.1  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n116          6.4         3.2          5.3         2.3  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n125          6.7         3.3          5.7         2.1  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n133          6.4         2.8          5.6         2.2  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n141          6.7         3.1          5.6         2.4  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n144          6.8         3.2          5.9         2.3  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\n```\n\n\n:::\n:::\n\n:::\n\n::: callout-note\n#### Building Habits\n\nCheck the structure of the iris dataset using the str() function.\n:::\n\n:::: {.content-visible when-profile=\"solution\"}\n::: callout-tip\n##### Food for thought\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(iris) # All variables are numeric, except for 'Species', which is a factor. \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\nEverything in this dataset is either a number or a factor. Perfect. We are ready for analysis.\n:::\n::::\n\nIt looks like the `iris` dataset contains quite a few variables. In particular, we have four response variables to test: sepal width, sepal length, petal width, and petal length (check out [this page](https://australian.museum/learn/teachers/learning/bugwise/plant2pollinator-support-materials/) to learn more about flower anatomy). All of these are measured in centimeters.\n\nOur first instinct might be to analyse them one at a time. Let's see how that works out:\n\n::: callout-note\n#### Building Habits\n\nChoose one of the four response variables in `iris`, and use `ggplot2` to make a box plot. What do you think? Is there a difference between the three iris species?\n:::\n\n:::: {.content-visible when-profile=\"solution\"}\n::: callout-tip\n##### Food for thought\n\nHere is a box plot for sepal length:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot(colour = 'black', fill = 'lightblue')+\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Lab10-pca_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThere seems to be a difference in sepal length between the three species, but we need to run statistical tests to be sure.\n:::\n::::\n\nDepending on which response variable you chose, you may or may not have seen much of a difference between the three species.\n\nFor example, *Iris setosa* certainly seems to have shorter petals than the other two species:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(iris, aes(x = Species, y = Petal.Length)) +\n  geom_boxplot(colour = 'black', fill = 'lightblue')+\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Lab10-pca_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nLet's run a one-way ANOVA to check.\n\n:::: callout-note\n#### Sharpen your skills\n\nUse a one-way ANOVA to test whether petal length differs between the three iris species.\n\n::: {.callout-tip collapse=\"true\"}\nTo run a one-way ANOVA in R, use the function `aov()`. Remember that petal length is the response variable in this case, and iris species is the explanatory variable.\n\nStructure your data as: `aov(response_variable ~ explanatory_variable, data = iris)`.\n:::\n::::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nHere's what we did:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nANOVA_petal_length <- aov(Petal.Length ~ Species, data =iris) # Runs the test\n\nsummary(ANOVA_petal_length) # Prints the output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nSpecies       2  437.1  218.55    1180 <2e-16 ***\nResiduals   147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nWe found a p-value of \\< 0.001, which means that at least one of the three iris species has a different average petal length to the others.\n:::\n\nSo... case closed? The irises are different, time to move on?\n\nNot so fast. Petal length is just one out of four response variables we could have analysed. What about the other three? Should we run 3 more ANOVAs?\n\nUnfortunately, performing multiple ANOVAs will run us into a series of problems.\n\nFirst, we have the multiple comparisons problem. We won't discuss this topic in detail; but if you want to read up on it, I have linked an article here. [^3]\n\n[^3]: Ranganathan, P., Pramesh, C., & Buyse, M. (2016). Common pitfalls in statistical analysis: The perils of multiple testing. *Perspectives in Clinical Research, 7*(2), 106. https://doi.org/10.4103/2229-3485.179436\n\nSecond, we have the problem of wasting our time. Really. You should be hanging out with your friends instead of running ANOVAs over and over.\n\nRemember the plot for petal length from earlier? Check out this plot for petal width:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(iris, aes(x = Species, y = Petal.Width)) +\n  geom_boxplot(colour = 'black', fill = 'lightblue')+\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Lab10-pca_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLooks... almost the same, right?\n\nThat is because in our dataset, petal length and petal width are **positively correlated** -- the iris species with long petals also tend to have wide petals. In fact, we could really combine petal width and length into a single characteristic: petal size.\n\nBy now, we are used to thinking about correlations between explanatory and response variables -- these are often the \"effects\" we look for in our experiments. We have also learned how to handle correlations between multiple explanatory variables -- recall our labs in weeks 7-9, where we learned about linear regression.\n\nHowever, this time we need to deal with correlations between multiple *response* variables, a situation we have never encountered before.\n\nHow do we do that? Enter multivariate statistics.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/2/2e/City_view%3B%3B_black_and_white.jpg)\n:::\n:::\n\n\n*City view from the Kuala Lumpar Tower, Malaysia. The magic of photographs lie in their ability to project 3D objects onto 2D planes without losing a sense of reality. From Wikimedia Commons (2018), by itsfatxn.*\n\n### Practice: Take a Picture\n\nAll forms of multivariate statistics share a common goal: to distill a large collection of response variables into a small set of useful information.\n\nThis process is called **dimension reduction**. I like to think of it as using mathematics to create a simple, low-dimensional (usually 2D) picture out of a complex, higher-dimensional (usually 4D+) situation.\n\nTo push this analogy further, think of all the different tools and techniques artists use to depict the world around them. As science students, we also have a rich collection of tools with which to handle multivariate data.\n\n**Principle Component Analysis (PCA)** is very much like an artist's camera. Next week, we will learn about **Cluster Analysis**, which works more like a paintbrush. In week 12, we will encounter **Non-metric Multidimensional Scaling (nMDS)**, which I like to think of as the pen-and-ink doodles of the statistical world.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/0/0d/Vintage_cameras_%28Unsplash%29.jpg)\n:::\n:::\n\n\n*A set of vintage cameras. Photography is one of many ways to compress the 3D world into a 2D image. From Wikimedia Commons (2014), by Jeff Sheldon.*\n\nI say PCA is like a camera, because it reduces the dimension of our sample space by directly projecting each sample point onto a lower dimensional plane. In technical terms, this is known as creating a **linear map**.\n\nNot all dimension reduction techniques work this way. Some preserve the **relative distance** or **rank distance** between points instead. We will talk more about these **ordination techniques** in week 12.\n\n::: {.callout-tip collapse=\"true\"}\n#### How come my sample points live in multi-dimensional space?\n\nEach time we take a sample of anything (plants, soil, fur, etc.) we can measure many different characteristics.\n\nIn univariate analysis, we only measure one characteristic; for example, soil pH. This gives us a single value per sample, which we can then plot on a number-line. The corresponding analysis will then be an ANOVA.\n\nIn multivariate analysis, however, we can measure multiple characteristics; for example, soil pH along with soil texture, soil moisture, and soil nutrients. We can't plot these measurements on a single number line anymore -- we need one axis for each measurement.\n\nThis is how your sample points can end up in multi-dimensional space. If you measure 8 response variables from each of your samples, then your data must occupy an 8-dimensional mathematical space.\n\nIn the case of the `iris` dataset, we have 4 response variables. This means our sample points live in 4-dimensional space. We have to find a way to bring them down to 3 or 2-dimensions if we want to plot them nicely.\n:::\n\n::::: callout-note\n#### Sharpen your skills\n\nImport the `kangaroo.csv` dataset using `read.csv()`, and name it `kangaroo`.\n\nCheck the structure of this dataset using `str()`. Compare it to the structure of the `iris` dataset.\n\ni)  How many response variables does `kangaroo` have? Is this more or fewer than `iris`? What dimensional space do the each of these datasets occupy?\n\nii) Recall that petal width and petal length were positively correlated in `iris`. Which response variables in `kangaroo` are positively correlated with each other?\n\n::: {.callout-tip collapse=\"true\"}\nTo check which variables are correlated in a dataset, you can use the `pairs()` function. Try it with the iris dataset first: `pairs(iris)`.\n:::\n\n::: {.callout-note collapse=\"true\"}\nThe column names in `kangaroo` are as follows:\n\n-   BLEN = basillar length\n-   PLEN = palatilar length\n-   OLEN = occipitonasal length\n-   NLEN = nasal length\n-   PWID = palate width\n-   SPECIES = 1: Eastern Grey Kangaroo *(M. giganteus)*, 2: Western Grey Kangaroo *(M. fuliginosus)*\n:::\n:::::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nFirst, we read in the dataset and check its structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkangaroo <- read.csv('data/kangaroo.csv') \nstr(kangaroo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t27 obs. of  6 variables:\n $ BLEN   : int  1439 1315 1294 1377 1470 1612 1388 1575 1717 1587 ...\n $ PLEN   : int  985 895 872 954 987 1119 936 1100 1184 1115 ...\n $ OLEN   : int  1503 1367 1421 1504 1563 1699 1500 1655 1821 1711 ...\n $ NLEN   : int  629 564 606 660 672 778 616 727 810 778 ...\n $ PWID   : int  230 230 239 248 236 281 227 295 307 293 ...\n $ SPECIES: int  1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\ni)  There are 5 response variables here (BLEN, PLEN, OLEN, NLEN, and PWID), and one explanatory variable (SPECIES). In the `iris` dataset, we only had 4 response variables.\n\nThis means the sample points in `kangaroo` occupy 5 dimensional space, while the sample points in `iris` occupy 4 dimensional space.\n\nNow, we can look for correlations between our variables. But we have to be a little bit clever here -- we don't want to include our explanatory variable `SPECIES` in our pairs plot, so we use `[,]` to select only the first 5 columns before applying `pairs()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(kangaroo[,1:5])\n```\n\n::: {.cell-output-display}\n![](Lab10-pca_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nIt looks like just about all of our response variables are positively correlated with each other. This means that kangaroos with longer noses also had longer and wider palattes, etc.\n:::\n\nEarlier, we mentioned that a PCA works by taking a 2D photograph of a multi-dimensional situation. This is only partly true. In reality, the real power of a PCA lies in what it does *before* it takes this photograph.\n\n#### Thought Experiment\n\nPicture a bicycle -- a real, solid bicycle that you can push around and ride. One day, you are commissioned to take a photograph of this bicycle for an information brochure. This brochure is meant to help people understand how a bicycle works by pointing out its various parts. Which angle would you take your photograph from?\n\nThis angle?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/3/31/Roadster_bicycle_%28Dutch%29.jpg)\n:::\n:::\n\n\n*A Roadster (Dutch) bicycle, side view. From Wikimedia Commons (2018), by Petar Milošević.*\n\nOr this angle?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/8/8f/Bicycles_against_wall_in_23_Foster_St_car_park_%28Unsplash%29.jpg)\n:::\n:::\n\n\n*Two bicycles on 23 Foster St, Boston, United States, front and back views. From Wikimedia Commons (2016), by Myles Tan.*\n\nWhen we reduce the number of dimensions in our data, we inevitably **lose information**. Even the best photographs cannot show you all sides of an object. Because of this, we need to be very careful which angle we take our photograph from, so as to lose the least amount of information.\n\nIn the bicycle example, the first angle is the one I would prefer. This is because it shows me **more of the bicycle** than the second. The second picture is lovely, but it would make a poor addition to any information brochure, because most of the bicycle parts are hidden.\n\nThe real power of a PCA is in its ability to **rotate** a multidimensional sample space until it finds the **best angle** from which to take a photograph of your data points. It does this through either one of two mathematical procedures: eigenvalue decomposition, or singular value decomposition.\n\nIn our case, we will use the built-in R function `prcomp()` to perform our PCAs, which is based on singular value decomposition. Do not worry if you are not familiar with singular value decomposition; R will handle that step for you automatically.\n\n## Part 2: How to Perform a PCA Without Panicking\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/e/e4/Beer_in_berlin.jpg)\n:::\n:::\n\n\n*Beer in Berlin. From Wikimedia Commons (2004), by k.ivoutin; Flickr: https://www.flickr.com/photos/ivoutin/1403816845/.*\n\n### Exercise: Go Home, Eigenvalues. You're Drunk.\n\nWhat makes a good beer? That was not my attempt to distract you -- in fact, it was questions like these that first led psychologists and mathematicians to join forces and develop multivariate techniques. [^4]\n\n[^4]: Groenen, P. J. F., Borg, I., Blasius, J., & Greenacre, M. J. (2014). The Past, Present, and Future of Multidimensional Scaling. *Visualization and verbalization of data* (pp. 96–116). Crc Press, Taylor & Francis Group.\n\nPsychologists realised that people made even very simple decisions, such as which restaurants to visit or what clothes to buy, based on a huge number of interacting variables where the weight of any one variable is unclear. [^5]\n\n[^5]: Ikasari, D. M., & Lestari, E. R. (2019). Analysis of fast food restaurant competition based on consumer perception using multidimensional scaling (MDS) (case study in Malang City, East Java, Indonesia). *IOP Conference Series: Earth and Environmental Science, 230*. https://doi.org/10.1088/1755-1315/230/1/012060\n\nFor example, if you could walk out of this lab right now (don't do it) and have a bite to eat, what would you go for? Rice paper rolls? Ramen? Pizza? How many variables might contribute to that decision? Price would be one, for sure. Taste might be another. What about the distance to the nearest Italian restaurant vs the nearest Japanese restaurant?\n\nSome of these variables might turn out to correlate with one another. For example, you may find that people who prefer restaurants that are nearby also tend to prefer restaurants that are affordable. We can combine these two variables together into one: \"accessibility\".\n\nPCA deals with this by creating new set of response variables called principle components, or PCs -- but more on that later.\n\nI'll stop tempting you with food for now. Let's get back to beer instead. Here are seven variables that people normally use to judge the quality of a beer:\n\n::: {.callout-note collapse=\"true\"}\n#### Seven influential variables in a beer\n\n1.  Cost\n2.  Bottle size\n3.  Alcohol content\n4.  Brand reputation\n5.  Colour\n6.  Aroma\n7.  Taste\n:::\n\n220 customers were asked to rate each of these variables from 0 to 100 based on how important they judge that variable to be. For example, a customer who scores \"Cost\" as 0 and \"Brand Reputation\" as 100 cares more about beer clout than financial security; whereas a customer who scores \"Cost\" as 100 and \"Brand Reputation\" as 0 just wants some free booze.\n\nLet's take a look at this dataset.\n\n::: callout-note\n#### Building Habits\n\nRead the dataset `FactBeer.csv` into R and check its structure. Rename the dataset `beer`.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nWe can use the `read.csv()` function to read in our dataset, and the `str()` function to check its structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer <- read.csv(\"data/FactBeer.csv\")\nstr(beer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t220 obs. of  7 variables:\n $ cost   : int  90 75 10 100 20 50 5 65 95 85 ...\n $ size   : int  80 95 15 70 10 100 15 30 95 80 ...\n $ alcohol: int  70 100 20 50 25 100 15 35 100 70 ...\n $ reputat: int  20 50 85 30 35 30 75 80 0 40 ...\n $ color  : int  50 55 40 75 30 90 20 80 80 60 ...\n $ aroma  : int  70 40 30 60 35 75 10 60 70 50 ...\n $ taste  : int  60 65 50 80 45 100 25 90 95 65 ...\n```\n\n\n:::\n:::\n\n\nAll the variables in this dataset are numeric.\n:::\n\n::: callout-note\n#### Building Habits\n\nHow many variables does the `beer` dataset have? Can we treat all of them as response variables?\n:::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nThis might seem like a trick question at first glance, because how can every variable be a response variable? We need an explanatory variable somewhere, don't we?\n\nAs strange as it might seem, we *can* in fact treat all 7 variables in `FactBeer` as response variables, and this is exactly what we will do for this exercise. What this means, however, is that our subsequent analyses will be **descriptive** rather than **deductive/inferential**. \n\nResearchers often use descriptive studies to refine their research questions. Then, they follow up on these questions with more rigorous, inferential tests. Think of descriptive statistics as a special case of Exploratory Data Analysis (EDA), if you will.\n:::\n\n::: callout-warning\n#### How do I know if my study is multivariate?\n\nNo strict rule dictates whether your study should be univariate or multivariate. As a researcher, you are free to choose your response and explanatory variables depending on your research question. Whether you decide to choose multiple response variables or not is up to you.\n\nAs a rule of thumb, whenever you are given a large dataset with more than three numeric variables, be aware that multivariate analysis is an option available to you. \n:::\n\nOur `beer` dataset definitely fits the criteria for multivariate analysis. We will treat all 7 variables in this dataset as response variables, and see if we can spot any patterns in them using a PCA.\n\nTo run the PCA, we use the `prcomp()` function in R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_beer <- prcomp(beer, scale = TRUE)\n```\n:::\n\n\nPCA done -- it's that easy! Everything is over in a flash, just like taking a photograph. \n\nR stores the results of our PCA as a dataset, from which we need to pull out the very important 'rotation' column:\n\n::: {.callout-note collapse=\"true\"}\n#### The 'rotation' column\n\n::: {.cell}\n\n```{.r .cell-code}\nround(pca_beer$rotation,6) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              PC1       PC2      PC3       PC4       PC5       PC6       PC7\ncost    -0.302309 -0.454046 0.084258  0.784602 -0.031082 -0.255689 -0.115793\nsize    -0.366651 -0.417510 0.310547 -0.174129  0.213427  0.469571  0.546301\nalcohol -0.347204 -0.432492 0.086555 -0.578538 -0.275153 -0.364266 -0.376567\nreputat  0.404031  0.043831 0.884035  0.018842 -0.101951 -0.204916  0.024101\ncolor   -0.417815  0.356163 0.307245  0.085150  0.056377  0.487470 -0.596761\naroma   -0.404393  0.379468 0.106137 -0.075887  0.595065 -0.541660  0.167180\ntaste   -0.390233  0.399636 0.042278  0.077508 -0.714212 -0.091785  0.402108\n```\n\n\n:::\n\n```{.r .cell-code}\n# I used the `round()` function to round everything to 6dcp. You don't have to do this.\n```\n:::\n\n:::\n\nWe can then see our results by running `summary()`:\n\n::: {.callout-note collapse=\"true\"}\n#### A summary of our PCA results\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pca_beer) # Tells us how much variance each PC explains.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8201 1.6173 0.75804 0.48978 0.36668 0.29231 0.19206\nProportion of Variance 0.4733 0.3737 0.08209 0.03427 0.01921 0.01221 0.00527\nCumulative Proportion  0.4733 0.8470 0.92905 0.96332 0.98252 0.99473 1.00000\n```\n\n\n:::\n:::\n\n:::\n\nWhat do these numbers mean? We will talk about that in the next section. For now, let's practice running a few more PCAs using the `prcomp()` function to get you comfortable with the process.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/f/f0/Eastern_Grey_kangaroo%2C_Majura_Nature_Reserve_ACT_01.jpg)\n:::\n:::\n\n\n*A young eastern grey kangaroo* (Macropus giganteus) *in Majura Nature Reserve, ACT. From Wikimedia Commons (2016), by Thennicke.*\n\n### Practice: You're Multidimensional (I Think That's a Compliment)\n\nYour turn! Let's bring back the `iris` and `kangaroo` datasets.\n\n::::: callout-note\n#### Sharpen your skills\n\nRe-open the `iris` and `kangaroo` datasets, and check their structure.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nEasy. We named them earlier, so now we can check their structure using the `str()` function:\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(iris) # the iris dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(kangaroo) # the kangaroo dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t27 obs. of  6 variables:\n $ BLEN   : int  1439 1315 1294 1377 1470 1612 1388 1575 1717 1587 ...\n $ PLEN   : int  985 895 872 954 987 1119 936 1100 1184 1115 ...\n $ OLEN   : int  1503 1367 1421 1504 1563 1699 1500 1655 1821 1711 ...\n $ NLEN   : int  629 564 606 660 672 778 616 727 810 778 ...\n $ PWID   : int  230 230 239 248 236 281 227 295 307 293 ...\n $ SPECIES: int  1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n:::\n\nNotice that both `iris` and `kangaroo` have categorical variables (\"Species\" for `iris` and \"SPECIES\" for `kangaroo`) as their last columns. Let's remove these columns before we proceed with our PCA.\n\n::::: callout-note\n#### Sharpen your skills\n\nUse the `[,]` function to remove the last columns of `iris` and `kangaroo` respectively. Then, use the `<-` operator to give these newly-cropped datasets their own names.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nRemember that to filter for columns, we use the space behind the comma `[,*]`:\n\n::: {.cell}\n\n```{.r .cell-code}\niris_cropped <- iris[,1:4] # columns 1 to 4 of the iris dataset\nkangaroo_cropped <- kangaroo[,1:5] # columns 1 to 5 of the kangaroo dataset\n```\n:::\n\n:::\n\nNow go for it! Run a PCA on the `iris` dataset.\n\n::::: callout-note\n#### Sharpen your skills\n\nUse the `prcomp()` function to perform a PCA on the `iris` dataset.\n\n::: {.callout-tip collapse=\"true\"}\nInstead of `scale = TRUE`, you can use `scale = FALSE` for this PCA. You don't have to scale your variables if they are all measured in the same units (in this case, centimeters).\n:::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nOne line of code should do the trick. Remember to use your cropped dataset, not the original `iris`:\n\n::: {.cell}\n\n```{.r .cell-code}\npca_iris <- prcomp(iris_cropped, scale = FALSE) # I used \"<-\" to name it \"pca_iris\".\n```\n:::\n\n:::\n::::\n\nEasy? Summarise your results.\n\n::::: callout-note\n#### Sharpen your skills\n\nSelect the \"rotation\" column from your PCA, and run `summary()` on your results.\n\n::: {.callout-tip collapse=\"true\"}\nRemember that you can select columns by name using the `$` operator.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nThe \"rotation\" column:\n\n::: {.cell}\n\n```{.r .cell-code}\npca_iris$rotation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     PC1         PC2         PC3        PC4\nSepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872\nSepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231\nPetal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390\nPetal.Width   0.35828920  0.07548102 -0.54583143  0.7536574\n```\n\n\n:::\n:::\n\nResults summary:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pca_iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000\n```\n\n\n:::\n:::\n\n:::\n::::\n\nThat's it! You have just performed your first PCA. Do you think you can do the whole thing by yourself without any hints? Give it a try:\n\n::: callout-note\n#### Sharpen your skills\n\nPerform a PCA on the `kangaroo_cropped` dataset, extracting both the \"rotation\" column and the results summary.\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n##### Solution\n\nCheck the structure of `kangaroo_cropped`:\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(kangaroo_cropped) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t27 obs. of  5 variables:\n $ BLEN: int  1439 1315 1294 1377 1470 1612 1388 1575 1717 1587 ...\n $ PLEN: int  985 895 872 954 987 1119 936 1100 1184 1115 ...\n $ OLEN: int  1503 1367 1421 1504 1563 1699 1500 1655 1821 1711 ...\n $ NLEN: int  629 564 606 660 672 778 616 727 810 778 ...\n $ PWID: int  230 230 239 248 236 281 227 295 307 293 ...\n```\n\n\n:::\n:::\n\nWe have 5 response variables and they are all numeric, so we can proceed with multivariate analysis.\n\nPerform the PCA:\n\n::: {.cell}\n\n```{.r .cell-code}\npca_kangaroo <- prcomp(kangaroo_cropped)\n```\n:::\n\n\nExtract the \"rotation\" column:\n\n::: {.cell}\n\n```{.r .cell-code}\npca_kangaroo$rotation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            PC1        PC2         PC3         PC4        PC5\nBLEN 0.58361875 -0.6506053 -0.16256188 -0.03837344 -0.4562927\nPLEN 0.44909213 -0.1860983  0.55797993  0.26346878  0.6188104\nOLEN 0.58327608  0.4672692 -0.34309615 -0.51343366  0.2451927\nNLEN 0.32970748  0.5027396 -0.07404816  0.72415472 -0.3296403\nPWID 0.09374119  0.2664444  0.73418931 -0.37562947 -0.4899883\n```\n\n\n:::\n:::\n\n\nSummarise the results:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pca_kangaroo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                            PC1     PC2      PC3      PC4      PC5\nStandard deviation     295.5783 34.6311 21.40951 16.50136 13.25427\nProportion of Variance   0.9765  0.0134  0.00512  0.00304  0.00196\nCumulative Proportion    0.9765  0.9899  0.99499  0.99804  1.00000\n```\n\n\n:::\n:::\n\n:::\n\nHow did you go?\n\n## Part 3: Admiring Your Handiwork\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/d/d4/French_Boule_Bread.jpg)\n:::\n:::\n\n\n*Boule, a classic French bread. Bakers take a long time to prepare their loaves beforehand, even though the baking process itself is taken care of by their ovens. Bakers must take their loaves out of their ovens at the right time to slice, decorate, and package each loaf nicely for their customers. We should do the same with our PCA results. From Wikimedia Commons (2014), by Lehightrails.*\n\nNow that the technical part is done, let's take some time to admire what we have created. \n\nWe will revisit the results of our `beer` dataset and see what they tell us.\n\n### Exercise: New Year, New Me, New Variables\n\nAs a reminder, here are the results from our beer PCA:\n\n::: {.callout-note collapse=\"true\"}\n#### The \"rotation\" column, rounded to 6dcp\n\n::: {.cell}\n\n```{.r .cell-code}\nround(pca_beer$rotation,6) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              PC1       PC2      PC3       PC4       PC5       PC6       PC7\ncost    -0.302309 -0.454046 0.084258  0.784602 -0.031082 -0.255689 -0.115793\nsize    -0.366651 -0.417510 0.310547 -0.174129  0.213427  0.469571  0.546301\nalcohol -0.347204 -0.432492 0.086555 -0.578538 -0.275153 -0.364266 -0.376567\nreputat  0.404031  0.043831 0.884035  0.018842 -0.101951 -0.204916  0.024101\ncolor   -0.417815  0.356163 0.307245  0.085150  0.056377  0.487470 -0.596761\naroma   -0.404393  0.379468 0.106137 -0.075887  0.595065 -0.541660  0.167180\ntaste   -0.390233  0.399636 0.042278  0.077508 -0.714212 -0.091785  0.402108\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n#### Results Summary\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pca_beer) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8201 1.6173 0.75804 0.48978 0.36668 0.29231 0.19206\nProportion of Variance 0.4733 0.3737 0.08209 0.03427 0.01921 0.01221 0.00527\nCumulative Proportion  0.4733 0.8470 0.92905 0.96332 0.98252 0.99473 1.00000\n```\n\n\n:::\n:::\n\n:::\n\nLet's interpret them one at a time.\n\n#### The Loadings Table\n\n::: callout-note\n##### Building Habits\n\nTake a look at `pca_beer$rotation`. What do you think the numbers stand for?\n:::\n\nThe \"rotation\" column of `pca_beer` is also called its **loadings table**. We will refer to it as such going forward.\n\nThe easiest way to read a loadings table is to consider one column at a time. Each column in this table is a principle component, and the numbers in that column tells you which combination of variables went into creating that component.\n\nYou can think of these principle components as a new set of axes. \n\nRemember that PCAs are good for finding the best angle from which to view multidimensional datasets. The 7 original variables in `FactBeer` (cost, reputation, etc.) represented our original point of view, while the 7 PCs (PC1, PC2, etc.) represent our new point of view.\n\nHowever, these new axes did not come out of nothing. The PCA used combinations from our 7 original variables to create them. \n\nWhich combinations? The loadings table will tell us.\n\n::: callout-note\n##### Building Habits\n\nLook at the fourth column in our loadings table. Which of our 7 original variables went into creating PC4? \n\nAre some number in the column larger than others? What do you think this means?\n:::\n\n::: {.callout-tip collapse=\"true\"}\n###### Solution\n\nAll 7 original variables went into creating PC4. However, some variables contributed more than others.\n\nFor instance, cost and alcohol contributed the most (with loadings of +0.78 and -0.58 respectively), while reputation contributed the least (with a loading of only 0.02).\n\nThis means that PC4 represents a preference for using cost as a way to judge beer quality (+ve loading for cost), and a preference against using alcohol levels to do the same (-ve loading for alcohol), but does not have much to do with reputation either way (~0 loading for reputation).\n:::\n\n##### Giving names to our PCs\n\nFor the moment, our principle components are named: \"PC1, PC2, PC3, etc.\"; but we can give them new names based on our loadings table. \n\nYou can be creative with these names -- as long as they make sense. For example, let's take a look at PC1.\n\n::: callout-note\n##### Building Habits\n\nLook at the first column in our loadings table. Which of our original variables went into creating PC1? Which ones contributed the most? Are their loadings positive or negative?\n:::\n\n::: {.callout-tip collapse=\"true\"}\n###### Solution\n\nReputation contributed positively to PC1 (+0.40), but every other variable contributed negatively. The strength of the contributions are spread quite evenly between the variables.\n:::\n\nHere, we see that PC1 is made up of a positive contribution from 'reputation' combined with negative contributions from the other 6 variables. \n\nThis means that customers who scored high on the PC1 axis thought reputation was an important signal of beer quality (+ve loading), but every other variable was unimportant (-ve loading). On the other hand, customers who scored low on the PC1 axis thought precisely the opposite.\n\nIn other words, PC1 separated customers who valued reputation from customers who valued other factors instead.\n\nTherefore, we can name PC1 something like \"pretentiousness\". \n\n::: callout-note\n##### Building Habits\n\nWhat would you name the other six PCs?\n:::\n\n::: {.callout-tip collapse=\"true\"}\n###### Solution\n\nHere's what I went with for PCs 1 to 4:\n\nPC2 = \"drunk for buck\". This axis separates customers who prefer to judge a beer on its taste, colour, and aroma from those who prefer to get as much alcohol as possible for the least amount of money.\n\nPC3 = \"aesthetic appeal\". This axis separates customers who value the reputation, size, and colour of beer from those who are indifferent to these characteristics. \n\nPC4 = \"financial and moral consciousness\". This axis separates customers who value money from those who value alcohol.\n\nI will leave the rest up to you!\n:::\n\nAt this point, you may have noticed a problem. If the point of a PCA is to reduce the number of variables in our study, why do we still have 7 PCs? \n\nIt seems that while we did successfully rotate our dataset to a new perspective, we are still working in 7D space. \n\nThe good news is that going from 7D space to 2D space is actually very *easy* now that we have our PCs. The photograph has already been taken -- all that is left is to print it out. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/8/8f/The_trail_traverses_the_steep_scree_slope._-_panoramio.jpg)\n:::\n:::\n\n*Stony deposits, or scree, at the base of a mountain. There is a sudden change in slope as solid rock transitions into unconsolidated material. From Wikimedia Commons (2013), by Sergey Pashko.*\n\n#### The Scree Plot\n\nOne very important thing to realise about our PCs is that they are ranked by the amount of variance they explain. In other words, sample points are more spread out along the PC1 axis than along the PC2 axis, and more spread out along the PC2 axis than along the PC3 axis, etc.\n\nThink back to the bicycle pictures from earlier. A bike is more 'spread out' in its side view, with all its features visible. On the other hand, these same features would be squished together in a front view. \n\nPC1 is like the side view of our data, while PC7 is like the front view. Along PC1, we can see more of our data because it is more spread out; along PC7, the same data looks like a congested cloud. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lab10-pca_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n*Our dataset with principle components 'PC1' and 'PC2' as the two viewing axes. Notice how the data is nicely spread out.*\n  \n\n::: {.cell}\n::: {.cell-output-display}\n![](Lab10-pca_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n*The same dataset with principle components 'PC6' and 'PC7' as the two viewing axes. Notice how congested the data becomes.*\n\nIf we remove the PC1 axis from our analysis, we will lose a lot of information. If we remove PC7, however, we will not lose much information at all. \n\nThis is the key to dimension reduction -- getting rid of all the bad photography angles, and keeping only the good ones. \n\nLet's use a scree plot to decide which PCs we should keep, and which ones we should remove.\n\n::: callout-note\n##### Sharpen your skills\n\nTake a look at the results summary of `pca_beer`. You should see a row in this table called \"Proportion of Variance\". \n\nHow much variance do each of the PCs explain? Which ones do you think we can remove without losing too much information?\n:::\n\n::: {.callout-tip collapse=\"true\"}\n###### Solution\n\nTo remind you, here is the summary for `pca_beer` we extracted earlier:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pca_beer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8201 1.6173 0.75804 0.48978 0.36668 0.29231 0.19206\nProportion of Variance 0.4733 0.3737 0.08209 0.03427 0.01921 0.01221 0.00527\nCumulative Proportion  0.4733 0.8470 0.92905 0.96332 0.98252 0.99473 1.00000\n```\n\n\n:::\n:::\n\nPC1 explains around 47% of the variance, while PC2 explains 37%. The two of them explain almost 85% of the variance together. This means that we can remove PCs 3 to 7 and still retain a lot of information about our dataset.\n\nA scree plot will show us the same information in visual form.\n:::\n\nTo make a scree plot, we will use base R instead of ggplot (one of the few instances where I recommend base R over ggplot). The function we will use is called `screeplot()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscreeplot(pca_beer, type = \"lines\")\n```\n\n::: {.cell-output-display}\n![](Lab10-pca_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nSee the big drop-off after PC2? This is called the **elbow** of the scree plot. We can remove every PC after the elbow from our analysis; in this case, PCs 3 to 7.\n\nAnother common protocol for removing PCs is **Kaiser's criterion**, which says to remove every PC lower than 1 on the y-axis of your screeplot. In our case, that still means removing PCs 3 to 7.\n\nIn most situations, I prefer the elbow protocol anyway.\n\nAt this point, we can consider our exploratory PCA more or less complete. Out of 7 original variables, we created 2 new ones. These 2 new variables explain almost as much variation as the original 7, but take up far fewer dimensions. \n\nPretty neat, right?\n\n:::: callout-warning\n#### Other forms of Factor Analysis\n\nPCA is one of many techniques that fall under the broad category of **factor analysis**. Factor analysis, in turn, is a subtopic under multivariate statistics.\n\nFor your interest, here is a different function in R (called `factanal`) that executes a slightly different type of factor analysis using a type of rotation called \"varimax rotation\". (Note: varimax rotation can be applied to PCAs as well, but we chose not to do so.) \n\nThe outputs of this factor analysis are similar to those of a PCA; but instead of creating PCs, `factanal` creates **latent variables** (also called factors). \n\nHere is `factanal` applied to our `beer` dataset. See if you can spot some key differences between the loadings tables of this analysis and the PCA we performed earlier.  \n\n::: {.callout-note collapse=\"true\"}\n##### Factor analysis with Varimax rotation, applied to our beer dataset\n\n::: {.cell}\n\n```{.r .cell-code}\nfa1 <- factanal(beer, 3, rotation=\"varimax\")\nfa1$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n        Factor1 Factor2 Factor3\ncost             0.839   0.141 \nsize             0.992         \nalcohol          0.904   0.255 \nreputat -0.342  -0.348  -0.629 \ncolor    0.994                 \naroma    0.913           0.173 \ntaste    0.913           0.261 \n\n               Factor1 Factor2 Factor3\nSS loadings      2.783   2.640   0.578\nProportion Var   0.398   0.377   0.083\nCumulative Var   0.398   0.775   0.857\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n##### What you might notice\n\nIt seems that the factors from our factor analysis are a little bit easier to interpret than the PCs from our PCA. This is because the varimax rotation constructed each factor so that they are influenced by only a few of the original variables. \n\nRecall that in our PCA, all 7 original variables contributed to PC1 in roughly equal proportions. In our factor analysis, however, Factor 1 is made up mostly of colour, aroma, and taste, with very little contribution from the other four variables.\n\nSometimes this is a good thing, sometimes not. For more on this topic, please see the week 10 lectures.\n:::\n::::\n\n### Practice: Sparrows\n\nWe have come to the end of the lesson, so now it is time to explore a multivariate dataset on your own from start to finish. \n\nYour demonstrators will be there to help you, and so will your classmates. We strongly encourage you to collaborate with your peers on this exercise. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/d/dc/House_Sparrow%28Passer_domesticus%29.jpg)\n:::\n:::\n\n*House Sparrow* (Passer domesticus), *a small bird native to the Northern Hemisphere. House sparrows are so named for their ability to live in urban settings. From Wikimedia Commons (2004), by Mathias Appel.*\n\n#### The sparrow dataset\n\nIn 1898, after a heavy storm, Professor Hermon Bumpus of Brown University came across a collection of house sparrows that had been injured in the storm. [^6] Some of these sparrows were dead, but others had just about survived. \n\nProfessor Bumpus wondered whether he could learn something about natural selection by studying these sparrows. In his study, he used a series of univariate analyses. \n\nYour task is to examine the same dataset using multivariate techniques.\n\n::: callout-note\n#### Case study\n\nThe name of the dataset is \"bumpus_sparrows_clean.csv\". Load this dataset into R, and analyse it using multivariate techniques.\n\nWe will leave it up to you to decide which variables to analyse and how to analyse them. Share your ideas with your classmates, and see what they think.\n:::\n\nThat's all for this week! We hope you had fun :)\n\nThanks!\n\n[^6]: Johnson, R. F., Niles, D. M., & Rohwer, S. A. (1972). Hermon Bumpus and Natural Selection in the House Sparrow Passer domesticus. Evolution, 26(1), 20. https://doi.org/10.2307/2406980\n",
    "supporting": [
      "Lab10-pca_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}