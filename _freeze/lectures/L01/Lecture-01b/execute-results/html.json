{
  "hash": "a6c6323e07ce5a13a091f139baa42564",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Lecture 01b -- Revision\nsubtitle: ENVX2001 Applied Statistical Methods\ndate: last-modified\ndate-format: \"MMM YYYY\"\nauthor: Januar Harianto\nformat:\n  ochre-revealjs: default\n  ochre-typst:\n    fig-format: png\n# format: revealjs\n# fontsize: 16pt\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n# Refresher\n\n## Assumed knowledge\n\n1. Samples, populations and statistical inference\n2. Probability distributions\n3. Parameter estimation\n   - central tendency\n   - spread or variability\n4. Sampling distribution of the mean\n    - Standard error\n    - Confidence intervals\n    - Central Limit Theorem\n\n## Why is this important?\n\n- Understanding sampling **informs experimental design (Week 4 onward)**. *How many samples do we need and are our samples representative?*\n- Recognising sample (*not sampling*) distributions helps us choose the right statistical model -- e.g. *t*-test to compare two means that are normally distributed.\n- Most statistical techniques use sample statistics for interpretation, e.g. the *t*-test can be explained using **confidence intervals**, and the ANOVA test can be interpreted in part using **means** and **standard errors**.\n\nAll of these concepts will make more sense as we go through the course, but if you do not try to understand them now, you **will** struggle.\n\n# Samples, populations and statistical inference\n\n## Populations and samples\n\n### Populations\n- **All** the possible units and their associated observations of interest\n- Scientists are often interested in making *inferences* about populations, but measuring every unit is impractical\n\n### Samples\n- A collection of observations from any population is a sample, and the number of observations in it is the **sample size**\n- We assume samples that we collect can be used to make inferences about the population\n- **NEW**: Samples need to be *representative* of the population\n\n\n\n## Statistics vs parameters\n\n- Characteristics of the **population** are called *parameters* (e.g. population mean or population regression slope)\n- Characteristics of the **sample** are called *statistics* (e.g. sample mean or sample regression slope) -- they are used to estimate the population parameters\n- Statistics are what we use to help us understand the population\n- Formal statistical methods can help us make inferences about the population based on the sample -- statistical inference\n- *Not all statistical techniques are inferential, but many are*\n\n\n\n## Sample data\n\nSample data are usually collected as **variables**, which are the characteristics we measure or record from each object.\n\n::: fragment\n\nVariables can be:\n\n### Categorical Variables\n- **Nominal**: categories without a natural order (e.g. colors, names)\n- **Ordinal**: categories with a natural order (e.g. ratings, rankings)\n\n### Numerical Variables\n- **Continuous**: can take any value within a range (e.g. height, weight)\n- **Discrete**: can take only specific values (e.g. counts, presence/absence)\n:::\n\n## YOU decide on what a variable represents\n\nA numerical, continuous variable can be treated as a categorical variable if you decide to categorise it.\n\n### Examples\n::: incremental\n- **height (in cm)** -- a numerical, **continuous** variable, can be treated as a categorical variable if you group it into categories (short, medium, tall)\n- **age (in years)** -- a numerical, **discrete** variable, can be treated as a continuous variable (if we allow for certain *issues*)\n- **treatment (A, B, C)** -- a categorical variable, can be treated as a numerical variable if we assign numbers to the treatments (1, 2, 3) and assume they are **ordered** e.g. effect of 1 < 2 < 3 -- *the basis of non-parametric tests*\n:::\n\n\n# Distribution of data\n## Types of probability distributions\nPopulations can be described by probability distributions, and by now, you should be familiar with these distributions and their properties\n\n::: incremental\n- **Normal Distribution**: Bell-shaped curve, symmetric around the mean. **Data is continuous**\n- **Binomial Distribution**: Models success/failure outcomes in a fixed number of trials. **Data is discrete**\n- **Poisson Distribution**: Models count data when events occur at a constant rate. **Data is discrete**\n:::\n::: fragment\nKnowing the distribution of your data is important for choosing the right statistical model -- although it is not always necessary.\n:::\n\n## \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nset.seed(908)\nnormal_data <- data.frame(x = rnorm(10000, mean = 0, sd = 1))\nbinomial_data <- data.frame(x = rbinom(10000, size = 10, prob = 0.5))\npoisson_data <- data.frame(x = rpois(500, lambda = 3))\n\n# normal\np1 <- ggplot(normal_data, aes(x = x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  ggtitle(\"Normal Distribution\")\n\n# binomial\np2 <- ggplot(binomial_data, aes(x = x)) +\n  geom_bar(fill = \"lightgreen\", color = \"black\") +\n  ggtitle(\"Binomial Distribution\")\n\n# poisson\np3 <- ggplot(poisson_data, aes(x = x)) +\n  geom_bar(fill = \"lightpink\", color = \"black\") +\n  ggtitle(\"Poisson Distribution\")\n\n# Arrange plots\nlibrary(patchwork)\np1 | p2 / p3\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n# Parameter estimation\n\n## Measures of central tendency\n\n\n### Mean $\\bar{x}$\n- The arithmetic average of all values in a dataset\n- Sum of all values divided by number of observations\n- Sensitive to extreme values (outliers)\n\n::: fragment\n::: callout-warning\n## Formula\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n\n- where $n$ is the number of observations\n- $x_i$ represents each individual value\n- $\\sum$ means we add up all values from $i=1$ to $n$\n- Example: for data {2,4,6,8}, $n=4$ and $\\bar{x} = \\frac{2+4+6+8}{4} = 5$\n:::\n:::\n\n## \n\n\n### Median\n- Middle value when data is ordered\n- 50th percentile of the data\n- More robust to outliers than mean\n- For even n, average of two middle values\n\n\n::: fragment\n### Mode\n- Most frequently occurring value\n- Can have multiple modes\n- Only measure of central tendency for categorical data\n- Not always meaningful for continuous data\n:::\n\n## How they compare\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/central-tendency-1.png){width=960}\n:::\n:::\n\n\nDepending on the distribution of the data, the mean and median can be different, and this can tell you something about the data.\n\n\n\n## Measures of dispersion\n\n### Variance $s^2$\n- Measures how spread out the data is from the mean\n- Calculated as average squared deviations from the mean\n- Squared units make it harder to interpret\n- Sensitive to outliers (squares large deviations)\n\n## Measures of dispersion\n\n::: callout-warning\n## Formula\n$$s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2$$\n\n- where $n$ is the number of observations\n- $x_i$ represents each individual value\n- $\\bar{x}$ is the sample mean\n- For data {2,4,6,8}: \n  - mean = 5 \n  - differences = (-3,-1,1,3), squares = (9,1,1,9), sum = 20\n  - Therefore, $s^2 = \\frac{20}{3} \\approx 6.67$\n:::\n\n## Measures of dispersion\n\n### Standard Deviation $s$\n- Square root of variance\n- Same units as original data\n- More interpretable than variance\n- Empirical rule for normal distributions:\n  - ≈68% of data within ±1 SD\n  - ≈95% of data within ±2 SD\n  - ≈99.7% of data within ±3 SD\n\n::: fragment\n::: callout-warning\n## Formula\n$$s = \\sqrt{s^2}$$\n\n- Simply the square root of variance\n- For our example data {2,4,6,8}:\n  - $s = \\sqrt{6.67} \\approx 2.58$\n- Interpretation: On average, values deviate about 2.58 units from the mean\n:::\n:::\n\n## Visualising standard deviation\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/dispersion-1.png){width=960}\n:::\n:::\n\n\n\n\n## Population parameters vs sample statistics\n\nFor those of you interested:\n\n### Mean\n| Population Parameter | Sample Statistic |\n|---------------------|------------------|\n| $\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i$ | $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ |\n\n### Variance\n| Population Parameter | Sample Statistic |\n|---------------------|------------------|\n| $\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2$ | $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2$ |\n\n### Standard Deviation\n| Population Parameter | Sample Statistic |\n|---------------------|------------------|\n| $\\sigma = \\sqrt{\\sigma^2}$ | $s = \\sqrt{s^2}$ |\n\n::: {.callout-note}\nNotice the use of $n-1$ in sample variance and standard deviation\n:::\n\n## Why n-1?\n- When calculating sample variance, we use $n-1$ instead of $n$ in the denominator\n- This is called \"[Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction)\"\n- Why? Because we lose one \"degree of freedom\" when we estimate the mean:\n  1. If you know the sample mean ($\\bar{x}$)\n  2. And you know all but one value in your sample\n  3. The last value is *constrained* - it must make the mean equal $\\bar{x}$\n\n# Sampling distributions and CLT\n\n## What is a sampling distribution?\n- Distribution of a statistic (e.g., mean) calculated from repeated samples\n- Shows how sample statistics vary from sample to sample\n- Important for understanding sampling variability and making inferences\n\n## Sampling distribution of the mean\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Central Limit Theorem\n\n> I know of scarcely anything so apt to impress the imagination as the wonderful form of **cosmic order** expressed by the Central Limit Theorem. The law would have been personified by the Greeks and deified, if they had known of it.”\n\n– Sir Francis Galton, 1889, Natural Inheritance\n\n\nThe Central Limit Theorem (CLT) states that for sufficiently large samples:\n\n1. The sampling distribution of the mean follows a normal distribution\n2. The mean of the sampling distribution equals the population mean\n3. The standard deviation of the sampling distribution (standard error) = $\\frac{\\sigma}{\\sqrt{n}}$\n\n\n## CLT in action\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a skewed population\nset.seed(456)\nskewed_pop <- exp(rnorm(10000, mean = 0, sd = 0.5))\n\n# Sample means for different sample sizes (ordered small to large)\nsample_sizes <- c(5, 30, 100)\nsample_labels <- factor(paste(\"n =\", sample_sizes),\n  levels = paste(\"n =\", sample_sizes)\n) # preserve order\nsample_dist_data <- lapply(sample_sizes, function(n) {\n  means <- replicate(1000, mean(sample(skewed_pop, size = n)))\n  data.frame(means = means, size = factor(paste(\"n =\", n), levels = levels(sample_labels)))\n})\nsample_dist_df <- do.call(rbind, sample_dist_data)\n\n# Plot\nggplot() +\n  geom_histogram(aes(x = means, y = ..density..),\n    data = sample_dist_df,\n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  geom_density(aes(x = means), data = sample_dist_df, color = \"blue\") +\n  facet_wrap(~size, scales = \"free_x\") +\n  ggtitle(\"Sampling distributions for different sample sizes\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Example\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(239)\n# Generate a skewed distribution\nskewed <- tibble(\n  x = rgamma(1000, shape = 2, scale = 1)\n)\n\n# plot in ggplot2\nggplot(data = skewed, aes(x = x)) +\n  geom_histogram(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlab(\"Height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n- Skewed population distribution for tree heights.\n- We want to estimate the mean height of the trees in the forest.\n\n## 1 sample (no summary statistic)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewed |>\n  infer::rep_sample_n(\n    size = 1,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\nWith only one sample, we are not really seeing a sampling distribution -- we are just replicating the same population distribution. A sampling distribution emerges when we take multiple samples and calculate their means.\n\n## 2 samples\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewed |>\n  infer::rep_sample_n(\n    size = 2,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n- We sample 2 trees and calculate the mean height, and repeat this 1000 times.\n- The distribution of sample means is starting to look more like a normal distribution.\n\n\n## 5 samples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewed |>\n  infer::rep_sample_n(\n    size = 5,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n- Five random samples per calculated mean, repeated 1000 times.\n- The distribution is becoming more normal, and the spread is decreasing: estimate is getting more **precise**.\n\n## 30 samples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewed |>\n  infer::rep_sample_n(\n    size = 30,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n- Thirty random samples per calculated mean, repeated 1000 times.\n- The distribution of sample means is very close to a normal distribution.\n\n## 50 samples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewed |>\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n- Fifty random samples per calculated mean, repeated 1000 times.\n- **How many samples is enough?**\n\n## Are 50 samples \"normal\" enough?\n\n::: {.cell}\n\n```{.r .cell-code}\nskewed |>\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = 2, # population mean for gamma(2,1)\n      sd = sqrt(2) / sqrt(50) # theoretical SE for gamma(2,1)\n    ),\n    linewidth = 1,\n    color = \"blue\",\n    linetype = \"dashed\"\n  ) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n- Fifty random samples per calculated mean, repeated 1000 times.\n- **How many samples is enough?**\n\n## Effect of sample size\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidymodels)\nlibrary(patchwork)\nset.seed(642)\n\nheights <- tibble(heights = rnorm(1000, 1.99, 1))\npopmean <- mean(heights$heights)\nsample_sizes <- c(2, 5, 25, 100)\nn <- length(sample_sizes)\n\nheights <- tibble(heights = rgamma(1000, shape = 2, scale = 1))\nsample_sizes <- c(2, 5, 25, 100)\nn <- length(sample_sizes)\n\nplots <- lapply(sample_sizes, function(size) {\n  df <- heights |>\n    rep_sample_n(size = size, reps = 2000) |>\n    group_by(replicate) |>\n    summarise(xbar = mean(heights))\n\n  mean_xbar <- mean(df$xbar)\n\n  ggplot(df, aes(x = xbar)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    geom_vline(aes(xintercept = mean_xbar), color = \"blue\", linetype = \"dashed\") +\n    geom_text(aes(x = mean_xbar, label = sprintf(\"%.2f\", mean_xbar), y = Inf), hjust = -0.1, vjust = 2, color = \"blue\") +\n    ggtitle(paste0(\"Sample Size: \", size)) +\n    xlab(\"Mean height (m)\") +\n    xlim(-3, 8)\n})\nwrap_plots(plots)\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\nIncreased sample size leads to a more accurate estimate of the population mean, reflected by the **narrower distribution** of the sample mean, which is captured by the **standard error**.\n\n## Effect of variability\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1221)\n\n# Define a function to generate ggplot objects\ngenerate_plot <- function(sd) {\n  data <- rnorm(500, 1.99, sd)\n  p <- ggplot(data = tibble(x = data), aes(x = x)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    ggtitle(paste(\"SD =\", sd)) +\n    xlim(-100, 100)\n  return(p)\n}\n\n# Apply the function to a list of standard deviations\nsds <- c(3, 6, 15, 25)\nplots <- lapply(sds, generate_plot)\n\n# Wrap the plots\nwrap_plots(plots)\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\nIncreased variability (i.e. wide range of tree heights) leads to a wider distribution of the sample mean (i.e. less precision), which is *also* reflected by the **standard error**.\n\n## CLT drives statistical inference\n\nBecause of how predictable the CLT applies to sample means, we can use this to make reasonably accurate inferences about the population mean, even if we do *not* know the population distribution.\n\n- A sampling distribution of the mean *will* be normally distributed for sufficiently large samples -- how large is \"sufficient\" depends on the population distribution\n- The mean of the sampling distribution trends towards the population mean with increasing sample size\n- To determine how well the sample mean estimates the population mean, we use the standard error of the mean -- basically a standard deviation of the sampling distribution\n\n# Standard error and confidence intervals\n\n## Standard Error of the Mean\n- Measures the precision of a sample mean\n- Describes variation in sample means -- around the true population mean\n- Decreases as sample size increases, because we become more \"confident\" in our estimate\n\n::: fragment\n::: callout-warning\n## Formula\n$$SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}$$\n\n- where $s$ is the sample standard deviation\n- $n$ is the sample size\n:::\n:::\n\n\n## When to report SD or SE\n### Standard Deviation (SD)\n- Describes variability in your **data**\n- **Stays constant regardless of sample size**\n\n### Standard Error (SE)\n- Describes precision of your **mean estimate**\n- **Decreases with larger sample size ($SE = \\frac{SD}{\\sqrt{n}}$)**\n\nWhen reporting statistics:\n\n- Use **mean ± SE** to show precision of your estimate\n- Use **mean ± SD** to show spread of your raw data\n- SE can appear deceptively small with large sample sizes -- **always report sample size**!\n\n## Confidence intervals\n### What is a confidence interval?\n- Range of values likely to contain the true population parameter\n- Level of confidence (usually 95%) indicates reliability\n- Wider intervals = less precise estimates\n\n::: fragment\n::: callout-warning\n## Formula for 95% CI\n$$\\bar{x} \\pm (t_{n-1} \\times SE_{\\bar{x}})$$\n\n:::\n:::\n\n## Visualising confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate sample data\nset.seed(253)\nsample_data <- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 30),\n  value = c(\n    rnorm(30, 100, 15),\n    rnorm(30, 110, 15),\n    rnorm(30, 105, 15)\n  )\n)\n\n# Calculate means and CIs\nci_data <- sample_data %>%\n  group_by(group) %>%\n  summarise(\n    mean = mean(value),\n    se = sd(value) / sqrt(n()),\n    ci_lower = mean - qt(0.975, n() - 1) * se,\n    ci_upper = mean + qt(0.975, n() - 1) * se\n  )\n\n# Plot\nggplot(ci_data, aes(x = group, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +\n  ggtitle(\"Means with 95% Confidence Intervals\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\nWe will learn more about confidence intervals in the next lecture.\n\n# Thanks for listening! Questions?\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by]\n",
    "supporting": [
      "Lecture-01b_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}