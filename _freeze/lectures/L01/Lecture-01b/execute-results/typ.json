{
  "hash": "12a342bf6319eebd4ec00516e3ae7fac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Lecture 01b -- The fundamentals\nsubtitle: ENVX2001 Applied Statistical Methods\ndate: last-modified\ndate-format: \"MMM YYYY\"\nauthor: Januar Harianto\nformat:\n  ochre-revealjs: default\n  ochre-typst:\n    fig-format: png\n# format: revealjs\n# fontsize: 16pt\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n# A refresher (for most)\n\n## You should be familiar with the following concepts:\n\n1. Populations, samples and statistical inference\n2. Probability distributions\n3. Parameter estimation: central tendency, spread and variability\n4. Sampling distribution of the mean: standard error, confidence intervals\n5. Central Limit Theorem\n\n**That's it!**\n\n## Why review these concepts?\n\n- These are concepts taught in first year statistics courses which we consider to be fundamental\n- Even if you come from a unit/course that does not do R, **you should at least be familiar with the basic building blocks of statistical thinking**\n\n::: {.fragment}\nLet's go through these concepts together!\n:::\n\n# Samples, populations\n\n## Populations\n\n- **All** the possible units and their associated observations of interest\n- Scientists are often interested in making *inferences* about populations, but measuring every unit is impractical\n\n### Samples\n- A collection of observations from any population is a sample, and the number of observations in it is the **sample size**\n- We assume samples that we collect can be used to make inferences about the population\n- **NEW**: Samples need to be *representative* of the population\n\n\n\n## Statistics vs parameters\n\n- Characteristics of the **population** are called *parameters* (e.g. the true average height of all trees in a forest)\n- Characteristics of the **sample** are called *statistics* (e.g. the average height of 30 trees that we measured)\n- In practice, we usually don't know the true population parameters, so we use sample statistics as our best estimates\n- Drawing conclusions about a population from sample data is called **statistical inference**\n\n::: fragment\n*Not all statistical techniques are inferential -- some are purely descriptive (e.g. summarising data with graphs or tables) -- but inference is the main focus of this course.*\n:::\n\n\n\n## Sample data\n\nSample data are usually collected as **variables** -- the characteristics we measure or record from each unit in our sample.\n\n::: fragment\n\nVariables can be:\n\n### Categorical Variables\n- **Nominal**: categories without a natural order (e.g. soil type, land use)\n- **Ordinal**: categories with a natural order (e.g. soil quality: poor, fair, good)\n\n### Numerical Variables\n- **Continuous**: can take any value within a range (e.g. height, temperature)\n- **Discrete**: can take only specific values (e.g. number of species, presence/absence)\n:::\n\n## Variable types can overlap\n\nThe categories on the previous slide are useful guidelines, but in practice, variables can cross between types depending on how you use them.\n\n### Examples\n::: incremental\n- **height (in cm)** -- a numerical, **continuous** variable, but can be treated as categorical if you group it into categories (short, medium, tall)\n- **age (in years)** -- a numerical, **discrete** variable, but often treated as continuous because the gaps between values are small relative to the range\n- **treatment (A, B, C)** -- a categorical variable, but can be treated as numerical if we assign numbers to the treatments (1, 2, 3) and assume they are **ordered** e.g. effect of 1 < 2 < 3\n:::\n\n## Be careful when converting variables\n\nConverting a numerical variable to a categorical one means **losing information** -- once you group heights into \"short\", \"medium\" and \"tall\", the exact measurements are gone.\n\n::: incremental\n- A tree that is 5.1 m and a tree that is 9.9 m might both be called \"tall\", but they are very different\n- Only categorise when there is good reason to do so, as this reduces your analysis power\n- Going the other way (categorical → numerical) doesn't lose data, but it does **add assumptions** -- e.g. that your categories have a meaningful order\n:::\n\n\n# Distribution of data\n## Types of probability distributions\nPopulations can be described by probability distributions. You may have encountered these common ones in first-year statistics:\n\n::: incremental\n- **Normal Distribution**: Bell-shaped curve, symmetric around the mean. **Data is continuous** (e.g. heights, weights)\n- **Binomial Distribution**: Models success/failure outcomes in a fixed number of trials. **Data is discrete** (e.g. how many seeds germinate out of 10 planted)\n- **Poisson Distribution**: Models count data over a fixed area or time period. **Data is discrete** (e.g. number of weeds found in a 1 m² quadrat)\n:::\n::: fragment\nKnowing the distribution of your data helps you choose the right statistical model. We will return to this throughout the course.\n:::\n\n## \n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\n\nset.seed(908)\nnormal_data <- data.frame(x = rnorm(10000, mean = 0, sd = 1))\nbinomial_data <- data.frame(x = rbinom(10000, size = 10, prob = 0.5))\npoisson_data <- data.frame(x = rpois(500, lambda = 3))\n\n# normal\np1 <- ggplot(normal_data, aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  ggtitle(\"Normal Distribution\")\n\n# binomial\np2 <- ggplot(binomial_data, aes(x = x)) +\n  geom_bar(fill = \"lightgreen\", color = \"black\") +\n  ggtitle(\"Binomial Distribution\")\n\n# poisson\np3 <- ggplot(poisson_data, aes(x = x)) +\n  geom_bar(fill = \"lightpink\", color = \"black\") +\n  ggtitle(\"Poisson Distribution\")\n\n# Arrange plots\nlibrary(patchwork)\np1 | p2 / p3\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-1-1.png)\n:::\n:::\n\n\n# Parameter estimation\n\n## Measures of central tendency\n\nCentral tendency tells you where the \"middle\" or \"typical\" value of your data falls. There are several ways to measure it, each with trade-offs.\n\n### Mean $\\bar{x}$\n- The sum of all values divided by the number of observations -- what `mean()` does in R\n- Sensitive to extreme values (outliers)\n\n::: fragment\n::: callout-warning\n## Formula\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n\n- where $n$ is the number of observations\n- $x_i$ represents each individual value\n- $\\sum$ means we add up all values from $i=1$ to $n$\n- Example: for data {2,4,6,8}, $n=4$ and $\\bar{x} = \\frac{2+4+6+8}{4} = 5$\n:::\n:::\n\n## Median and mode\n\n### Median\n- Sort your data from smallest to largest and take the middle value\n- The median is resistant to extreme values, making it a better representation of typical observations when data is skewed\n\n::: fragment\n### Mode\n- The most frequently occurring value in a dataset\n- Can have multiple modes (e.g. a bimodal distribution has two peaks)\n- The only measure of central tendency that works for categorical data\n:::\n\n## How they compare\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/central-tendency-1.png)\n:::\n:::\n\n\nWhen the mean and median are close, your data is roughly symmetric. When they differ substantially, the data is skewed -- the mean shifts toward extreme values.\n\n\n\n## Measures of dispersion\n\nDispersion measures how spread out your data is. Two datasets can have identical means but very different spreads.\n\n### Variance $s^2$\n- The average squared distance of each value from the mean\n- Since the result is in squared units (e.g. m²), it's hard to interpret directly -- we usually take the square root to get standard deviation\n\n## Measures of dispersion\n\n::: callout-warning\n## Formula\n$$s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2$$\n\n- where $n$ is the number of observations\n- $x_i$ represents each individual value\n- $\\bar{x}$ is the sample mean\n- For data {2,4,6,8}: \n  - mean = 5 \n  - differences = (-3,-1,1,3), squares = (9,1,1,9), sum = 20\n  - Therefore, $s^2 = \\frac{20}{3} \\approx 6.67$\n:::\n\n## Measures of dispersion\n\n### Standard Deviation $s$\n- The square root of variance -- expressed in your data's original units (e.g. metres, not metres²)\n- Shows how far a typical observation is from the mean\n\n::: fragment\n::: callout-warning\n## Formula\n$$s = \\sqrt{s^2}$$\n\n- For our example data {2,4,6,8}: $s = \\sqrt{6.67} \\approx 2.58$\n:::\n:::\n\n## Visualising standard deviation\n\nFor normally distributed data, standard deviation tells you what proportion of values fall within each range of the mean:\n\n- about **68%** of values fall within ±1 SD\n- about **95%** within ±2 SD\n- about **99.7%** within ±3 SD\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/dispersion-1.png)\n:::\n:::\n\n\n\n\n## Population parameters vs sample statistics\n\nThroughout the course you'll see Greek letters ($\\mu$, $\\sigma$) for population parameters and Roman letters ($\\bar{x}$, $s$) for sample statistics:\n\n### Mean\n| Population Parameter | Sample Statistic |\n|---------------------|------------------|\n| $\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i$ | $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ |\n\n### Variance\n| Population Parameter | Sample Statistic |\n|---------------------|------------------|\n| $\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2$ | $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2$ |\n\n### Standard Deviation\n| Population Parameter | Sample Statistic |\n|---------------------|------------------|\n| $\\sigma = \\sqrt{\\sigma^2}$ | $s = \\sqrt{s^2}$ |\n\n::: {.callout-note}\nNotice the use of $n-1$ in sample variance and standard deviation\n:::\n\n## Why n-1?\n- Sample variance uses $n-1$ instead of $n$ in the denominator\n- This correction ([Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction)) accounts for bias when estimating the mean from the sample itself\n\n::: fragment\n- Here's the logic: once you know the mean and all but one value, the last value is determined -- it must satisfy the mean. Only $n-1$ values are truly free to vary\n- These independent values are your **degrees of freedom**\n:::\n\n# Sampling distributions and CLT\n\n## What is a sampling distribution?\n\nImagine you collected your sample of 30 trees, calculated the mean height, then did it again -- 1000 times. Each time, you'd get a slightly different mean.\n\n::: fragment\n- The distribution of all those means is a **sampling distribution**\n- It shows how much your estimate (the sample mean) varies due to sampling randomness\n- This foundation of statistical inference lets you quantify confidence in your estimates\n:::\n\n## Sampling distribution of the mean\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-2-1.png)\n:::\n:::\n\n\n## Central Limit Theorem\n\n> I know of scarcely anything so apt to impress the imagination as the wonderful form of **cosmic order** expressed by the Central Limit Theorem. The law would have been personified by the Greeks and deified, if they had known of it.”\n\n– Sir Francis Galton, 1889, Natural Inheritance\n\n\nThe Central Limit Theorem (CLT) states that for sufficiently large samples:\n\n1. The distribution of sample means becomes **approximately normal**, regardless of the population's shape\n2. The average of those sample means equals the **true population mean**\n3. The spread of the sampling distribution decreases as sample size increases: $SE = \\frac{\\sigma}{\\sqrt{n}}$\n\n::: {.notes}\nIn short: with larger samples, their means form a predictable, bell-shaped pattern -- even if the original data is skewed.\n:::\n\n\n## CLT in action\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create a skewed population\nset.seed(456)\nskewed_pop <- exp(rnorm(10000, mean = 0, sd = 0.5))\n\n# Sample means for different sample sizes (ordered small to large)\nsample_sizes <- c(5, 30, 100)\nsample_labels <- factor(paste(\"n =\", sample_sizes),\n  levels = paste(\"n =\", sample_sizes)\n) # preserve order\nsample_dist_data <- lapply(sample_sizes, function(n) {\n  means <- replicate(1000, mean(sample(skewed_pop, size = n)))\n  data.frame(means = means, size = factor(paste(\"n =\", n), levels = levels(sample_labels)))\n})\nsample_dist_df <- do.call(rbind, sample_dist_data)\n\n# Plot\nggplot() +\n  geom_histogram(aes(x = means, y = after_stat(density)),\n    data = sample_dist_df,\n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  geom_density(aes(x = means), data = sample_dist_df, color = \"blue\") +\n  facet_wrap(~size, scales = \"free_x\") +\n  ggtitle(\"Sampling distributions for different sample sizes\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-3-1.png)\n:::\n:::\n\n\n## Example\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(239)\n# Generate a skewed distribution\nskewed <- tibble(\n  x = rgamma(1000, shape = 2, scale = 1)\n)\n\n# plot in ggplot2\nggplot(data = skewed, aes(x = x)) +\n  geom_histogram(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlab(\"Height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-4-1.png)\n:::\n:::\n\n\n- Skewed population distribution for tree heights.\n- We want to estimate the mean height of the trees in the forest.\n\n## Sample size: n = 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nskewed |>\n  infer::rep_sample_n(\n    size = 1,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-5-1.png)\n:::\n:::\n\n\nWith a sample size of one, the sampling distribution mirrors the population -- larger samples are needed to see convergence to normality.\n\n## Sample size: n = 2\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nskewed |>\n  infer::rep_sample_n(\n    size = 2,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-6-1.png)\n:::\n:::\n\n\n- We sample 2 trees, calculate the sample mean, and repeat 1000 times.\n- The distribution of sample means is starting to look more like a normal distribution.\n\n\n## Sample size: n = 5\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nskewed |>\n  infer::rep_sample_n(\n    size = 5,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-7-1.png)\n:::\n:::\n\n\n- Each sample mean is based on 5 observations, repeated 1000 times.\n- The distribution is becoming more normal, and the spread is decreasing: estimate is getting more **precise**.\n\n## Sample size: n = 30\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nskewed |>\n  infer::rep_sample_n(\n    size = 30,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-8-1.png)\n:::\n:::\n\n\n- Each sample mean is based on 30 observations, repeated 1000 times.\n- The distribution of sample means is very close to a normal distribution.\n\n## Sample size: n = 50\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nskewed |>\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-9-1.png)\n:::\n:::\n\n\n- Each sample mean is based on 50 observations, repeated 1000 times.\n- **How many samples is enough?**\n\n## Is n = 50 \"normal\" enough?\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nskewed |>\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |>\n  group_by(replicate) |>\n  summarise(xbar = mean(x)) |>\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = 2, # population mean for gamma(2,1)\n      sd = sqrt(2) / sqrt(50) # theoretical SE for gamma(2,1)\n    ),\n    linewidth = 1,\n    color = \"blue\",\n    linetype = \"dashed\"\n  ) +\n  xlab(\"Mean height (m)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-10-1.png)\n:::\n:::\n\n\n- Each sample mean is based on 50 observations, repeated 1000 times.\n- **How many samples is enough?**\n\n## Effect of sample size\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidymodels)\nlibrary(patchwork)\nset.seed(642)\n\nheights <- tibble(heights = rnorm(1000, 1.99, 1))\npopmean <- mean(heights$heights)\nsample_sizes <- c(2, 5, 25, 100)\nn <- length(sample_sizes)\n\nheights <- tibble(heights = rgamma(1000, shape = 2, scale = 1))\nsample_sizes <- c(2, 5, 25, 100)\nn <- length(sample_sizes)\n\nplots <- lapply(sample_sizes, function(size) {\n  df <- heights |>\n    rep_sample_n(size = size, reps = 2000) |>\n    group_by(replicate) |>\n    summarise(xbar = mean(heights))\n\n  mean_xbar <- mean(df$xbar)\n\n  ggplot(df, aes(x = xbar)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    geom_vline(aes(xintercept = mean_xbar), color = \"blue\", linetype = \"dashed\") +\n    geom_text(aes(x = mean_xbar, label = sprintf(\"%.2f\", mean_xbar), y = Inf), hjust = -0.1, vjust = 2, color = \"blue\") +\n    ggtitle(paste0(\"Sample Size: \", size)) +\n    xlab(\"Mean height (m)\") +\n    xlim(-3, 8)\n})\nwrap_plots(plots)\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-11-1.png)\n:::\n:::\n\n\n\nLarger samples give more precise estimates of the population mean. The **narrower distribution** of sample means reflects this precision, measured by the **standard error**.\n\n## Effect of variability\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1221)\n\n# Define a function to generate ggplot objects\ngenerate_plot <- function(sd) {\n  data <- rnorm(500, 1.99, sd)\n  p <- ggplot(data = tibble(x = data), aes(x = x)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    ggtitle(paste(\"SD =\", sd)) +\n    xlim(-100, 100)\n  return(p)\n}\n\n# Apply the function to a list of standard deviations\nsds <- c(3, 6, 15, 25)\nplots <- lapply(sds, generate_plot)\n\n# Wrap the plots\nwrap_plots(plots)\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-12-1.png)\n:::\n:::\n\n\nMore variable populations (e.g. tree heights ranging from 1 m to 50 m) produce more spread in sample means. The **standard error** captures this reduced precision.\n\n## CLT drives statistical inference\n\nThe CLT enables inference about the population mean without knowing the population distribution.\n\n- With sufficient observations, the sampling distribution of the mean is approximately normal\n- The centre of that distribution is the true population mean\n- The standard error measures the spread -- how precise your estimate is\n\n# Standard error and confidence intervals\n\n## Standard Error of the Mean\n- The standard deviation of the sampling distribution -- how much sample means vary across samples\n- Decreases as sample size increases, so larger samples yield more precise estimates\n\n::: fragment\n::: callout-warning\n## Formula\n$$SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}$$\n\n- where $s$ is the sample standard deviation\n- $n$ is the sample size\n:::\n:::\n\n\n## When to report SD or SE\n\n### Standard Deviation (SD)\n- Describes variability in your **data**\n- Does not change with sample size\n\n### Standard Error (SE)\n- Describes precision of your **mean estimate**\n- Shrinks as sample size increases ($SE = \\frac{SD}{\\sqrt{n}}$)\n\nWhen reporting:\n\n- **mean ± SE** shows how precisely you've estimated the mean — but SE can be misleadingly small with large samples, so always report $n$\n- **mean ± SD** shows the spread of your raw data\n\n## Confidence intervals\n### What is a confidence interval?\n- A range of values that likely contains the true population parameter, computed from your sample\n- 95% is the standard choice\n- Wider intervals indicate less precision\n\n::: fragment\n::: callout-warning\n## Formula for 95% CI\n$$\\bar{x} \\pm (t_{n-1} \\times SE_{\\bar{x}})$$\n\n- where $t_{n-1}$ comes from the t-distribution (covered next lecture)\n:::\n:::\n\n## Visualising confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#| warning: false\n\n# Generate sample data\nset.seed(253)\nsample_data <- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 30),\n  value = c(\n    rnorm(30, 100, 15),\n    rnorm(30, 110, 15),\n    rnorm(30, 105, 15)\n  )\n)\n\n# Calculate means and CIs\nci_data <- sample_data %>%\n  group_by(group) %>%\n  summarise(\n    mean = mean(value),\n    se = sd(value) / sqrt(n()),\n    ci_lower = mean - qt(0.975, n() - 1) * se,\n    ci_upper = mean + qt(0.975, n() - 1) * se\n  )\n\n# Plot\nggplot(ci_data, aes(x = group, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +\n  ggtitle(\"Means with 95% Confidence Intervals\")\n```\n\n::: {.cell-output-display}\n![](Lecture-01b_files/figure-typst/unnamed-chunk-13-1.png)\n:::\n:::\n\n\nWe will learn more about confidence intervals in the next lecture.\n\n# Thanks for listening! Questions?\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by]\n",
    "supporting": [
      "Lecture-01b_files/figure-typst"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}