{
  "hash": "8d79da7b6853ea723e07272d862991a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 4 lecture code and notes\"\nsubtitle: ENVX2001 Applied Statistical Methods\nauthor: \"Aaron Greenville\"\n---\n\n## Load packages\n\nFirst lets load some helpful packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(moments) # for some summary statistics\n```\n:::\n\n\n# Synthetic Example\n\nFirst we will generate some synthetic data to illustrate the concepts of ANOVA and the assumptions of the model. Here you will see the workflow for fitting an ANOVA model, checking the assumptions and then performing post-hoc tests.\n\nLet's generate some synthetic data. We will generate data from 4 treatments with different means. This will help illustrate the assumptions of the ANOVA model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # for reproducibiity\n\n## Generate data\nrep <- 25\nTreatment <- as.factor(rep(LETTERS[1:4], each = rep))\nmu <- rep(c(1, 2, 3, 1.5), each = rep)\ny <- exp(rnorm(4 * rep, mu, 1))\n```\n:::\n\n\n## Raw data exploration\n\nIt is always a good idea to explore the raw data before fitting any models. This can help identify any potential issues with the data that may need to be addressed before fitting the model.\n\n::: callout-tip\n`par(mfrow = c(1,2))` is used to set the layout of the plots. The first number is the number of rows and the second number is the number of columns. `par(mfrow = c(1,1))` is used to reset the layout to the default.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(y ~ Treatment)\nhist(y, main = \"\", xlab = \"Y\")\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nAlready we see there could be issues with the data. The data is right skewed and the variance is not constant across the treatments. This is a common issue with ecological data and is why we often need to transform the data before fitting the model.\n\nGenerate some summary statistics for the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Summary stats by treatment\ntapply(y, Treatment, median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        A         B         C         D \n 2.185894  6.945461 21.178875  5.690122 \n```\n\n\n:::\n\n```{.r .cell-code}\ntapply(y, Treatment, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        A         B         C         D \n 4.113848 12.234283 31.387065  8.418323 \n```\n\n\n:::\n\n```{.r .cell-code}\ntapply(y, Treatment, skewness)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       A        B        C        D \n1.744465 2.728535 2.320537 2.362395 \n```\n\n\n:::\n\n```{.r .cell-code}\ntapply(y, Treatment, kurtosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        A         B         C         D \n 4.807209 11.564391  8.460269  9.181076 \n```\n\n\n:::\n:::\n\n\nThe above summary statistics show that the data is right skewed and has a high kurtosis. This is not ideal for ANOVA and is consistent with the above plots.\n\n::: callout-tip\nThe skewness and kurtosis are measures of the shape of the distribution. Skewness measures the asymmetry of the distribution and kurtosis measures the thickness of the tails of the distribution.\n:::\n\n## Fit ANOVA\n\nFit the ANOVA model to the data. This is done using the `aov` function in R. The model is fitted using the formula `y ~ Treatment` where `y` is the response variable and `Treatment` is the factor variable.\n\nIt seems weird to fit the model to the data before checking the assumptions, but we need to calculate the residuals from the model to use to test our assumptions. We now focus on the residuals of the model, not the raw data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny.aov <- aov(y ~ Treatment)\n```\n:::\n\n\n## Check assumptions\n\nThe assumptions of the ANOVA model are that the residuals are normally distributed and have constant variance. We can check these assumptions using a variety of diagnostic plots and tests.\n\nThere are two main schools of thought on how to check these assumptions. The first is to check the residuals of the model and use formal statistical tests (an older approach), the second is to use graphical approaches. The more statistical tests you do the greater the chance of a type 1 error - more on that when we get to post-hoc tests.\n\n::: callout-tip\nThe residuals are the difference between the observed values and the predicted values from the model.\n:::\n\n### Formal statistical tests\n\nHere we will use the Shapiro-Wilk test to test the normality of the residuals. We will also use the Bartlett test to test the homogeneity of variance. As a comparison, we will also use the raw data to test for normality, but this isn't the best approach (see tutorial for more).\n\n#### Normality tests\n\nShapiro-Wilk test on the raw data. Note how we need to use the `tapply` function to apply the test to each treatment level. If you have lots of treatments or a more complex model you may need to use a different approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntapply(y, Treatment, shapiro.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$A\n\n\tShapiro-Wilk normality test\n\ndata:  X[[i]]\nW = 0.71793, p-value = 1.283e-05\n\n\n$B\n\n\tShapiro-Wilk normality test\n\ndata:  X[[i]]\nW = 0.69122, p-value = 5.566e-06\n\n\n$C\n\n\tShapiro-Wilk normality test\n\ndata:  X[[i]]\nW = 0.71734, p-value = 1.258e-05\n\n\n$D\n\n\tShapiro-Wilk normality test\n\ndata:  X[[i]]\nW = 0.72923, p-value = 1.85e-05\n```\n\n\n:::\n:::\n\n\n**Graphical approach**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\ntapply(y, Treatment, hist)\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$A\n$breaks\n [1]  0  2  4  6  8 10 12 14 16 18\n\n$counts\n[1] 11  6  4  0  1  0  1  1  1\n\n$density\n[1] 0.22 0.12 0.08 0.00 0.02 0.00 0.02 0.02 0.02\n\n$mids\n[1]  1  3  5  7  9 11 13 15 17\n\n$xname\n[1] \"X[[i]]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n$B\n$breaks\n[1]  0 10 20 30 40 50 60 70\n\n$counts\n[1] 14  8  2  0  0  0  1\n\n$density\n[1] 0.056 0.032 0.008 0.000 0.000 0.000 0.004\n\n$mids\n[1]  5 15 25 35 45 55 65\n\n$xname\n[1] \"X[[i]]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n$C\n$breaks\n[1]   0  20  40  60  80 100 120 140 160\n\n$counts\n[1] 12  8  2  1  1  0  0  1\n\n$density\n[1] 0.024 0.016 0.004 0.002 0.002 0.000 0.000 0.002\n\n$mids\n[1]  10  30  50  70  90 110 130 150\n\n$xname\n[1] \"X[[i]]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n$D\n$breaks\n[1]  0  5 10 15 20 25 30 35 40\n\n$counts\n[1] 11  7  4  1  1  0  0  1\n\n$density\n[1] 0.088 0.056 0.032 0.008 0.008 0.000 0.000 0.008\n\n$mids\n[1]  2.5  7.5 12.5 17.5 22.5 27.5 32.5 37.5\n\n$xname\n[1] \"X[[i]]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n```\n\n\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nShapiro-Wilk test on the residuals of the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(resid(y.aov))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  resid(y.aov)\nW = 0.66003, p-value = 6.934e-14\n```\n\n\n:::\n:::\n\n\nMuch easier! The residuals are not normally distributed, but we still need to check the variance.\n\n**Graphical approach**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nhist(y.aov$resid, main = \"\", xlab = \"Residuals\")\nhist(rstandard(y.aov), main = \"\", xlab = \"Standardised Residuals\")\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nBetter approach is to use QQplots. This is a graphical approach to check the normality of the residuals. The residuals should fall on the line if they are normally distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\n\nqqnorm(rstandard(y.aov)) # standarised residuals\nqqline(rstandard(y.aov))\nqqnorm(resid(y.aov)) # residuals\nqqline(resid(y.aov))\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n::: callout-tip\nThe standardised residuals are the residuals divided by the standard deviation of the residuals. This is a way to standardise the residuals so they are easier to interpret. You can use either the residuals or the standardised residuals to check the assumptions. Note how the y-axis is different between the two plots.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewness(rstandard(y.aov))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.712531\n```\n\n\n:::\n:::\n\n\nNote the skewness of the residuals is 3.7 which is very high. This is consistent with the right skewed data we generated.\n\n#### Equal variance tests\n\n**Graphical approach**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(fitted(y.aov), rstandard(y.aov), xlab = \"fitted values\", ylab = \"standardised residuals\")\nabline(0, 0)\nplot(fitted(y.aov), resid(y.aov), xlab = \"fitted values\", ylab = \"residuals\")\nabline(0, 0)\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n::: callout-tip\nThe residuals should be homoscedastic, meaning they have constant variance across the range of the fitted values. This is an assumption of the ANOVA model. We are looking for no pattern in the residuals as the fitted values increase. Similar to the QQplot, you can use the residuals or standarised residuals. Note how the y-axis is different between the two plots.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntapply(y, Treatment, sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        A         B         C         D \n 4.457697 13.027553 34.078634  8.354337 \n```\n\n\n:::\n:::\n\n\nThe standard deviations of the residuals are not constant across the treatments. This is consistent with the right skewed data we generated.\n\n**Formal statistical test**\n\nEqual variance tests. These are ok for simple models, but can be problematic for more complex models. The Bartlett test is used to test the homogeneity of variance across the treatments. Recommended to use the graphical approach above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbartlett.test(rstandard(y.aov) ~ Treatment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  rstandard(y.aov) by Treatment\nBartlett's K-squared = 95.877, df = 3, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe Bartlett test is significant, indicating that the variances are not equal across the treatments. This is consistent with the graphical approach.\n\n## Assumptions not met - now what?\n\nThe assumptions of the ANOVA model are not met. The residuals are not normally distributed and the variance is not constant across the treatments. This is consistent with the right skewed data we generated.\n\nSo we need to transform the data to meet the assumptions of the model. A common transformation is the log transformation. This is often used for right skewed data.\n\nThe workflow is the same as above, but we will use the transformed data. If the log transformation doesn't work, you can try other transformations or use a non-parametric test. You need to repeat the above steps for each transformation.\n\n## Fit ANOVA\n\nFit the ANOVA model to the transformed data. This is done using the `aov` function in R. The model is fitted using the formula `log(y) ~ Treatment` where `log(y)` is the response variable and `Treatment` is the factor variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny.aov <- aov(log(y) ~ Treatment)\n```\n:::\n\n\n## Data exploration on the transformed data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Raw data\npar(mfrow = c(1, 2))\nplot(log(y) ~ Treatment)\nhist(log(y), main = \"\", xlab = \"Y\")\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewness(y.aov$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1042677\n```\n\n\n:::\n:::\n\n\nLooking better! The skewness of the residuals is now 0.1 which is much better than the 3.7 we had before.\n\n## Check assumptions\n\n### Normality tests\n\n**Graphical approach**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nhist(y.aov$resid, main = \"\", xlab = \"Residuals\")\nhist(rstandard(y.aov), main = \"\", xlab = \"Standardised Residuals\")\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nHistograms of the residuals and standardised residuals show the residuals are now normally distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nqqnorm(rstandard(y.aov))\nqqline(rstandard(y.aov))\n\nqqnorm(resid(y.aov))\nqqline(resid(y.aov))\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nThe QQplots show the residuals are now normally distributed.\n\n### Equal variance tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(fitted(y.aov), rstandard(y.aov), xlab = \"fitted values\", ylab = \"standardised residuals\")\nabline(0, 0)\n\nplot(fitted(y.aov), resid(y.aov), xlab = \"fitted values\", ylab = \"residuals\")\nabline(0, 0)\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nThere are no patterns in the residuals as the fitted values increase, indicating the variance is constant across the treatments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntapply(log(y), Treatment, sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        A         B         C         D \n0.9467324 0.9188734 0.9724214 0.8304201 \n```\n\n\n:::\n:::\n\n\nThe standard deviations of the residuals are now constant across the treatments.\n\n## ANOVA results\n\nNow that the assumptions of the ANOVA model are met, we can look at the ANOVA results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(y.aov)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nTreatment    3  53.53  17.844   21.14 1.34e-10 ***\nResiduals   96  81.02   0.844                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe ANOVA results show that the treatment has a significant effect on the response variable. We don't know what is driving the result, so we need to do post-hoc tests to determine which treatments are different from each other.\n\n# Post-hoc test\n\nAgain there are two main schools of thought on how to do post-hoc tests. The first is to use formal statistical tests, the second is to use graphical approaches. The more statistical tests you do the greater the chance of a type 1 error.\n\n**Formal statistical tests** There are many post-hoc tests available to determine which treatments are different from each other. The most common is the Tukey HSD test. This is a conservative test that controls the family-wise error rate. There are other tests available, but they are less conservative and can lead to more type 1 errors.\n\nWe will illustrate two functions in R to do the Tukey HSD test. The first is the `TukeyHSD` function and the second is the `emmeans` function. The `emmeans` function is more flexible and can be used for more complex models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(y.aov)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = log(y) ~ Treatment)\n\n$Treatment\n          diff        lwr        upr     p adj\nB-A  1.1354677  0.4560893  1.8148461 0.0001824\nC-A  2.0435713  1.3641929  2.7229498 0.0000000\nD-A  0.8159058  0.1365274  1.4952842 0.0118471\nC-B  0.9081036  0.2287252  1.5874821 0.0039507\nD-B -0.3195619 -0.9989403  0.3598165 0.6095421\nD-C -1.2276655 -1.9070440 -0.5482871 0.0000461\n```\n\n\n:::\n:::\n\n\nThe above results show that all treatment levels are different from each other, except D and C. The first column `diff` is the difference between the means, the second column `lwr` is the lower bound of the confidence interval and the third column `upr` is the upper bound of the confidence interval. The last column `p adj` is the p-value adjusted for multiple comparisons.\n\n::: callout-tip\nUsing the emmeans package (recommended way) to do the Tukey HSD test. This is more flexible and can be used for more complex models.\n:::\n\nThe `emmeans` function calculates the estimated marginal means for each treatment level and then calculates the differences between the means. The `pairwise` argument is used to specify which treatments to compare. The `adjust` argument is used to specify the method to adjust for multiple comparisons. The default is the Tukey HSD test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(emmeans)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\n```\n\n\n:::\n\n```{.r .cell-code}\nemmeans(y.aov, pairwise ~ Treatment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n Treatment emmean    SE df lower.CL upper.CL\n A          0.967 0.184 96    0.602     1.33\n B          2.102 0.184 96    1.737     2.47\n C          3.010 0.184 96    2.646     3.37\n D          1.783 0.184 96    1.418     2.15\n\nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast estimate   SE df t.ratio p.value\n A - B      -1.135 0.26 96  -4.370  0.0002\n A - C      -2.044 0.26 96  -7.865  <.0001\n A - D      -0.816 0.26 96  -3.140  0.0118\n B - C      -0.908 0.26 96  -3.495  0.0040\n B - D       0.320 0.26 96   1.230  0.6095\n C - D       1.228 0.26 96   4.725  <.0001\n\nResults are given on the log (not the response) scale. \nP value adjustment: tukey method for comparing a family of 4 estimates \n```\n\n\n:::\n:::\n\n\nThe above results show the same results as the `TukeyHSD` function. There are two tables in the output. The first table shows the estimated marginal means for each treatment level. The second table shows the differences between the means and the p-values adjusted for multiple comparisons. See how estimate is the same a diff in the TukeyHSD output.\n\n**Graphical approach**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(emmeans(y.aov, pairwise ~ Treatment))\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nThe above plot shows the differences between the means and the confidence intervals. The confidence intervals that do not cross zero are significantly different from each other.\n\n::: callout-tip\nThe graphical approach is a good way to visualise the differences between the means. It is often easier to interpret than the tables of results. However it is up to use to decide which approach to use. Just justify it and often a combination of both helps you understand the results better until you get more experience.\n:::\n\n# Example data - chicken weights\n\n## Read in data and check structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchicks <- read.csv(\"data/Chick weights.csv\")\nhead(chicks) # first 6 records\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Diet WtGain\n1    1     99\n2    1     88\n3    1     76\n4    1     38\n5    1     94\n6    2     61\n```\n\n\n:::\n:::\n\n\nWe need to check the structure of the data to make sure it is in the correct format. The `Diet` variable is a factor variable, so we need to convert it to a factor using the `as.factor` function. The `WtGain` variable is a numeric variable, so we don't need to do anything to it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(chicks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t20 obs. of  2 variables:\n $ Diet  : int  1 1 1 1 1 2 2 2 2 2 ...\n $ WtGain: int  99 88 76 38 94 61 112 30 89 63 ...\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchicks$Diet <- as.factor(chicks$Diet)\nstr(chicks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t20 obs. of  2 variables:\n $ Diet  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 2 2 2 2 2 ...\n $ WtGain: int  99 88 76 38 94 61 112 30 89 63 ...\n```\n\n\n:::\n:::\n\n\n## Data exploration\n\n### summary stats\n\nCalculate the summary statistics for the data. This will help us understand the data better and identify any potential issues.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(chicks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Diet      WtGain      \n 1:5   Min.   : 30.00  \n 2:5   1st Qu.: 72.75  \n 3:5   Median : 90.50  \n 4:5   Mean   : 93.55  \n       3rd Qu.:102.25  \n       Max.   :169.00  \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(chicks$WtGain)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.00   72.75   90.50   93.55  102.25  169.00 \n```\n\n\n:::\n:::\n\n\nLets have a look at the means, sd, skewness and kurtosis for the data. This will help us understand the shape of the data and identify any potential issues.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naggregate(WtGain ~ Diet, mean, data = chicks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Diet WtGain\n1    1   79.0\n2    2   71.0\n3    3   81.4\n4    4  142.8\n```\n\n\n:::\n\n```{.r .cell-code}\naggregate(WtGain ~ Diet, sd, data = chicks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Diet   WtGain\n1    1 24.47448\n2    2 31.02418\n3    3 22.87575\n4    4 34.90272\n```\n\n\n:::\n\n```{.r .cell-code}\naggregate(WtGain ~ Diet, summary, data = chicks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Diet WtGain.Min. WtGain.1st Qu. WtGain.Median WtGain.Mean WtGain.3rd Qu.\n1    1        38.0           76.0          88.0        79.0           94.0\n2    2        30.0           61.0          63.0        71.0           89.0\n3    3        42.0           81.0          92.0        81.4           95.0\n4    4        85.0          137.0         154.0       142.8          169.0\n  WtGain.Max.\n1        99.0\n2       112.0\n3        97.0\n4       169.0\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary stats by treatment\nlibrary(moments)\ntapply(chicks$WtGain, chicks$Diet, median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  1   2   3   4 \n 88  63  92 154 \n```\n\n\n:::\n\n```{.r .cell-code}\ntapply(chicks$WtGain, chicks$Diet, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    1     2     3     4 \n 79.0  71.0  81.4 142.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ntapply(chicks$WtGain, chicks$Diet, skewness)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1           2           3           4 \n-1.08377571  0.04043688 -1.25291289 -1.02499681 \n```\n\n\n:::\n\n```{.r .cell-code}\ntapply(chicks$WtGain, chicks$Diet, kurtosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4 \n2.650344 1.946566 2.871036 2.552305 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(WtGain ~ Diet, data = chicks, ylab = \"Weight gain (g)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\nCan you see any issues with the data?\n\n## Testing assumptions\n\n### Graphical approach\n\n**Normality tests**\n\nFit the model to the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.aov <- aov(WtGain ~ Diet, data = chicks) # one-way anova\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nhist(model.aov$resid, main = \"\", xlab = \"Residuals\")\nhist(rstandard(model.aov), main = \"\", xlab = \"Standardised Residuals\")\n\n\nqqnorm(rstandard(model.aov), main = \"Normal Q-Q Plot:Standardised Residuals\")\nqqline(rstandard(model.aov))\n\nqqnorm(resid(model.aov), main = \"Normal Q-Q Plot:Residuals\")\nqqline(resid(model.aov))\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewness(rstandard(model.aov))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.7718594\n```\n\n\n:::\n:::\n\n\nskewness of the residuals is -0.77.\n\nThere is some skewness in the residuals, but it is not too bad. The QQplots show the residuals are normally distributed, but with some values below the line.\n\n**Equal variance tests**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(fitted(model.aov), rstandard(model.aov), xlab = \"fitted values\", ylab = \"standardised residuals\")\nabline(0, 0, lty = 2)\n\nplot(fitted(model.aov), resid(model.aov), xlab = \"fitted values\", ylab = \"residuals\")\nabline(0, 0, lty = 2)\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nNote you only need one of these plots, but I've included both to show you the difference between the residuals and standardised residuals.\n\nThere isn't a pattern in the residuals as the fitted values increase, indicating the variance is constant across the treatments.\n\nA quicker coding approach is to use the `plot` function on the model object. This will give you a range of diagnostic plots for the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(model.aov)\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nThe plots of interest are the residuals vs fitted values and the normal Q-Q plot of the residuals. The residuals vs fitted values plot shows no pattern in the residuals as the fitted values increase. The normal Q-Q plot of the residuals shows the residuals are normally distributed.\n\nIf you just want to pull out these two plots, you can use this code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(model.aov, which = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n### Formal test approach\n\n**Equal variance tests**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbartlett.test(rstandard(model.aov) ~ chicks$Diet)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  rstandard(model.aov) by chicks$Diet\nBartlett's K-squared = 0.85164, df = 3, p-value = 0.8371\n```\n\n\n:::\n:::\n\n\n**Normality test**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(resid(model.aov))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  resid(model.aov)\nW = 0.90961, p-value = 0.06265\n```\n\n\n:::\n:::\n\n\n## ANOVA results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model.aov)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value Pr(>F)   \nDiet         3  16467    5489   6.647  0.004 **\nResiduals   16  13212     826                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe ANOVA results show that the diet has a significant effect on the weight gain of the chicks. The p-value is less than 0.05, so we can reject the null hypothesis that the means are equal.\n\n## Example Kruskal-Wallis test (not really needed for the chick data)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.kruskal <- kruskal.test(WtGain ~ Diet, data = chicks)\nmodel.kruskal\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  WtGain by Diet\nKruskal-Wallis chi-squared = 7.0625, df = 3, p-value = 0.06993\n```\n\n\n:::\n:::\n\n\n::: callout-note\nNote the Kruskal-Wallis test is not significant, but the ANOVA test is. This is because the Kruskal-Wallis test is a non-parametric test and is less powerful than the ANOVA test.\n:::\n\n## Post-hoc tests\n\nUsing the Tukey HSD test to determine which treatments are different from each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(model.aov)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = WtGain ~ Diet, data = chicks)\n\n$Diet\n    diff        lwr       upr     p adj\n2-1 -8.0 -59.996625  43.99662 0.9705671\n3-1  2.4 -49.596625  54.39662 0.9991415\n4-1 63.8  11.803375 115.79662 0.0138464\n3-2 10.4 -41.596625  62.39662 0.9389332\n4-2 71.8  19.803375 123.79662 0.0056645\n4-3 61.4   9.403375 113.39662 0.0180630\n```\n\n\n:::\n:::\n\n\nFrom the table above, we can see that diet 4 is different from the other diets and the difference is positve i.e. chicks on diet 4 have a higher weight gain than the other diets.\n\nYou can also plot the results of the Tukey HSD test to visualise the differences between the means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(TukeyHSD(model.aov))\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\nAlternative method is to use the `emmeans` function to do the Tukey HSD test. This is more flexible and can be used for more complex models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"emmeans\")\nlibrary(emmeans)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(model.aov, ~Diet)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Diet emmean   SE df lower.CL upper.CL\n 1      79.0 12.9 16     51.8    106.2\n 2      71.0 12.9 16     43.8     98.2\n 3      81.4 12.9 16     54.2    108.6\n 4     142.8 12.9 16    115.6    170.0\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(model.aov, pairwise ~ Diet)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n Diet emmean   SE df lower.CL upper.CL\n 1      79.0 12.9 16     51.8    106.2\n 2      71.0 12.9 16     43.8     98.2\n 3      81.4 12.9 16     54.2    108.6\n 4     142.8 12.9 16    115.6    170.0\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast      estimate   SE df t.ratio p.value\n Diet1 - Diet2      8.0 18.2 16   0.440  0.9706\n Diet1 - Diet3     -2.4 18.2 16  -0.132  0.9991\n Diet1 - Diet4    -63.8 18.2 16  -3.510  0.0138\n Diet2 - Diet3    -10.4 18.2 16  -0.572  0.9389\n Diet2 - Diet4    -71.8 18.2 16  -3.951  0.0057\n Diet3 - Diet4    -61.4 18.2 16  -3.378  0.0181\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(emmeans(model.aov, pairwise ~ Diet))\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(emmeans(model.aov, pairwise ~ Diet), comparisons = TRUE)\n```\n\n::: {.cell-output-display}\n![](Lecture-04_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n:::\n\n\n::: callout-tip\nIn the plot function above, we've specified `comparisons = TRUE`. The blue bars are confidence intervals for the EMMs, and the red arrows are for the comparisons among them. If an arrow from one mean overlaps an arrow from another group, the difference is not significant.\n:::\n\nWhat would you conclude from the above plot and recommend to a farmer wanting to maximise growth?\n",
    "supporting": [
      "Lecture-04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}