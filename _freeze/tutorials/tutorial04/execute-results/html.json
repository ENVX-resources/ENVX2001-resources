{
  "hash": "7550ee7f904443cd13aa51d25ec9d504",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tutorial 04: ANOVA Assumptions and Post-hoc Tests\"\n# reviewed-by:\n---\n\n\n\nIn this week's lectures we covered how to check the assumptions of ANOVA using residual diagnostics, and how to determine which pairs of treatment means are significantly different using post-hoc tests. A solid understanding of these concepts is essential for conducting robust statistical analyses in R and before we move to more complex models in future weeks.\n\n::: callout-tip\n## Key learning outcomes\n\n-   Understand and apply components of the experimental design workflow.\n\n-   Assess whether a fitted ANOVA model meets its statistical assumptions using residuals.\n    -   What are residuals?\n    -   What are the assumptions of an ANOVA?\n    -   What tests and graphical approaches to use to test assumptions?\n\n-   Identify which pair(s) of treatment means are significantly different.\n    -   When to do a post-hoc test\n    -   Statistical tests and graphical methods for post-hoc tests\n    -   Family-wise error rate and power trade-off.\n\nData for this tutorial is in the **Data4.xlsx** file.\n:::\n\n# Experimental Design Workflow\n\nIn this tutorial we will work through the experimental design workflow for a one-way ANOVA.\n\nWe introduced the workflow outlined by Fox, G. A., S. Negrete-Yankelevich, and V. J. Sosa. (2015). [Ecological statistics: contemporary theory and application](https://global.oup.com/academic/product/ecological-statistics-9780199672547). Oxford University Press, USA.\n\n![](images/Lecture_Topic4.png)\n\nWe will assume we have formed a hypothesis, designed an experiment, collected data and entered it into R. In our case we have chosen an ANOVA for our model and now need to do an assessment of model assumptions.\n\n# Assessing ANOVA Assumptions Using Residuals\n\nFirst we need to understand a few key concepts.\n\n## What are residuals?\n\n::: callout-important\n## Residuals\n\nResiduals are the differences between the observed values and the values predicted by our statistical model. In the context of ANOVA, residuals help us assess how well our model fits the data.\n:::\n\nMathematically, the residual for each observation can be calculated as:\n\n$$\n\\text{Residual} = \\text{Observed Value} - \\text{Predicted Value}\n$$\n\nResiduals have key advantages over raw data for testing assumptions and you will be required to use the residuals of your model to test the assumptions of ANOVA and regression from now on.\n\nKey advantages of using residuals:\n\n-   They isolate the unexplained variation in the data after accounting for the effects of the independent variables.\n-   They allow us to assess the assumptions of the model more directly, as they should ideally be randomly distributed if the model is appropriate.\n-   They are the best way to test for assumptions of more complex models where there are multiple factors, which we will introduce in following weeks.\n\n## Exercise 1: Why use residuals?\n\n::: callout-important\nThis exercise illustrates why it is not ideal to test the normality assumption using all of the observations irrespective of the treatments or the size of dataset.\n:::\n\nFirst we will create 2 synthetic datasets which we sample 50 times (`n=50`) from a normally distributed population. Both underlying populations have the same variation (`sd=3`) but have a different mean (`mean=10`, `mean=40`). We then plot the histograms for each individually, both groups combined and the combined residuals (observation minus group mean).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\ngroup1 <- rnorm(n = 50, mean = 10, sd = 3)\ngroup2 <- rnorm(n = 50, mean = 40, sd = 3)\n\npar(mfrow = c(2, 2))\nhist(group1, main = \"A: Group1\", xlab = \"\")\nhist(group2, main = \"B: Group2\", xlab = \"\")\nhist(c(group1, group2), main = \"C: Group1&2\", xlab = \"\")\nhist(c(group1 - mean(group1), group2 - mean(group2)), main = \"D: Residuals Group1&2\", xlab = \"\")\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nWe can see that histogram of each group is normally distributed (A, B), however when we combine the data we have 2 distinct groupings centred on the mean of each group (C). Therefore, if we look at the raw data irrespective of the groups we **would not see a normally distributed dataset**. This is because the effect of individual treatments (or groups) is different so each observation is perturbed according to the treatment it receives or group it is in. If we examine the residuals (D), the treatment (or group) effects have been removed and we can then test if the data is normal or has constant variance. It requires fitting of a model to the data, in this case a 1-way ANOVA model. This is why we test the assumptions on the residuals. You could look at the distribution of each group separately but then for some experiments the replication is small so it is hard to assess normality, using residuals allows all of the observations to be pooled together.\n\n\n### 1.1) Testing assumptions using residuals\n\nNow we will fit a one-way ANOVA model to the data and extract the residuals to test the assumptions of normality and homogeneity of variance.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create a data frame\ndata <- data.frame(\n  value = c(group1, group2),\n  group = as.factor(rep(c(\"Group1\", \"Group2\"), each = 50))\n)\n# Fit a one-way ANOVA model\nanova_model <- aov(value ~ group, data = data)\n# Extract residuals\nresiduals_anova <- residuals(anova_model)\n# Plot histogram of residuals\nhist(residuals_anova, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nWe can see that the histogram of the residuals appears to be normally distributed.\n\nLet's compare this to the histogram of the raw data (also plot C above).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Plot histogram of raw data\nhist(data$value, main = \"Histogram of Raw Data\", xlab = \"Value\")\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe histogram of the raw data shows two distinct peaks corresponding to the two groups, indicating that the data is not normally distributed when considering all observations together. This highlights the importance of using residuals to assess normality in the context of ANOVA.\n\nWe can also create a Q-Q plot to further assess normality.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Q-Q plot of residuals\nqqnorm(residuals_anova)\nqqline(residuals_anova, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe Q-Q plot shows that the residuals closely follow the reference line, indicating that they are approximately normally distributed.\n\n::: callout-important\n## ANOVA has several key assumptions:\n\n1.  **Independence**: The observations are independent of each other.\n2.  **Normality**: The residuals of the model are normally distributed.\n3.  **Homogeneity of variances**: The variances across the different groups are equal.\n:::\n\n## Statistical tests and graphical approaches\n\n## Exercise 2: Diatoms in streams\n\nHere we will test the assumptions using residual diagnostics and finding significant differences using plots and Tukey's test. The data is found in the **Diatoms** worksheet.\n\n![Image credit: B. Caissie, https://diatoms.org/](images/Sea_ice_diatoms.jpeg){fig-align=\"center\" width=\"486\"}\n\nA researcher is interested in the effect of zinc pollution on the diversity of diatom communities in streams. They set up an experiment with four levels of zinc concentration: background (back), low, medium, and high. After a certain period, they measure the diversity of diatom species in each stream.\n\n### 2.1) Importing and processing data, then fitting an ANOVA model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(readxl)\ndiatoms <- read_excel(\"data/Data4.xlsx\", sheet = \"Diatoms\")\ndiatoms$Zinc <- as.factor(diatoms$Zinc)\nstr(diatoms)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [34 × 4] (S3: tbl_df/tbl/data.frame)\n $ Stream   : chr [1:34] \"Eagle\" \"Blue\" \"Blue\" \"Blue\" ...\n $ Zinc     : Factor w/ 4 levels \"BACK\",\"HIGH\",..: 1 1 1 1 1 1 1 1 3 3 ...\n $ Diversity: num [1:34] 2.27 1.7 2.05 1.98 2.2 1.53 0.76 1.89 1.4 2.18 ...\n $ Group    : num [1:34] 1 1 1 1 1 1 1 1 2 2 ...\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nanova.diatoms <- aov(Diversity ~ Zinc, data = diatoms)\nsummary(anova.diatoms)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value Pr(>F)  \nZinc         3  2.567  0.8555   3.939 0.0176 *\nResiduals   30  6.516  0.2172                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n### 2.2) Testing assumptions\n\nTo test the model assumptions we encourage you to produce 3 figures:\n\n-   histogram of the residuals;\n-   a QQ plot of the residuals;\n-   plot residuals against fitted values.\n\nIt is good practice to base this on the standardised residuals which can be extracted from a model object using the `rstandard` function. Standardised residuals are $\\sim N(0,1)$, and make it easier to interpret the plots for outliers. Based on the normal distribution 95% of observations fall within ±2 SD's of the mean or in the case of standardised residuals ±2.\n\n**Histogram of standardised residuals:**\n\nThe figure below presents the histogram of the standardised residuals. The majority of the observations plot as a bell-shaped (normal) distribution. The exception are 2 observations less than -2. Given there are 34 observations this is about 6% of the dataset so acceptable given we expect 95% observations to be in the interval of ~[-2, 2].\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nhist(rstandard(anova.diatoms))\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n**Q-Q plot:**\n\nThe QQ plot below shows that the observed quantiles match the theoretical quantiles (assuming normality) based on the observations reasonably following the 1:1 line. We can assume the data is normally distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nqqnorm(rstandard(anova.diatoms))\nabline(0, 1)\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nAn alternative method for testing normality is to use the Shapiro-Wilk test. This test has the null hypothesis that the data is normally distributed. A p-value > 0.05 indicates we fail to reject the null hypothesis and therefore the data is normally distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nshapiro.test(rstandard(anova.diatoms))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  rstandard(anova.diatoms)\nW = 0.96923, p-value = 0.4405\n```\n\n\n:::\n:::\n\n\nThe p-value is > 0.05 so we can assume the data is normally distributed.\n\n\n### Testing the assumption of constant variance (homoscedasticity)\n\nThe plot below shows the standardised residuals plotted against the fitted values (the group means in this case). To test the assumption of constant variance we want to have the same spread of observations for increases in the fitted values. This is the case here. We don't want to see *fanning* where the spread of residuals increases or decreases while the fitted values increase.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(fitted(anova.diatoms), rstandard(anova.diatoms))\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nAlternatively we can use the Bartlett test to test for homogeneity of variance using the residuals. The null hypothesis is that the variances are equal across groups. A p-value > 0.05 indicates we fail to reject the null hypothesis and therefore the variances are equal.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbartlett.test(rstandard(anova.diatoms) ~ diatoms$Zinc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  rstandard(anova.diatoms) by diatoms$Zinc\nBartlett's K-squared = 0.25337, df = 3, p-value = 0.9685\n```\n\n\n:::\n:::\n\n\nThe p-value is > 0.05 so we can assume the variances are equal.\n\nWe can also use the built-in `plot()` function to produce the diagnostics plots.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(2, 2))\nplot(anova.diatoms)\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nor:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 2))\nplot(anova.diatoms, which = 1) # Residuals vs fitted values\nplot(anova.diatoms, which = 2) # Normal Q-Q plot\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n---\n\n# Identifying Significant Differences: Post-hoc Tests\n\nNow that we have tested the assumptions of the ANOVA and found them to be acceptable we can determine which pairs of treatment means are significantly different. We can do this using Tukey's Honest Significant Difference using the `emmeans` package.\n\n::: callout-important\n## When to do a post-hoc test?\n\nPost-hoc tests are performed after an ANOVA when there are significant differences among group means. The purpose of post-hoc tests is to identify which specific pairs of group means are significantly different from each other.\n\n**Don't do a post-hoc test if the ANOVA is not significant!**\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(emmeans)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\ntukey.diatoms <- emmeans(anova.diatoms, pairwise ~ \"Zinc\")\ntukey.diatoms\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n Zinc emmean    SE df lower.CL upper.CL\n BACK   1.80 0.165 30    1.461     2.13\n HIGH   1.28 0.155 30    0.961     1.60\n LOW    2.03 0.165 30    1.696     2.37\n MED    1.72 0.155 30    1.401     2.04\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate    SE df t.ratio p.value\n BACK - HIGH   0.5197 0.226 30   2.295  0.1219\n BACK - LOW   -0.2350 0.233 30  -1.008  0.7457\n BACK - MED    0.0797 0.226 30   0.352  0.9847\n HIGH - LOW   -0.7547 0.226 30  -3.333  0.0117\n HIGH - MED   -0.4400 0.220 30  -2.003  0.2096\n LOW - MED     0.3147 0.226 30   1.390  0.5153\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n```\n\n\n:::\n:::\n\n\nThe output shows the pairwise comparisons between the different Zinc treatments. The `p.value` column indicates whether the differences between the means are statistically significant. A p-value less than 0.05 indicates a significant difference between the treatment means.\n\nFrom the results, we can see that:\n\n-   The comparison between Zinc High and Zinc low has a p-value of 0.0117, indicating a significant difference in means.\n-   The other comparisons were not significant as their p-values are greater than 0.05.\n\nWe can also visualise the results using a plot.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(tukey.diatoms, comparisons = TRUE)\n```\n\n::: {.cell-output-display}\n![](tutorial04_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n::: callout-tip\n## Interpreting the plot\n\nIf the confidence intervals do not cross over each other, then that pair significantly differs from each other.\n:::\n\nThis plot shows the estimated marginal means (EMM) for each Zinc treatment. In the plot function above, we've specified `comparisons = TRUE`. The blue bars are confidence intervals for the EMMs, and the red arrows are for the comparisons among them. If an arrow from one mean overlaps an arrow from another group, the difference is not significant.\n\nOverall, this analysis allows us to identify which Zinc treatments have significantly different effects on diatom diversity in streams.\n\nWe will explore this experiment further in the practical.\n\n---\n\n# Family-wise Error Rate and Power Trade-off\n\n## Exercise 3: Family-wise error rate\n\nWhen conducting multiple pairwise comparisons, the family-wise error rate increases. To control the family-wise error rate, post-hoc tests like Tukey's HSD are used, which adjust the significance levels to account for the number of comparisons being made.\n\n::: callout-important\n## Family-wise error rate\n\nThe family-wise error rate is the probability of making at least one Type I error (false positive) among all the comparisons.\n:::\n\nTukey's HSD test is designed to maintain the overall family-wise error rate at a specified level (commonly 0.05) while still providing sufficient power to detect true differences between group means.\n\nHowever, there is a trade-off between controlling the family-wise error rate and maintaining statistical power. As the number of comparisons increases, the adjustments made by post-hoc tests can lead to a reduction in power, making it more difficult to detect true differences.\n\nTherefore, it is important to carefully consider the number of comparisons being made and choose appropriate post-hoc tests that balance the need for controlling Type I errors with the desire to maintain adequate statistical power.\n\nLet's look at an example of the trade-off between family-wise error rate and power using simulation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn_groups <- 5\nn_per_group <- 10\nn_simulations <- 1000\nalpha <- 0.05\nfamily_wise_errors <- 0\n\nfor (i in 1:n_simulations) {\n  data <- data.frame(\n    value = rnorm(n_groups * n_per_group, mean = 0, sd = 1),\n    group = as.factor(rep(1:n_groups, each = n_per_group))\n  )\n  \n  anova_model <- aov(value ~ group, data = data)\n  tukey_result <- TukeyHSD(anova_model)\n  \n  if (any(tukey_result$group[, \"p adj\"] < alpha)) {\n    family_wise_errors <- family_wise_errors + 1\n  }\n}\nfamily_wise_error_rate <- family_wise_errors / n_simulations\nfamily_wise_error_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.044\n```\n\n\n:::\n:::\n\n\nIn this simulation, we create 5 groups with 10 observations each, and we run 1000 simulations. We fit a one-way ANOVA model to the data and perform Tukey's HSD test for pairwise comparisons. We count how many times at *least one comparison is significant (p-value < 0.05)* across all simulations to estimate the family-wise error rate (~0.04 or 4% error rate).\n\nThe resulting family-wise error rate should be close to the nominal level of 0.05, demonstrating that Tukey's HSD test effectively controls the family-wise error rate while maintaining reasonable power to detect true differences between group means.\n\nIf we don't use a post-hoc test and just do multiple t-tests we can see how the family-wise error rate increases. For example let's not adjust the p-values for multiple comparisons:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfamily_wise_errors_no_adjust <- 0\nfor (i in 1:n_simulations) {\n  data <- data.frame(\n    value = rnorm(n_groups * n_per_group, mean = 0, sd = 1),\n    group = as.factor(rep(1:n_groups, each = n_per_group))\n  )\n  \n  p_values <- c()\n  for (j in 1:(n_groups - 1)) {\n    for (k in (j + 1):n_groups) {\n      t_test_result <- t.test(value ~ group, data = subset(data, group %in% c(j, k)))\n      p_values <- c(p_values, t_test_result$p.value)\n    }\n  }\n  \n  if (any(p_values < alpha)) {\n    family_wise_errors_no_adjust <- family_wise_errors_no_adjust + 1\n  }\n}\nfamily_wise_error_rate_no_adjust <- family_wise_errors_no_adjust / n_simulations\nfamily_wise_error_rate_no_adjust\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.286\n```\n\n\n:::\n:::\n\n\nIn this simulation, we perform multiple t-tests without adjusting the p-values for multiple comparisons. We count how many times at *least one comparison is significant (p-value < 0.05)* across all simulations to estimate the family-wise error rate.\n\nThe resulting family-wise error rate should be significantly higher than the nominal level of 0.05 (i.e. ~0.286 or 29% error rate!), demonstrating that not adjusting for multiple comparisons leads to an increased risk of Type I errors.\n\nThis example highlights the importance of using post-hoc tests like Tukey's HSD to control the family-wise error rate when conducting multiple pairwise comparisons in ANOVA.\n\n::: callout-tip\n## Visual methods for multiple comparisons\n\nStatistical tests for multiple comparisons can sometimes be difficult to interpret, especially when there are many groups involved. Hence many propose graphical methods as a more powerful way to determine which groups are different, as we saw in the plot above.\n:::\n\n## Power trade-off\n\nWhen conducting multiple comparisons, there is a trade-off between controlling the family-wise error rate and maintaining statistical power. As the number of comparisons increases, the adjustments made by post-hoc tests can lead to a reduction in power, making it more difficult to detect true differences.\n\n::: callout-important\n## Statistical power\n\nStatistical power is the probability of correctly rejecting the null hypothesis when it is false (i.e., detecting a true effect).\n:::\n\nTo illustrate this trade-off, we can simulate data with a known effect size and compare the power of Tukey's HSD test to that of unadjusted t-tests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn_groups <- 5\nn_per_group <- 10\nn_simulations <- 1000\nalpha <- 0.05\ntrue_effect_size <- 1\npower_tukey <- 0\npower_t_test <- 0\nfor (i in 1:n_simulations) {\n  data <- data.frame(\n    value = c(rnorm(n_per_group, mean = 0, sd = 1),\n              rnorm(n_per_group, mean = true_effect_size, sd = 1),\n              rnorm(n_per_group, mean = 0, sd = 1),\n              rnorm(n_per_group, mean = 0, sd = 1),\n              rnorm(n_per_group, mean = 0, sd = 1)),\n    group = as.factor(rep(1:n_groups, each = n_per_group))\n  )\n  \n  anova_model <- aov(value ~ group, data = data)\n  tukey_result <- TukeyHSD(anova_model)\n  \n  if (tukey_result$group[\"2-1\", \"p adj\"] < alpha) {\n    power_tukey <- power_tukey + 1\n  }\n  \n  t_test_result <- t.test(value ~ group, data = subset(data, group %in% c(1, 2)))\n  if (t_test_result$p.value < alpha) {\n    power_t_test <- power_t_test + 1\n  }\n}\npower_tukey_rate <- power_tukey / n_simulations\npower_t_test_rate <- power_t_test / n_simulations\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# print results:\ncat(\"Adjusted for family-rate errors using Tukey's test. Proportion correctly detected:\", power_tukey_rate, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdjusted for family-rate errors using Tukey's test. Proportion correctly detected: 0.299 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Unadjusted t-tests. Proportion correctly detected:\", power_t_test_rate, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnadjusted t-tests. Proportion correctly detected: 0.568 \n```\n\n\n:::\n:::\n\n\nIn this simulation, we create 5 groups with 10 observations each, where one group has a true effect size of 1. We run 1000 simulations and fit a one-way ANOVA model to the data. We then perform Tukey's HSD test and unadjusted t-tests to compare the power of each method in detecting the true effect.\n\nThe resulting power rates indicate the proportion of simulations in which each method correctly detected the true effect. We expect the power of the unadjusted t-tests to be *higher (~57%)* than that of Tukey's HSD test *(\\~30%)*, demonstrating the trade-off between controlling the family-wise error rate and maintaining statistical power.\n\n---\n\n# Summary\n\nIn this tutorial, we covered the following key concepts:\n\n-   The importance of using residuals to assess the assumptions of ANOVA.\n-   How to test the assumptions of normality and homogeneity of variances using graphical methods and statistical tests.\n-   When to perform post-hoc tests and how to interpret the results of Tukey's test.\n-   The trade-off between controlling the family-wise error rate and maintaining statistical power when conducting multiple comparisons.\n\nBy understanding and applying these concepts, you will be better equipped to conduct robust experimental design and statistical analyses in R and interpret the results accurately.\n\n## Example exam questions\n\n1.  Explain why residuals are used to assess the assumptions of ANOVA instead of the raw data. Provide an example to illustrate your explanation.\n\n2.  Describe the assumptions of ANOVA and outline the graphical and statistical methods used to test these assumptions using residuals.\n\n3.  A researcher conducts a one-way ANOVA and finds a significant effect of treatment on a response variable. Explain when and why post-hoc tests are necessary, and describe how to interpret the results of Tukey's Honest Significant Difference (HSD) test.\n\n4.  Discuss the trade-off between controlling the family-wise error rate and maintaining statistical power when conducting multiple comparisons. Provide an example to illustrate this trade-off.\n",
    "supporting": [
      "tutorial04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}