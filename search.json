[
  {
    "objectID": "tutorials/tutorial12.html",
    "href": "tutorials/tutorial12.html",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "We are going to use the same dataset as last tutorial, but do very different analyses.\n\n\nTo do these in R, you need to use the vegan package, so first load it and the MASS package:\n\nlibrary(vegan)\nlibrary(MASS)\nlibrary(ggplot2)\n\n\n\n\nNext load the bird data:\n\nBirds &lt;- read.csv(\"data/macnally.csv\", header = TRUE)\n\n\n\n\nLet’s transform the data using a fourth-root transformation:\n\nTransBirdsData &lt;- (Birds[, 3:104])^(1/4)\n\n\n\n\nNow let’s generate a Bray-Curtis dissimilarity matrix on this data and perform a nMDS. Note this data has already been standardised.\n\nBird.dis &lt;- vegdist(TransBirdsData, method = \"bray\")\n\n\n\n\nLet’s plot the nMDS and label with Habitat Type:\n\npchs &lt;- c(0:5)\nBird_Factor &lt;- factor(Birds$HABITAT)\nnMDS_Bird &lt;- metaMDS(TransBirdsData, distance = \"bray\", k = 2)\n\nRun 0 stress 0.1112209 \nRun 1 stress 0.1112209 \n... Procrustes: rmse 2.378988e-05  max resid 0.0001124603 \n... Similar to previous best\nRun 2 stress 0.1112209 \n... Procrustes: rmse 9.96519e-06  max resid 4.122183e-05 \n... Similar to previous best\nRun 3 stress 0.1268505 \nRun 4 stress 0.1112209 \n... Procrustes: rmse 3.758145e-05  max resid 0.0001780056 \n... Similar to previous best\nRun 5 stress 0.1417702 \nRun 6 stress 0.1112209 \n... Procrustes: rmse 4.312368e-05  max resid 0.0002048902 \n... Similar to previous best\nRun 7 stress 0.111221 \n... Procrustes: rmse 0.0001033651  max resid 0.0004882732 \n... Similar to previous best\nRun 8 stress 0.1112209 \n... Procrustes: rmse 2.339721e-05  max resid 0.000103012 \n... Similar to previous best\nRun 9 stress 0.1417215 \nRun 10 stress 0.1112209 \n... Procrustes: rmse 3.180471e-06  max resid 1.386164e-05 \n... Similar to previous best\nRun 11 stress 0.1112209 \n... Procrustes: rmse 5.780775e-06  max resid 2.582454e-05 \n... Similar to previous best\nRun 12 stress 0.1181205 \nRun 13 stress 0.1627193 \nRun 14 stress 0.1112209 \n... Procrustes: rmse 1.680034e-05  max resid 7.951508e-05 \n... Similar to previous best\nRun 15 stress 0.1112209 \n... Procrustes: rmse 1.63033e-05  max resid 7.724869e-05 \n... Similar to previous best\nRun 16 stress 0.1145319 \nRun 17 stress 0.1112209 \n... Procrustes: rmse 3.321037e-05  max resid 0.0001568831 \n... Similar to previous best\nRun 18 stress 0.1415031 \nRun 19 stress 0.1112209 \n... Procrustes: rmse 1.625044e-05  max resid 7.894352e-05 \n... Similar to previous best\nRun 20 stress 0.1389817 \n*** Best solution repeated 12 times\n\nplot_Bird &lt;- ordiplot(nMDS_Bird, display = \"sites\", type = \"n\")\npoints(nMDS_Bird, col = \"black\", pch = pchs[Bird_Factor])\nlegend(\"topright\", bty = \"n\", legend = levels(Bird_Factor), pch = pchs)\n\n\n\n\n\n\n\n\n\nnMDS_Bird\n\n\nCall:\nmetaMDS(comm = TransBirdsData, distance = \"bray\", k = 2) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     TransBirdsData \nDistance: bray \n\nDimensions: 2 \nStress:     0.1112209 \nStress type 1, weak ties\nBest solution was repeated 12 times in 20 tries\nThe best solution was from try 0 (metric scaling or null solution)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'TransBirdsData' \n\n\n\n\n\nLet’s do an ANOSIM, using Habitat as the Factor:\n\nBird.anosim &lt;- with(TransBirdsData, anosim(Bird.dis, Birds$HABITAT))\nsummary(Bird.anosim)\n\n\nCall:\nanosim(x = Bird.dis, grouping = Birds$HABITAT) \nDissimilarity: bray \n\nANOSIM statistic R: 0.3936 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.135 0.179 0.223 0.264 \n\nDissimilarity ranks between and within classes:\n             0%    25%   50%    75% 100%   N\nBetween       1 224.25 380.5 520.25  665 500\nBox-Ironbark  6  22.50  29.5  45.50   78   6\nFoothills Wo 10  47.00  54.0  71.50  145   6\nGippsland Ma 51  72.25  90.0  95.00  130   6\nMixed         4 124.00 220.5 410.25  666 136\nMontane Fore  3  17.75  54.5  87.50   97   6\nRiver Red Gu 12  33.75  96.5 179.50  227   6\n\n\n\n\n\nLet’s do a PERMANOVA, with HABITAT as a factor. PERMANOVA is called adonis in the vegan package. Note adonis is being replaced by adonis2, which is basically the same.\n\nadonis2(TransBirdsData ~ HABITAT, data = Birds, permutations = 999)\n\nPermutation test for adonis under reduced model\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = TransBirdsData ~ HABITAT, data = Birds, permutations = 999)\n         Df SumOfSqs      R2      F Pr(&gt;F)    \nModel     5   2.2749 0.50098 6.2245  0.001 ***\nResidual 31   2.2660 0.49902                  \nTotal    36   4.5409 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nIf we find an overall significant p-value, we need to see which level is significant from which other level. Here we can do pairwise tests (a bit like post-hoc tests in ANOVAs), with the p-value adjusted with a Bonferroni correction.\nTo do the pairwise tests, there is a new R command on GitHub. Note the first time you do this, you need to first install Rtools from here.\n\nlibrary(devtools)\ninstall_github(\"pmartinezarbizu/pairwiseAdonis/pairwiseAdonis\")\n\n\nlibrary(pairwiseAdonis)\npairwise.adonis(TransBirdsData, Birds$HABITAT)\n\n\n\n\nTo see what species are contributing to the differences between the vegetation communities, we can use a SIMPER analysis (Similarity Percentages):\n\nBirdsim &lt;- simper(TransBirdsData, Birds$HABITAT)\nsummary(Birdsim)\n\n\nContrast: Mixed_Gippsland Ma \n\n          average       sd    ratio      ava      avb cumsum     p    \nV17WPHE  0.019779 0.011557 1.711400 0.602000 2.247700  0.040 0.001 ***\nV34WSW   0.018979 0.007112 2.668500 0.247500 1.856000  0.079 0.001 ***\nV11CR    0.016850 0.007496 2.247800 1.443400 0.000000  0.114 0.001 ***\nV6WTTR   0.015456 0.006359 2.430500 1.325800 0.000000  0.145 0.001 ***\nV15STTH  0.014940 0.009012 1.657800 1.475400 0.323400  0.176 0.002 ** \nV25WAG   0.013966 0.005802 2.407300 0.205100 1.376000  0.204 0.001 ***\nV27NHHE  0.012769 0.008435 1.513800 0.689200 1.646300  0.231 0.029 *  \nV3GF     0.012483 0.008488 1.470700 1.569300 0.646500  0.256 0.002 ** \nV4BTH    0.011472 0.009944 1.153700 1.946400 1.184200  0.280 0.096 .  \nV36YFHE  0.010708 0.008006 1.337500 1.121400 1.243700  0.302 0.100 .  \nV42SIL   0.010651 0.008608 1.237400 0.952800 0.420400  0.323 0.021 *  \nV13RWB   0.010585 0.007337 1.442700 1.080500 1.922100  0.345 0.133    \nV33RWH   0.010302 0.006859 1.501900 1.318700 0.579800  0.366 0.026 *  \nV5GWH    0.010203 0.008334 1.224100 1.325200 0.674800  0.387 0.004 ** \nV16LR    0.009557 0.008123 1.176500 0.852900 1.184900  0.407 0.117    \nV65LWB   0.009521 0.009391 1.013900 0.258700 0.741500  0.426 0.038 *  \nV21ESP   0.009484 0.007739 1.225400 1.236300 0.644000  0.445 0.139    \nV23RBFT  0.009142 0.006803 1.343900 0.990600 0.937700  0.464 0.241    \nV63DWS   0.008333 0.007483 1.113600 0.313100 0.724800  0.481 0.021 *  \nV35STP   0.008318 0.007536 1.103700 0.700100 0.993200  0.498 0.356    \nV7WEHE   0.008195 0.007490 1.094100 1.252500 1.241500  0.515 0.140    \nV91NMIN  0.008158 0.007918 1.030300 0.103300 0.719700  0.532 0.025 *  \nV19ER    0.007905 0.006671 1.184900 0.816000 1.308600  0.548 0.394    \nV29CST   0.007715 0.005456 1.414100 0.548200 1.059100  0.564 0.075 .  \nV56FTC   0.007610 0.006779 1.122500 0.948600 0.615700  0.579 0.105    \nV14AUR   0.007440 0.007024 1.059200 0.893600 0.962100  0.594 0.169    \nV81SHBC  0.007124 0.006361 1.120000 0.641600 0.271900  0.609 0.108    \nV28VS    0.007070 0.008926 0.792000 0.599300 0.000000  0.624 0.926    \nV12LK    0.006733 0.005709 1.179300 1.372500 0.801000  0.637 0.011 *  \nV80RF    0.006689 0.007273 0.919800 0.568000 0.000000  0.651 0.251    \nV50CHE   0.006645 0.008280 0.802400 0.580200 0.000000  0.664 0.355    \nV46BHHE  0.006634 0.006911 0.959900 0.491200 0.307900  0.678 0.960    \nV9SFW    0.006024 0.006604 0.912200 1.384400 1.734700  0.690 0.373    \nV48YTBC  0.005842 0.007167 0.815200 0.511500 0.000000  0.702 0.383    \nV41SPP   0.005728 0.005620 1.019200 1.058800 1.200500  0.714 0.716    \nV8WNHE   0.005294 0.006131 0.863400 1.588600 1.843200  0.725 0.820    \nV24BFCS  0.005165 0.005210 0.991300 0.751600 1.112500  0.736 0.535    \nV37WHIP  0.005130 0.007087 0.723800 0.441900 0.000000  0.746 0.522    \nV83SFC   0.005026 0.007077 0.710200 0.424600 0.000000  0.756 0.700    \nV43GCU   0.004955 0.006908 0.717300 0.423100 0.000000  0.766 0.966    \nV32SCC   0.004898 0.010846 0.451600 0.441300 0.000000  0.776 0.798    \nV20PCU   0.004806 0.006607 0.727400 0.411800 0.000000  0.786 0.948    \nV38GAL   0.004767 0.007107 0.670700 0.178700 0.323400  0.796 0.564    \nV87LFC   0.004719 0.006574 0.717800 0.224900 0.289600  0.806 0.141    \nV10WBSW  0.004538 0.005738 0.791000 1.426000 1.760400  0.815 0.998    \nV2EYR    0.004523 0.005813 0.778100 1.285300 1.599500  0.824 0.890    \nV53MB    0.004509 0.006393 0.705200 0.224200 0.250000  0.833 0.444    \nV31AMAG  0.004040 0.007500 0.538600 0.370900 0.000000  0.842 0.770    \nV55LHE   0.004024 0.006359 0.632900 0.354900 0.000000  0.850 0.512    \nV45MGLK  0.003945 0.005906 0.668000 0.142600 0.271900  0.858 0.505    \nV69SKF   0.003692 0.005819 0.634500 0.334500 0.000000  0.866 0.953    \nV70RSL   0.003469 0.005750 0.603200 0.094600 0.236400  0.873 0.676    \nV49LYRE  0.003229 0.005942 0.543500 0.268000 0.000000  0.879 0.683    \nV85ROSE  0.002816 0.005168 0.545000 0.239700 0.000000  0.885 0.466    \nV39FHE   0.002732 0.007692 0.355200 0.248500 0.000000  0.891 0.848    \nV68PIL   0.002684 0.005897 0.455200 0.232900 0.000000  0.896 0.677    \nV44MUSK  0.002563 0.007101 0.361000 0.236300 0.000000  0.901 0.757    \nV98FLAME 0.002557 0.005598 0.456700 0.220500 0.000000  0.907 0.691    \nV51OWH   0.002408 0.005315 0.453100 0.211500 0.000000  0.911 0.401    \nV1GST    0.002323 0.001704 1.363300 1.466600 1.493300  0.916 0.327    \nV22SCR   0.002318 0.005111 0.453500 0.210400 0.000000  0.921 0.999    \nV52TRM   0.002247 0.006427 0.349700 0.202800 0.000000  0.926 0.692    \nV47RFC   0.002229 0.004899 0.455000 0.214300 0.000000  0.930 0.866    \nV30BTR   0.002214 0.006274 0.352900 0.200700 0.000000  0.935 0.942    \nV58OBO   0.002124 0.004652 0.456500 0.198800 0.000000  0.939 0.987    \nV26WWCH  0.002007 0.005550 0.361500 0.180300 0.000000  0.943 1.000    \nV77RRP   0.001900 0.005290 0.359200 0.174100 0.000000  0.947 0.202    \nV76GBB   0.001874 0.004111 0.455800 0.166700 0.000000  0.951 0.593    \nV18YTH   0.001726 0.004872 0.354300 0.171800 0.000000  0.954 1.000    \nV86BCOO  0.001595 0.004421 0.360800 0.142300 0.000000  0.958 0.587    \nV67GGC   0.001558 0.004376 0.356000 0.135400 0.000000  0.961 0.780    \nV64BELL  0.001553 0.006272 0.247500 0.127500 0.000000  0.964 0.421    \nV62RBTR  0.001522 0.004260 0.357300 0.127400 0.000000  0.967 0.587    \nV57PINK  0.001480 0.004113 0.359800 0.127300 0.000000  0.970 0.409    \nV92NFB   0.001412 0.003904 0.361800 0.141400 0.000000  0.973 0.595    \nV97PCL   0.001237 0.004997 0.247500 0.102200 0.000000  0.976 0.095 .  \nV40BRTH  0.001151 0.004652 0.247400 0.088800 0.000000  0.978 0.995    \nV78LLOR  0.001038 0.004191 0.247800 0.106900 0.000000  0.980 0.119    \nV60LFB   0.000929 0.003754 0.247500 0.076800 0.000000  0.982 0.946    \nV84YRTH  0.000851 0.003434 0.247700 0.082700 0.000000  0.984 0.609    \nV89PCOO  0.000836 0.003378 0.247500 0.069100 0.000000  0.985 0.095 .  \nV54STHR  0.000766 0.003093 0.247700 0.073200 0.000000  0.987 0.827    \nV66CBW   0.000735 0.002968 0.247500 0.058800 0.000000  0.989 0.875    \nV79YTHE  0.000733 0.002957 0.247800 0.075400 0.000000  0.990 0.654    \nV96DF    0.000688 0.002777 0.247800 0.070800 0.000000  0.991 0.763    \nV82AZKF  0.000680 0.002747 0.247500 0.053800 0.000000  0.993 0.102    \nV100WBWS 0.000643 0.002594 0.247800 0.066200 0.000000  0.994 0.119    \nV93DB    0.000634 0.002557 0.247700 0.061600 0.000000  0.995 0.927    \nV102KING 0.000608 0.002454 0.247600 0.053800 0.000000  0.997 0.422    \nV90WTG   0.000572 0.002311 0.247700 0.055600 0.000000  0.998 0.957    \nV71PDOV  0.000540 0.002181 0.247800 0.055600 0.000000  0.999 0.789    \nV94RBEE  0.000523 0.002110 0.247800 0.053800 0.000000  1.000 0.978    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV61SPW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV88WG    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV95HBC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Mixed_Montane Fore \n\n          average       sd    ratio      ava      avb cumsum     p    \nV13RWB   0.009338 0.006981 1.337700 1.080500 0.662400  0.025 0.512    \nV43GCU   0.009042 0.005499 1.644400 0.423100 1.256100  0.049 0.015 *  \nV83SFC   0.008981 0.004949 1.814500 0.424600 1.220000  0.073 0.018 *  \nV20PCU   0.008970 0.005919 1.515400 0.411800 1.268100  0.097 0.019 *  \nV67GGC   0.008783 0.005683 1.545500 0.135400 0.926600  0.121 0.005 ** \nV16LR    0.008555 0.006520 1.312100 0.852900 1.023200  0.144 0.350    \nV46BHHE  0.008352 0.007086 1.178600 0.491200 0.828800  0.166 0.535    \nV33RWH   0.008286 0.006874 1.205500 1.318700 0.753700  0.188 0.210    \nV37WHIP  0.008217 0.006567 1.251200 0.441900 1.020400  0.211 0.039 *  \nV23RBFT  0.008056 0.006201 1.299200 0.990600 0.883700  0.232 0.729    \nV36YFHE  0.008026 0.007167 1.120000 1.121400 1.134500  0.254 0.865    \nV28VS    0.007929 0.007261 1.091900 0.599300 0.709200  0.275 0.613    \nV19ER    0.007909 0.006742 1.173100 0.816000 0.266900  0.296 0.374    \nV48YTBC  0.007759 0.006195 1.252500 0.511500 0.958400  0.317 0.054 .  \nV54STHR  0.007620 0.004661 1.634900 0.073200 0.758900  0.337 0.001 ***\nV68PIL   0.007514 0.004974 1.510900 0.232900 0.785200  0.358 0.015 *  \nV35STP   0.007322 0.006426 1.139400 0.700100 0.944200  0.377 0.776    \nV27NHHE  0.007298 0.008654 0.843300 0.689200 0.000000  0.397 0.673    \nV49LYRE  0.007170 0.005092 1.408100 0.268000 0.803100  0.416 0.021 *  \nV50CHE   0.006930 0.005950 1.164700 0.580200 0.509600  0.435 0.284    \nV80RF    0.006902 0.006031 1.144500 0.568000 0.913500  0.453 0.178    \nV42SIL   0.006868 0.005994 1.145900 0.952800 1.363600  0.472 0.969    \nV14AUR   0.006236 0.005868 1.062700 0.893600 0.919800  0.488 0.414    \nV55LHE   0.006136 0.005947 1.031900 0.354900 0.590500  0.505 0.115    \nV29CST   0.006120 0.006185 0.989500 0.548200 0.314400  0.521 0.880    \nV24BFCS  0.006092 0.005192 1.173200 0.751600 0.496700  0.537 0.259    \nV26WWCH  0.006052 0.009091 0.665800 0.180300 0.462400  0.554 0.972    \nV17WPHE  0.005965 0.009471 0.629800 0.602000 0.000000  0.570 0.991    \nV81SHBC  0.005961 0.005479 1.088100 0.641600 1.191300  0.586 0.883    \nV8WNHE   0.005525 0.005093 1.084800 1.588600 1.709100  0.601 0.778    \nV21ESP   0.005370 0.004726 1.136100 1.236300 1.502100  0.615 0.998    \nV15STTH  0.005290 0.006652 0.795300 1.475400 1.857500  0.629 0.947    \nV86BCOO  0.005217 0.005078 1.027400 0.142300 0.481700  0.643 0.054 .  \nV11CR    0.004834 0.005601 0.863000 1.443400 1.710400  0.656 1.000    \nV41SPP   0.004678 0.005784 0.808700 1.058800 1.352200  0.669 0.873    \nV22SCR   0.004598 0.006404 0.718100 0.210400 0.317500  0.681 0.923    \nV85ROSE  0.004568 0.005803 0.787200 0.239700 0.331700  0.693 0.087 .  \nV9SFW    0.004450 0.004930 0.902600 1.384400 1.525000  0.705 0.806    \nV32SCC   0.004345 0.009627 0.451300 0.441300 0.000000  0.717 0.854    \nV4BTH    0.004124 0.005531 0.745600 1.946400 2.294000  0.728 0.998    \nV56FTC   0.004054 0.005016 0.808300 0.948600 1.231300  0.739 0.990    \nV6WTTR   0.003848 0.005024 0.765900 1.325800 1.567100  0.749 0.974    \nV62RBTR  0.003800 0.005744 0.661600 0.127400 0.281200  0.759 0.122    \nV7WEHE   0.003646 0.003640 1.001600 1.252500 1.224500  0.769 0.990    \nV31AMAG  0.003590 0.006655 0.539500 0.370900 0.000000  0.779 0.824    \nV51OWH   0.003548 0.004941 0.718000 0.211500 0.220000  0.788 0.258    \nV5GWH    0.003405 0.004259 0.799600 1.325200 1.486100  0.797 0.954    \nV2EYR    0.003300 0.004546 0.725800 1.285300 1.440100  0.806 0.992    \nV69SKF   0.003277 0.005158 0.635200 0.334500 0.000000  0.815 0.981    \nV57PINK  0.003208 0.004800 0.668200 0.127300 0.250000  0.824 0.146    \nV76GBB   0.003203 0.004482 0.714700 0.166700 0.220000  0.832 0.177    \nV63DWS   0.003119 0.004987 0.625500 0.313100 0.000000  0.840 0.837    \nV10WBSW  0.002993 0.004868 0.614800 1.426000 1.622600  0.849 1.000    \nV3GF     0.002942 0.004175 0.704800 1.569300 1.724600  0.856 0.987    \nV65LWB   0.002877 0.008006 0.359400 0.258700 0.000000  0.864 0.599    \nV102KING 0.002549 0.004128 0.617500 0.053800 0.236400  0.871 0.088 .  \nV39FHE   0.002424 0.006810 0.356000 0.248500 0.000000  0.877 0.880    \nV34WSW   0.002385 0.005233 0.455700 0.247500 0.000000  0.884 0.996    \nV53MB    0.002366 0.005345 0.442500 0.224200 0.000000  0.890 0.897    \nV44MUSK  0.002278 0.006306 0.361200 0.236300 0.000000  0.896 0.802    \nV98FLAME 0.002256 0.004937 0.457000 0.220500 0.000000  0.902 0.802    \nV87LFC   0.002212 0.004906 0.450900 0.224900 0.000000  0.908 0.708    \nV52TRM   0.001993 0.005682 0.350700 0.202800 0.000000  0.914 0.744    \nV47RFC   0.001992 0.004375 0.455300 0.214300 0.000000  0.919 0.914    \nV30BTR   0.001964 0.005551 0.353800 0.200700 0.000000  0.924 0.947    \nV25WAG   0.001953 0.004286 0.455700 0.205100 0.000000  0.929 0.997    \nV58OBO   0.001891 0.004139 0.456800 0.198800 0.000000  0.934 0.987    \nV1GST    0.001754 0.001220 1.437400 1.466600 1.379200  0.939 0.874    \nV38GAL   0.001728 0.004792 0.360600 0.178700 0.000000  0.944 0.971    \nV77RRP   0.001687 0.004691 0.359700 0.174100 0.000000  0.948 0.377    \nV18YTH   0.001548 0.004368 0.354500 0.171800 0.000000  0.953 1.000    \nV45MGLK  0.001409 0.003896 0.361600 0.142600 0.000000  0.956 0.981    \nV64BELL  0.001363 0.005506 0.247500 0.127500 0.000000  0.960 0.584    \nV92NFB   0.001268 0.003504 0.361700 0.141400 0.000000  0.963 0.765    \nV12LK    0.001220 0.000891 1.368900 1.372500 1.324100  0.967 0.998    \nV97PCL   0.001087 0.004390 0.247500 0.102200 0.000000  0.970 0.477    \nV70RSL   0.001006 0.004066 0.247500 0.094600 0.000000  0.972 0.988    \nV40BRTH  0.001003 0.004051 0.247500 0.088800 0.000000  0.975 1.000    \nV91NMIN  0.000978 0.002735 0.357500 0.103300 0.000000  0.978 0.966    \nV78LLOR  0.000935 0.003773 0.247700 0.106900 0.000000  0.980 0.514    \nV60LFB   0.000816 0.003298 0.247500 0.076800 0.000000  0.982 0.953    \nV84YRTH  0.000761 0.003073 0.247700 0.082700 0.000000  0.984 0.791    \nV89PCOO  0.000734 0.002967 0.247500 0.069100 0.000000  0.986 0.477    \nV79YTHE  0.000659 0.002662 0.247700 0.075400 0.000000  0.988 0.815    \nV66CBW   0.000643 0.002597 0.247500 0.058800 0.000000  0.990 0.927    \nV96DF    0.000619 0.002499 0.247700 0.070800 0.000000  0.991 0.821    \nV82AZKF  0.000594 0.002400 0.247500 0.053800 0.000000  0.993 0.479    \nV100WBWS 0.000579 0.002335 0.247700 0.066200 0.000000  0.995 0.514    \nV93DB    0.000567 0.002289 0.247700 0.061600 0.000000  0.996 0.964    \nV90WTG   0.000512 0.002068 0.247700 0.055600 0.000000  0.997 0.987    \nV71PDOV  0.000486 0.001964 0.247700 0.055600 0.000000  0.999 0.888    \nV94RBEE  0.000470 0.001899 0.247700 0.053800 0.000000  1.000 0.984    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV61SPW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV88WG    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV95HBC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Mixed_Foothills Wo \n\n          average       sd    ratio      ava      avb cumsum     p   \nV40BRTH  0.014392 0.009230 1.559200 0.088800 1.423800  0.037 0.006 **\nV22SCR   0.012437 0.004863 2.557800 0.210400 1.364500  0.068 0.002 **\nV26WWCH  0.011907 0.007887 1.509700 0.180300 1.210300  0.099 0.021 * \nV18YTH   0.011886 0.007877 1.508900 0.171800 1.199300  0.129 0.039 * \nV10WBSW  0.009520 0.007570 1.257700 1.426000 0.773600  0.154 0.236   \nV23RBFT  0.009458 0.007442 1.270900 0.990600 0.243500  0.178 0.185   \nV28VS    0.009159 0.007107 1.288800 0.599300 1.088900  0.201 0.130   \nV53MB    0.008400 0.005483 1.532100 0.224200 0.851600  0.222 0.010 **\nV46BHHE  0.008159 0.006034 1.352200 0.491200 0.931700  0.243 0.638   \nV16LR    0.008067 0.006758 1.193700 0.852900 1.479800  0.264 0.534   \nV19ER    0.007778 0.006808 1.142400 0.816000 0.654600  0.284 0.438   \nV69SKF   0.007708 0.006107 1.262200 0.334500 0.890600  0.304 0.036 * \nV35STP   0.007702 0.006775 1.136900 0.700100 1.011200  0.323 0.643   \nV42SIL   0.007673 0.005328 1.440100 0.952800 0.761200  0.343 0.813   \nV27NHHE  0.007657 0.007842 0.976500 0.689200 0.256000  0.362 0.610   \nV50CHE   0.007225 0.007685 0.940200 0.580200 0.384600  0.381 0.227   \nV36YFHE  0.007150 0.005676 1.259600 1.121400 1.319700  0.399 0.980   \nV29CST   0.007090 0.006413 1.105700 0.548200 0.659900  0.417 0.289   \nV20PCU   0.007080 0.006583 1.075500 0.411800 0.673000  0.435 0.188   \nV9SFW    0.007048 0.006634 1.062400 1.384400 1.081900  0.453 0.190   \nV14AUR   0.006603 0.006170 1.070200 0.893600 0.987000  0.470 0.299   \nV43GCU   0.006573 0.006112 1.075300 0.423100 0.583000  0.487 0.485   \nV80RF    0.006348 0.006410 0.990200 0.568000 0.304500  0.503 0.380   \nV13RWB   0.006090 0.004647 1.310500 1.080500 1.170600  0.519 0.999   \nV17WPHE  0.006088 0.009642 0.631400 0.602000 0.000000  0.534 0.988   \nV64BELL  0.005858 0.009467 0.618800 0.127500 0.492000  0.549 0.144   \nV81SHBC  0.005798 0.005361 1.081400 0.641600 0.807100  0.564 0.929   \nV15STTH  0.005476 0.006465 0.847000 1.475400 1.788200  0.578 0.945   \nV83SFC   0.005382 0.006098 0.882700 0.424600 0.261700  0.592 0.588   \nV24BFCS  0.005280 0.004956 1.065500 0.751600 0.794400  0.605 0.499   \nV41SPP   0.005277 0.006727 0.784500 1.058800 1.526000  0.619 0.827   \nV48YTBC  0.005274 0.006448 0.817800 0.511500 0.000000  0.632 0.552   \nV8WNHE   0.005251 0.004942 1.062400 1.588600 1.582200  0.645 0.833   \nV11CR    0.004856 0.004784 1.015100 1.443400 1.484400  0.658 0.998   \nV21ESP   0.004753 0.004815 0.987100 1.236300 1.346100  0.670 0.998   \nV37WHIP  0.004623 0.006367 0.726100 0.441900 0.000000  0.682 0.681   \nV32SCC   0.004434 0.009801 0.452400 0.441300 0.000000  0.693 0.857   \nV63DWS   0.004331 0.005235 0.827300 0.313100 0.243500  0.704 0.579   \nV7WEHE   0.004274 0.004093 1.044200 1.252500 1.318200  0.715 0.970   \nV56FTC   0.004240 0.005159 0.821900 0.948600 1.244700  0.726 0.993   \nV33RWH   0.004130 0.005045 0.818600 1.318700 1.597500  0.736 0.993   \nV4BTH    0.004096 0.004957 0.826300 1.946400 1.975000  0.747 0.998   \nV96DF    0.003970 0.006435 0.617000 0.070800 0.349000  0.757 0.078 . \nV58OBO   0.003798 0.005307 0.715600 0.198800 0.261700  0.767 0.804   \nV6WTTR   0.003785 0.005196 0.728400 1.325800 1.579900  0.776 0.977   \nV2EYR    0.003733 0.004281 0.872000 1.285300 1.316800  0.786 0.969   \nV31AMAG  0.003663 0.006775 0.540700 0.370900 0.000000  0.795 0.824   \nV55LHE   0.003636 0.005729 0.634700 0.354900 0.000000  0.804 0.588   \nV5GWH    0.003634 0.003729 0.974400 1.325200 1.309900  0.814 0.927   \nV61SPW   0.003414 0.005979 0.571000 0.000000 0.326200  0.823 0.206   \nV3GF     0.003253 0.003748 0.868100 1.569300 1.569600  0.831 0.971   \nV62RBTR  0.003142 0.004683 0.670900 0.127400 0.220000  0.839 0.279   \nV65LWB   0.002942 0.008165 0.360300 0.258700 0.000000  0.846 0.618   \nV49LYRE  0.002899 0.005317 0.545200 0.268000 0.000000  0.854 0.704   \nV66CBW   0.002697 0.004351 0.619700 0.058800 0.220000  0.861 0.401   \nV85ROSE  0.002535 0.004640 0.546400 0.239700 0.000000  0.867 0.549   \nV39FHE   0.002474 0.006936 0.356600 0.248500 0.000000  0.874 0.870   \nV34WSW   0.002433 0.005328 0.456700 0.247500 0.000000  0.880 0.993   \nV1GST    0.002432 0.002103 1.156600 1.466600 1.577800  0.886 0.226   \nV68PIL   0.002421 0.005305 0.456500 0.232900 0.000000  0.892 0.704   \nV44MUSK  0.002324 0.006421 0.361900 0.236300 0.000000  0.898 0.785   \nV98FLAME 0.002304 0.005029 0.458100 0.220500 0.000000  0.904 0.767   \nV87LFC   0.002258 0.004995 0.452000 0.224900 0.000000  0.910 0.644   \nV51OWH   0.002175 0.004792 0.453900 0.211500 0.000000  0.915 0.440   \nV52TRM   0.002033 0.005787 0.351300 0.202800 0.000000  0.920 0.769   \nV47RFC   0.002030 0.004451 0.456200 0.214300 0.000000  0.926 0.915   \nV30BTR   0.002004 0.005654 0.354500 0.200700 0.000000  0.931 0.945   \nV25WAG   0.001992 0.004362 0.456700 0.205100 0.000000  0.936 0.996   \nV38GAL   0.001763 0.004879 0.361300 0.178700 0.000000  0.940 0.972   \nV77RRP   0.001722 0.004777 0.360400 0.174100 0.000000  0.945 0.379   \nV76GBB   0.001693 0.003703 0.457300 0.166700 0.000000  0.949 0.666   \nV86BCOO  0.001443 0.003991 0.361600 0.142300 0.000000  0.953 0.631   \nV45MGLK  0.001438 0.003967 0.362300 0.142600 0.000000  0.956 0.967   \nV12LK    0.001432 0.001053 1.360800 1.372500 1.350100  0.960 0.982   \nV67GGC   0.001406 0.003942 0.356500 0.135400 0.000000  0.964 0.868   \nV57PINK  0.001334 0.003700 0.360500 0.127300 0.000000  0.967 0.485   \nV92NFB   0.001291 0.003564 0.362400 0.141400 0.000000  0.970 0.752   \nV97PCL   0.001110 0.004475 0.248100 0.102200 0.000000  0.973 0.419   \nV70RSL   0.001028 0.004145 0.248100 0.094600 0.000000  0.976 0.991   \nV91NMIN  0.000997 0.002784 0.358300 0.103300 0.000000  0.978 0.975   \nV78LLOR  0.000952 0.003836 0.248100 0.106900 0.000000  0.981 0.423   \nV60LFB   0.000834 0.003362 0.248100 0.076800 0.000000  0.983 0.963   \nV84YRTH  0.000776 0.003127 0.248100 0.082700 0.000000  0.985 0.754   \nV89PCOO  0.000750 0.003025 0.248100 0.069100 0.000000  0.987 0.419   \nV54STHR  0.000698 0.002812 0.248100 0.073200 0.000000  0.989 0.855   \nV79YTHE  0.000672 0.002706 0.248100 0.075400 0.000000  0.990 0.759   \nV82AZKF  0.000607 0.002448 0.248100 0.053800 0.000000  0.992 0.414   \nV100WBWS 0.000589 0.002375 0.248100 0.066200 0.000000  0.993 0.422   \nV93DB    0.000578 0.002329 0.248100 0.061600 0.000000  0.995 0.961   \nV102KING 0.000549 0.002214 0.248100 0.053800 0.000000  0.996 0.597   \nV90WTG   0.000522 0.002104 0.248100 0.055600 0.000000  0.998 0.972   \nV71PDOV  0.000495 0.001997 0.248100 0.055600 0.000000  0.999 0.894   \nV94RBEE  0.000479 0.001931 0.248100 0.053800 0.000000  1.000 0.988   \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV88WG    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV95HBC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Mixed_Box-Ironbark \n\n          average       sd    ratio      ava      avb cumsum     p    \nV18YTH   0.018658 0.005852 3.188000 0.171800 1.909900  0.039 0.001 ***\nV26WWCH  0.015902 0.005536 2.872000 0.180300 1.655500  0.072 0.001 ***\nV10WBSW  0.015288 0.006045 2.529000 1.426000 0.000000  0.103 0.002 ** \nV39FHE   0.014967 0.004566 3.278000 0.248500 1.532700  0.134 0.001 ***\nV46BHHE  0.014098 0.007088 1.989000 0.491200 1.798900  0.163 0.001 ***\nV40BRTH  0.013699 0.008353 1.640000 0.088800 1.379300  0.192 0.007 ** \nV21ESP   0.013293 0.006583 2.019000 1.236300 0.000000  0.219 0.002 ** \nV22SCR   0.012484 0.005255 2.376000 0.210400 1.350800  0.245 0.002 ** \nV44MUSK  0.011843 0.007846 1.509000 0.236300 1.180800  0.269 0.006 ** \nV75RCR   0.010748 0.003075 3.495000 0.000000 1.000900  0.292 0.001 ***\nV58OBO   0.009787 0.004514 2.168000 0.198800 1.059200  0.312 0.003 ** \nV23RBFT  0.009490 0.007770 1.221000 0.990600 0.320500  0.332 0.181    \nV13RWB   0.008878 0.006402 1.387000 1.080500 1.792200  0.350 0.691    \nV56FTC   0.008670 0.006269 1.383000 0.948600 0.281200  0.368 0.030 *  \nV28VS    0.008644 0.006295 1.373000 0.599300 0.971800  0.386 0.264    \nV11CR    0.008437 0.007081 1.192000 1.443400 0.992900  0.403 0.736    \nV36YFHE  0.008272 0.006141 1.347000 1.121400 1.383500  0.420 0.825    \nV16LR    0.008079 0.006977 1.158000 0.852900 0.576500  0.437 0.511    \nV35STP   0.007717 0.006654 1.160000 0.700100 0.656700  0.453 0.648    \nV27NHHE  0.007497 0.008904 0.842000 0.689200 0.000000  0.468 0.635    \nV19ER    0.007343 0.005945 1.235000 0.816000 0.846400  0.484 0.652    \nV43GCU   0.007068 0.005570 1.269000 0.423100 0.818400  0.498 0.289    \nV42SIL   0.006994 0.005548 1.261000 0.952800 1.255000  0.513 0.948    \nV4BTH    0.006788 0.003994 1.699000 1.946400 1.520900  0.527 0.831    \nV2EYR    0.006709 0.005999 1.118000 1.285300 0.874900  0.541 0.402    \nV81SHBC  0.006489 0.005846 1.110000 0.641600 0.281200  0.554 0.416    \nV66CBW   0.006361 0.006379 0.997000 0.058800 0.566600  0.567 0.010 ** \nV70RSL   0.006278 0.006199 1.013000 0.094600 0.557400  0.580 0.144    \nV17WPHE  0.006117 0.009722 0.629000 0.602000 0.000000  0.593 0.984    \nV80RF    0.006050 0.006578 0.920000 0.568000 0.000000  0.605 0.498    \nV90WTG   0.006046 0.006464 0.935000 0.055600 0.542000  0.618 0.047 *  \nV50CHE   0.006025 0.007514 0.802000 0.580200 0.000000  0.630 0.485    \nV29CST   0.005983 0.006099 0.981000 0.548200 0.256000  0.643 0.929    \nV61SPW   0.005859 0.005982 0.980000 0.000000 0.593000  0.655 0.024 *  \nV14AUR   0.005388 0.005537 0.973000 0.893600 1.211600  0.666 0.717    \nV48YTBC  0.005300 0.006506 0.815000 0.511500 0.000000  0.677 0.562    \nV15STTH  0.005211 0.006022 0.865000 1.475400 1.627000  0.688 0.956    \nV9SFW    0.005137 0.005014 1.024000 1.384400 1.472800  0.698 0.586    \nV41SPP   0.004977 0.006330 0.786000 1.058800 1.440100  0.709 0.826    \nV30BTR   0.004843 0.007338 0.660000 0.200700 0.382900  0.719 0.539    \nV5GWH    0.004733 0.003669 1.290000 1.325200 1.171900  0.729 0.706    \nV3GF     0.004725 0.003539 1.335000 1.569300 1.330200  0.738 0.691    \nV8WNHE   0.004707 0.005030 0.936000 1.588600 1.595400  0.748 0.923    \nV37WHIP  0.004647 0.006424 0.723000 0.441900 0.000000  0.758 0.690    \nV83SFC   0.004543 0.006404 0.710000 0.424600 0.000000  0.767 0.826    \nV24BFCS  0.004457 0.004309 1.034000 0.751600 1.014400  0.776 0.803    \nV32SCC   0.004454 0.009873 0.451000 0.441300 0.000000  0.785 0.839    \nV20PCU   0.004351 0.005983 0.727000 0.411800 0.000000  0.794 0.980    \nV79YTHE  0.004127 0.006650 0.621000 0.075400 0.396100  0.803 0.029 *  \nV63DWS   0.004112 0.005037 0.816000 0.313100 0.220000  0.811 0.571    \nV7WEHE   0.004030 0.003983 1.012000 1.252500 1.296700  0.820 0.976    \nV33RWH   0.003758 0.004830 0.778000 1.318700 1.512200  0.828 0.996    \nV6WTTR   0.003705 0.004996 0.741000 1.325800 1.525100  0.835 0.979    \nV31AMAG  0.003680 0.006826 0.539000 0.370900 0.000000  0.843 0.818    \nV55LHE   0.003654 0.005776 0.633000 0.354900 0.000000  0.850 0.602    \nV69SKF   0.003359 0.005292 0.635000 0.334500 0.000000  0.857 0.977    \nV76GBB   0.003230 0.004486 0.720000 0.166700 0.210200  0.864 0.173    \nV65LWB   0.002960 0.008240 0.359000 0.258700 0.000000  0.870 0.603    \nV49LYRE  0.002915 0.005365 0.543000 0.268000 0.000000  0.876 0.705    \nV96DF    0.002818 0.004531 0.622000 0.070800 0.256000  0.882 0.205    \nV12LK    0.002704 0.001574 1.718000 1.372500 1.122800  0.888 0.590    \nV94RBEE  0.002692 0.004346 0.620000 0.053800 0.256000  0.893 0.418    \nV85ROSE  0.002548 0.004679 0.545000 0.239700 0.000000  0.898 0.540    \nV34WSW   0.002444 0.005368 0.455000 0.247500 0.000000  0.903 0.992    \nV68PIL   0.002434 0.005347 0.455000 0.232900 0.000000  0.908 0.732    \nV53MB    0.002430 0.005496 0.442000 0.224200 0.000000  0.914 0.885    \nV1GST    0.002363 0.001590 1.487000 1.466600 1.425700  0.918 0.277    \nV98FLAME 0.002315 0.005070 0.457000 0.220500 0.000000  0.923 0.801    \nV87LFC   0.002268 0.005033 0.451000 0.224900 0.000000  0.928 0.654    \nV51OWH   0.002186 0.004829 0.453000 0.211500 0.000000  0.932 0.453    \nV88WG    0.002185 0.003827 0.571000 0.000000 0.210200  0.937 0.286    \nV95HBC   0.002185 0.003827 0.571000 0.000000 0.210200  0.942 0.171    \nV52TRM   0.002043 0.005832 0.350000 0.202800 0.000000  0.946 0.747    \nV47RFC   0.002039 0.004481 0.455000 0.214300 0.000000  0.950 0.901    \nV25WAG   0.002001 0.004394 0.455000 0.205100 0.000000  0.954 0.997    \nV38GAL   0.001771 0.004914 0.360000 0.178700 0.000000  0.958 0.968    \nV77RRP   0.001730 0.004812 0.359000 0.174100 0.000000  0.961 0.301    \nV86BCOO  0.001450 0.004021 0.361000 0.142300 0.000000  0.964 0.645    \nV45MGLK  0.001444 0.003997 0.361000 0.142600 0.000000  0.967 0.966    \nV67GGC   0.001413 0.003972 0.356000 0.135400 0.000000  0.970 0.867    \nV64BELL  0.001400 0.005660 0.247000 0.127500 0.000000  0.973 0.549    \nV62RBTR  0.001374 0.003843 0.358000 0.127400 0.000000  0.976 0.684    \nV57PINK  0.001341 0.003729 0.359000 0.127300 0.000000  0.979 0.493    \nV92NFB   0.001296 0.003585 0.362000 0.141400 0.000000  0.981 0.709    \nV97PCL   0.001116 0.004512 0.247000 0.102200 0.000000  0.984 0.381    \nV91NMIN  0.001002 0.002802 0.357000 0.103300 0.000000  0.986 0.972    \nV78LLOR  0.000955 0.003858 0.248000 0.106900 0.000000  0.988 0.378    \nV60LFB   0.000839 0.003390 0.247000 0.076800 0.000000  0.990 0.956    \nV84YRTH  0.000779 0.003146 0.248000 0.082700 0.000000  0.991 0.724    \nV89PCOO  0.000755 0.003050 0.247000 0.069100 0.000000  0.993 0.381    \nV54STHR  0.000701 0.002830 0.248000 0.073200 0.000000  0.994 0.871    \nV82AZKF  0.000611 0.002470 0.247000 0.053800 0.000000  0.995 0.366    \nV100WBWS 0.000591 0.002388 0.248000 0.066200 0.000000  0.997 0.378    \nV93DB    0.000580 0.002343 0.248000 0.061600 0.000000  0.998 0.948    \nV102KING 0.000552 0.002230 0.247000 0.053800 0.000000  0.999 0.604    \nV71PDOV  0.000497 0.002008 0.248000 0.055600 0.000000  1.000 0.857    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Mixed_River Red Gu \n\n          average       sd    ratio      ava      avb cumsum     p    \nV4BTH    0.020445 0.006218 3.288000 1.946400 0.000000  0.030 0.001 ***\nV32SCC   0.020280 0.007521 2.697000 0.441300 2.225000  0.060 0.001 ***\nV17WPHE  0.015917 0.009167 1.736000 0.602000 2.034900  0.084 0.028 *  \nV10WBSW  0.015087 0.005860 2.574000 1.426000 0.000000  0.106 0.001 ***\nV11CR    0.015054 0.006597 2.282000 1.443400 0.000000  0.128 0.004 ** \nV38GAL   0.014937 0.005417 2.758000 0.178700 1.582800  0.151 0.002 ** \nV26WWCH  0.014881 0.004857 3.064000 0.180300 1.578000  0.173 0.002 ** \nV8WNHE   0.014481 0.007804 1.856000 1.588600 0.289600  0.194 0.002 ** \nV30BTR   0.014462 0.004730 3.057000 0.200700 1.520300  0.216 0.001 ***\nV2EYR    0.013450 0.005370 2.505000 1.285300 0.000000  0.236 0.001 ***\nV7WEHE   0.013287 0.005547 2.395000 1.252500 0.000000  0.255 0.001 ***\nV60LFB   0.013139 0.004117 3.192000 0.076800 1.312300  0.275 0.001 ***\nV21ESP   0.013118 0.006422 2.043000 1.236300 0.000000  0.294 0.001 ***\nV25WAG   0.012318 0.004959 2.484000 0.205100 1.365200  0.312 0.004 ** \nV31AMAG  0.011976 0.005536 2.163000 0.370900 1.393900  0.330 0.001 ***\nV45MGLK  0.011411 0.003990 2.860000 0.142600 1.216900  0.347 0.002 ** \nV36YFHE  0.011370 0.007879 1.443000 1.121400 0.000000  0.364 0.047 *  \nV13RWB   0.011326 0.007442 1.522000 1.080500 0.000000  0.381 0.040 *  \nV47RFC   0.011238 0.004799 2.342000 0.214300 1.251400  0.397 0.001 ***\nV70RSL   0.011018 0.007241 1.522000 0.094600 1.085200  0.414 0.003 ** \nV41SPP   0.010892 0.006237 1.746000 1.058800 0.000000  0.430 0.013 *  \nV15STTH  0.010499 0.008202 1.280000 1.475400 0.745400  0.445 0.137    \nV35STP   0.010090 0.007467 1.351000 0.700100 1.638300  0.460 0.026 *  \nV73JW    0.009687 0.005914 1.638000 0.000000 0.934100  0.474 0.001 ***\nV33RWH   0.009674 0.006163 1.570000 1.318700 0.532700  0.489 0.036 *  \nV42SIL   0.009667 0.007712 1.253000 0.952800 0.442300  0.503 0.112    \nV34WSW   0.009358 0.006570 1.424000 0.247500 0.961700  0.517 0.156    \nV52TRM   0.009124 0.005824 1.567000 0.202800 0.871300  0.531 0.009 ** \nV98FLAME 0.009079 0.006396 1.419000 0.220500 0.964700  0.544 0.006 ** \nV69SKF   0.008992 0.005367 1.675000 0.334500 1.156400  0.557 0.004 ** \nV18YTH   0.008930 0.008900 1.003000 0.171800 0.845000  0.571 0.288    \nV23RBFT  0.008751 0.006449 1.357000 0.990600 0.456500  0.584 0.398    \nV6WTTR   0.008747 0.007272 1.203000 1.325800 0.736100  0.597 0.182    \nV56FTC   0.008722 0.005609 1.555000 0.948600 0.220000  0.609 0.033 *  \nV93DB    0.008385 0.005276 1.589000 0.061600 0.829800  0.622 0.002 ** \nV91NMIN  0.008172 0.007902 1.034000 0.103300 0.767400  0.634 0.031 *  \nV28VS    0.007982 0.007161 1.115000 0.599300 0.686000  0.646 0.612    \nV5GWH    0.007767 0.005012 1.550000 1.325200 0.765000  0.657 0.102    \nV94RBEE  0.007444 0.004665 1.596000 0.053800 0.732200  0.668 0.003 ** \nV27NHHE  0.007397 0.008745 0.846000 0.689200 0.000000  0.679 0.662    \nV19ER    0.007375 0.006436 1.146000 0.816000 1.397300  0.690 0.649    \nV16LR    0.006856 0.005658 1.212000 0.852900 1.252000  0.700 0.858    \nV59YR    0.006854 0.006952 0.986000 0.000000 0.651800  0.711 0.007 ** \nV81SHBC  0.006705 0.005721 1.172000 0.641600 0.000000  0.720 0.277    \nV3GF     0.006585 0.003090 2.131000 1.569300 1.077300  0.730 0.303    \nV29CST   0.006553 0.005132 1.277000 0.548200 0.776000  0.740 0.612    \nV20PCU   0.006505 0.006055 1.074000 0.411800 0.602800  0.750 0.508    \nV88WG    0.006448 0.006586 0.979000 0.000000 0.612900  0.759 0.007 ** \nV90WTG   0.006446 0.006479 0.995000 0.055600 0.612900  0.769 0.021 *  \nV99WWT   0.006439 0.006551 0.983000 0.000000 0.615300  0.778 0.007 ** \nV58OBO   0.006407 0.006318 1.014000 0.198800 0.611700  0.788 0.108    \nV24BFCS  0.006242 0.005422 1.151000 0.751600 1.328700  0.797 0.201    \nV9SFW    0.006159 0.003969 1.552000 1.384400 1.145500  0.806 0.345    \nV80RF    0.005971 0.006465 0.924000 0.568000 0.000000  0.815 0.518    \nV50CHE   0.005950 0.007394 0.805000 0.580200 0.000000  0.824 0.534    \nV92NFB   0.005676 0.005583 1.017000 0.141400 0.513100  0.832 0.030 *  \nV72CRP   0.005564 0.005644 0.986000 0.000000 0.545600  0.840 0.007 ** \nV71PDOV  0.005528 0.005717 0.967000 0.055600 0.530500  0.849 0.010 ** \nV48YTBC  0.005233 0.006402 0.817000 0.511500 0.000000  0.856 0.563    \nV14AUR   0.005141 0.005536 0.929000 0.893600 1.237700  0.864 0.764    \nV12LK    0.005031 0.005700 0.883000 1.372500 0.910300  0.871 0.139    \nV46BHHE  0.004971 0.006225 0.799000 0.491200 0.000000  0.879 0.999    \nV37WHIP  0.004588 0.006321 0.726000 0.441900 0.000000  0.885 0.696    \nV84YRTH  0.004512 0.007311 0.617000 0.082700 0.389600  0.892 0.040 *  \nV83SFC   0.004484 0.006299 0.712000 0.424600 0.000000  0.899 0.823    \nV43GCU   0.004425 0.006145 0.720000 0.423100 0.000000  0.905 0.984    \nV87LFC   0.004386 0.006114 0.717000 0.224900 0.314400  0.912 0.210    \nV53MB    0.003834 0.005402 0.710000 0.224200 0.210200  0.917 0.625    \nV101LCOR 0.003699 0.006477 0.571000 0.000000 0.370000  0.923 0.072 .  \nV55LHE   0.003608 0.005688 0.634000 0.354900 0.000000  0.928 0.622    \nV63DWS   0.003160 0.005040 0.627000 0.313100 0.000000  0.933 0.844    \nV65LWB   0.002918 0.008100 0.360000 0.258700 0.000000  0.937 0.595    \nV49LYRE  0.002876 0.005277 0.545000 0.268000 0.000000  0.942 0.713    \nV74BCHE  0.002855 0.005003 0.571000 0.000000 0.261700  0.946 0.058 .  \nV85ROSE  0.002515 0.004606 0.546000 0.239700 0.000000  0.950 0.570    \nV39FHE   0.002455 0.006886 0.357000 0.248500 0.000000  0.953 0.880    \nV68PIL   0.002403 0.005266 0.456000 0.232900 0.000000  0.957 0.716    \nV44MUSK  0.002307 0.006375 0.362000 0.236300 0.000000  0.960 0.799    \nV95HBC   0.002170 0.003801 0.571000 0.000000 0.198800  0.963 0.156    \nV51OWH   0.002158 0.004757 0.454000 0.211500 0.000000  0.967 0.475    \nV22SCR   0.002084 0.004592 0.454000 0.210400 0.000000  0.970 1.000    \nV1GST    0.002006 0.001368 1.466000 1.466600 1.343300  0.973 0.641    \nV77RRP   0.001709 0.004743 0.360000 0.174100 0.000000  0.975 0.387    \nV76GBB   0.001680 0.003676 0.457000 0.166700 0.000000  0.978 0.673    \nV86BCOO  0.001432 0.003963 0.361000 0.142300 0.000000  0.980 0.619    \nV67GGC   0.001395 0.003914 0.356000 0.135400 0.000000  0.982 0.870    \nV64BELL  0.001382 0.005570 0.248000 0.127500 0.000000  0.984 0.550    \nV62RBTR  0.001356 0.003781 0.359000 0.127400 0.000000  0.986 0.659    \nV57PINK  0.001324 0.003673 0.360000 0.127300 0.000000  0.988 0.504    \nV97PCL   0.001101 0.004441 0.248000 0.102200 0.000000  0.990 0.430    \nV40BRTH  0.001017 0.004100 0.248000 0.088800 0.000000  0.991 0.995    \nV78LLOR  0.000946 0.003812 0.248000 0.106900 0.000000  0.992 0.472    \nV89PCOO  0.000744 0.003002 0.248000 0.069100 0.000000  0.994 0.430    \nV54STHR  0.000693 0.002793 0.248000 0.073200 0.000000  0.995 0.857    \nV79YTHE  0.000667 0.002689 0.248000 0.075400 0.000000  0.996 0.774    \nV66CBW   0.000652 0.002628 0.248000 0.058800 0.000000  0.997 0.924    \nV96DF    0.000626 0.002525 0.248000 0.070800 0.000000  0.997 0.826    \nV82AZKF  0.000602 0.002428 0.248000 0.053800 0.000000  0.998 0.453    \nV100WBWS 0.000585 0.002359 0.248000 0.066200 0.000000  0.999 0.472    \nV102KING 0.000545 0.002198 0.248000 0.053800 0.000000  1.000 0.601    \nV61SPW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Gippsland Ma_Montane Fore \n\n           average        sd     ratio       ava       avb cumsum     p    \nV17WPHE   0.025511  0.002976  8.572000  2.247700  0.000000  0.047 0.001 ***\nV34WSW    0.020923  0.002668  7.841000  1.856000  0.000000  0.086 0.001 ***\nV11CR     0.019365  0.002542  7.617000  0.000000  1.710400  0.121 0.001 ***\nV27NHHE   0.018629  0.005197  3.584000  1.646300  0.000000  0.156 0.001 ***\nV6WTTR    0.017764  0.002227  7.977000  0.000000  1.567100  0.189 0.002 ** \nV15STTH   0.017530  0.007233  2.423000  0.323400  1.857500  0.221 0.003 ** \nV25WAG    0.015594  0.002143  7.276000  1.376000  0.000000  0.250 0.002 ** \nV13RWB    0.014804  0.009495  1.559000  1.922100  0.662400  0.277 0.008 ** \nV20PCU    0.014406  0.001861  7.739000  0.000000  1.268100  0.304 0.001 ***\nV43GCU    0.014108  0.001624  8.689000  0.000000  1.256100  0.330 0.002 ** \nV83SFC    0.013763  0.001275 10.794000  0.000000  1.220000  0.355 0.001 ***\nV4BTH     0.012989  0.009916  1.310000  1.184200  2.294000  0.379 0.080 .  \nV42SIL    0.012586  0.005694  2.210000  0.420400  1.363600  0.402 0.022 *  \nV3GF      0.012521  0.008426  1.486000  0.646500  1.724600  0.425 0.002 ** \nV19ER     0.011718  0.005658  2.071000  1.308600  0.266900  0.447 0.023 *  \nV37WHIP   0.011155  0.006729  1.658000  0.000000  1.020400  0.468 0.002 ** \nV48YTBC   0.010991  0.006611  1.663000  0.000000  0.958400  0.488 0.002 ** \nV81SHBC   0.010480  0.005715  1.834000  0.271900  1.191300  0.507 0.011 *  \nV21ESP    0.010274  0.006547  1.569000  0.644000  1.502100  0.526 0.128    \nV67GGC    0.010101  0.006264  1.613000  0.000000  0.926600  0.545 0.001 ***\nV80RF     0.010024  0.006191  1.619000  0.000000  0.913500  0.563 0.021 *  \nV5GWH     0.009874  0.007909  1.248000  0.674800  1.486100  0.582 0.023 *  \nV29CST    0.009751  0.004639  2.102000  1.059100  0.314400  0.599 0.032 *  \nV36YFHE   0.009681  0.007030  1.377000  1.243700  1.134500  0.617 0.413    \nV16LR     0.009375  0.007159  1.310000  1.184900  1.023200  0.635 0.224    \nV46BHHE   0.008981  0.008370  1.073000  0.307900  0.828800  0.651 0.360    \nV49LYRE   0.008738  0.005375  1.626000  0.000000  0.803100  0.667 0.012 *  \nV33RWH    0.008651  0.007180  1.205000  0.579800  0.753700  0.683 0.255    \nV68PIL    0.008604  0.005317  1.618000  0.000000  0.785200  0.699 0.010 ** \nV54STHR   0.008499  0.005150  1.650000  0.000000  0.758900  0.715 0.002 ** \nV65LWB    0.008071  0.008400  0.961000  0.741500  0.000000  0.730 0.150    \nV63DWS    0.007899  0.008217  0.961000  0.724800  0.000000  0.744 0.051 .  \nV91NMIN   0.007851  0.008209  0.956000  0.719700  0.000000  0.759 0.073 .  \nV28VS     0.007829  0.008154  0.960000  0.000000  0.709200  0.773 0.606    \nV7WEHE    0.007421  0.005457  1.360000  1.241500  1.224500  0.787 0.363    \nV24BFCS   0.007277  0.005961  1.221000  1.112500  0.496700  0.800 0.130    \nV56FTC    0.007098  0.006534  1.086000  0.615700  1.231300  0.813 0.398    \nV35STP    0.006718  0.007148  0.940000  0.993200  0.944200  0.826 0.799    \nV23RBFT   0.006640  0.006306  1.053000  0.937700  0.883700  0.838 0.890    \nV14AUR    0.006610  0.006785  0.974000  0.962100  0.919800  0.850 0.411    \nV55LHE    0.006240  0.006468  0.965000  0.000000  0.590500  0.862 0.185    \nV12LK     0.005918  0.005486  1.079000  0.801000  1.324100  0.873 0.062 .  \nV26WWCH   0.005801  0.010413  0.557000  0.000000  0.462400  0.883 0.878    \nV50CHE    0.005678  0.005941  0.956000  0.000000  0.509600  0.894 0.599    \nV86BCOO   0.005379  0.005588  0.962000  0.000000  0.481700  0.904 0.078 .  \nV22SCR    0.003983  0.007149  0.557000  0.000000  0.317500  0.911 0.830    \nV8WNHE    0.003758  0.001583  2.374000  1.843200  1.709100  0.918 0.846    \nV85ROSE   0.003564  0.006391  0.558000  0.000000  0.331700  0.925 0.308    \nV62RBTR   0.003528  0.006332  0.557000  0.000000  0.281200  0.931 0.224    \nV38GAL    0.003497  0.006277  0.557000  0.323400  0.000000  0.938 0.800    \nV87LFC    0.003183  0.005712  0.557000  0.289600  0.000000  0.944 0.560    \nV45MGLK   0.002941  0.005278  0.557000  0.271900  0.000000  0.949 0.822    \nV41SPP    0.002919  0.001917  1.523000  1.200500  1.352200  0.954 0.894    \nV9SFW     0.002819  0.002623  1.075000  1.734700  1.525000  0.960 0.907    \nV53MB     0.002704  0.004852  0.557000  0.250000  0.000000  0.965 0.776    \nV57PINK   0.002686  0.004816  0.558000  0.000000  0.250000  0.970 0.280    \nV76GBB    0.002568  0.004606  0.557000  0.000000  0.220000  0.974 0.425    \nV70RSL    0.002557  0.004589  0.557000  0.236400  0.000000  0.979 0.834    \nV2EYR     0.002508  0.001941  1.292000  1.599500  1.440100  0.984 0.916    \nV102KING  0.002459  0.004408  0.558000  0.000000  0.236400  0.988 0.067 .  \nV51OWH    0.002364  0.004239  0.558000  0.000000  0.220000  0.993 0.508    \nV10WBSW   0.002324  0.001192  1.950000  1.760400  1.622600  0.997 0.966    \nV1GST     0.001754  0.001373  1.278000  1.493300  1.379200  1.000 0.734    \nV18YTH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV30BTR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV31AMAG   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV32SCC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV40BRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV47RFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV52TRM    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV58OBO    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV59YR     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV60LFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV61SPW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV66CBW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV69SKF    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV71PDOV   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV72CRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV73JW     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV74BCHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV84YRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV88WG     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV90WTG    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV92NFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV93DB     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV94RBEE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV95HBC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV96DF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV98FLAME  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV99WWT    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV101LCOR  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Gippsland Ma_Foothills Wo \n\n           average        sd     ratio       ava       avb cumsum     p    \nV17WPHE   0.026097  0.002383 10.951000  2.247700  0.000000  0.052 0.001 ***\nV34WSW    0.021402  0.002230  9.597000  1.856000  0.000000  0.094 0.001 ***\nV6WTTR    0.018324  0.001427 12.839000  0.000000  1.579900  0.130 0.001 ***\nV11CR     0.017226  0.001813  9.499000  0.000000  1.484400  0.164 0.005 ** \nV15STTH   0.017112  0.007416  2.307000  0.323400  1.788200  0.198 0.003 ** \nV40BRTH   0.016327  0.010384  1.572000  0.000000  1.423800  0.230 0.001 ***\nV27NHHE   0.015988  0.007215  2.216000  1.646300  0.256000  0.262 0.006 ** \nV25WAG    0.015952  0.001857  8.589000  1.376000  0.000000  0.293 0.001 ***\nV22SCR    0.015801  0.001670  9.460000  0.000000  1.364500  0.325 0.001 ***\nV26WWCH   0.013884  0.008667  1.602000  0.000000  1.210300  0.352 0.022 *  \nV18YTH    0.013759  0.008550  1.609000  0.000000  1.199300  0.379 0.024 *  \nV28VS     0.012735  0.007685  1.657000  0.000000  1.088900  0.404 0.019 *  \nV10WBSW   0.011739  0.009043  1.298000  1.760400  0.773600  0.428 0.081 .  \nV3GF      0.011589  0.007893  1.468000  0.646500  1.569600  0.451 0.005 ** \nV33RWH    0.011462  0.006709  1.709000  0.579800  1.597500  0.473 0.026 *  \nV69SKF    0.010219  0.006162  1.658000  0.000000  0.890600  0.494 0.004 ** \nV4BTH     0.010179  0.009715  1.048000  1.184200  1.975000  0.514 0.369    \nV23RBFT   0.009465  0.006668  1.419000  0.937700  0.243500  0.532 0.316    \nV46BHHE   0.009439  0.007230  1.305000  0.307900  0.931700  0.551 0.292    \nV42SIL    0.009295  0.005353  1.736000  0.420400  0.761200  0.569 0.284    \nV21ESP    0.009082  0.006693  1.357000  0.644000  1.346100  0.587 0.302    \nV5GWH     0.008800  0.007323  1.202000  0.674800  1.309900  0.605 0.102    \nV13RWB    0.008699  0.002320  3.750000  1.922100  1.170600  0.622 0.665    \nV53MB     0.008525  0.006365  1.339000  0.250000  0.851600  0.639 0.033 *  \nV36YFHE   0.008313  0.004873  1.706000  1.243700  1.319700  0.655 0.704    \nV65LWB    0.008250  0.008548  0.965000  0.741500  0.000000  0.671 0.131    \nV63DWS    0.008179  0.007487  1.092000  0.724800  0.243500  0.688 0.047 *  \nV81SHBC   0.008171  0.006178  1.323000  0.271900  0.807100  0.704 0.099 .  \nV19ER     0.008140  0.007125  1.143000  1.308600  0.654600  0.720 0.385    \nV91NMIN   0.008026  0.008354  0.961000  0.719700  0.000000  0.736 0.069 .  \nV7WEHE    0.007740  0.006507  1.189000  1.241500  1.318200  0.751 0.340    \nV29CST    0.007736  0.004933  1.568000  1.059100  0.659900  0.766 0.208    \nV20PCU    0.007716  0.008005  0.964000  0.000000  0.673000  0.782 0.161    \nV9SFW     0.007643  0.007705  0.992000  1.734700  1.081900  0.797 0.201    \nV56FTC    0.007448  0.006647  1.120000  0.615700  1.244700  0.811 0.293    \nV14AUR    0.006977  0.007154  0.975000  0.962100  0.987000  0.825 0.375    \nV35STP    0.006914  0.007551  0.916000  0.993200  1.011200  0.839 0.805    \nV43GCU    0.006636  0.007051  0.941000  0.000000  0.583000  0.852 0.524    \nV12LK     0.006350  0.005685  1.117000  0.801000  1.350100  0.865 0.043 *  \nV16LR     0.006246  0.006500  0.961000  1.184900  1.479800  0.877 0.845    \nV64BELL   0.005568  0.009988  0.557000  0.000000  0.492000  0.888 0.219    \nV50CHE    0.004608  0.008269  0.557000  0.000000  0.384600  0.897 0.706    \nV96DF     0.004006  0.007186  0.557000  0.000000  0.349000  0.905 0.104    \nV41SPP    0.003877  0.002906  1.334000  1.200500  1.526000  0.913 0.906    \nV24BFCS   0.003777  0.005287  0.714000  1.112500  0.794400  0.920 0.850    \nV61SPW    0.003744  0.006716  0.557000  0.000000  0.326200  0.927 0.104    \nV80RF     0.003648  0.006547  0.557000  0.000000  0.304500  0.935 0.847    \nV2EYR     0.003593  0.002576  1.394000  1.599500  1.316800  0.942 0.865    \nV38GAL    0.003574  0.006396  0.559000  0.323400  0.000000  0.949 0.780    \nV87LFC    0.003254  0.005822  0.559000  0.289600  0.000000  0.955 0.538    \nV8WNHE    0.003139  0.002678  1.172000  1.843200  1.582200  0.962 0.919    \nV58OBO    0.003041  0.005456  0.557000  0.000000  0.261700  0.968 0.783    \nV83SFC    0.003041  0.005456  0.557000  0.000000  0.261700  0.974 0.879    \nV45MGLK   0.003006  0.005378  0.559000  0.271900  0.000000  0.980 0.762    \nV1GST     0.002663  0.001997  1.334000  1.493300  1.577800  0.985 0.218    \nV70RSL    0.002613  0.004676  0.559000  0.236400  0.000000  0.990 0.802    \nV62RBTR   0.002557  0.004588  0.557000  0.000000  0.220000  0.995 0.469    \nV66CBW    0.002557  0.004588  0.557000  0.000000  0.220000  1.000 0.541    \nV30BTR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV31AMAG   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV32SCC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV37WHIP   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV47RFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV48YTBC   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV49LYRE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV51OWH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV52TRM    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV54STHR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV55LHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV57PINK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV59YR     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV60LFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV67GGC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV68PIL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV71PDOV   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV72CRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV73JW     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV74BCHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV76GBB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV84YRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV85ROSE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV86BCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV88WG     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV90WTG    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV92NFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV93DB     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV94RBEE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV95HBC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV98FLAME  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV99WWT    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV101LCOR  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV102KING  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Gippsland Ma_Box-Ironbark \n\n          average       sd    ratio      ava      avb cumsum     p    \nV17WPHE  0.026261 0.003224 8.146000 2.247700 0.000000  0.046 0.001 ***\nV18YTH   0.022239 0.002654 8.378000 0.000000 1.909900  0.086 0.001 ***\nV34WSW   0.021534 0.002823 7.627000 1.856000 0.000000  0.124 0.001 ***\nV10WBSW  0.020600 0.003335 6.178000 1.760400 0.000000  0.160 0.001 ***\nV26WWCH  0.019377 0.002919 6.638000 0.000000 1.655500  0.195 0.001 ***\nV27NHHE  0.019175 0.005389 3.559000 1.646300 0.000000  0.229 0.003 ** \nV6WTTR   0.017796 0.001927 9.234000 0.000000 1.525100  0.260 0.001 ***\nV46BHHE  0.017556 0.007155 2.454000 0.307900 1.798900  0.291 0.001 ***\nV39FHE   0.017550 0.003257 5.388000 0.000000 1.532700  0.322 0.001 ***\nV25WAG   0.016052 0.002282 7.033000 1.376000 0.000000  0.351 0.001 ***\nV22SCR   0.015788 0.003354 4.707000 0.000000 1.350800  0.379 0.002 ** \nV40BRTH  0.015511 0.009337 1.661000 0.000000 1.379300  0.406 0.009 ** \nV15STTH  0.015362 0.007523 2.042000 0.323400 1.627000  0.433 0.009 ** \nV44MUSK  0.013318 0.008639 1.542000 0.000000 1.180800  0.457 0.003 ** \nV58OBO   0.012439 0.003104 4.007000 0.000000 1.059200  0.479 0.002 ** \nV42SIL   0.012248 0.004880 2.510000 0.420400 1.255000  0.500 0.026 *  \nV75RCR   0.011824 0.003514 3.365000 0.000000 1.000900  0.521 0.001 ***\nV28VS    0.011169 0.006772 1.649000 0.000000 0.971800  0.541 0.067 .  \nV11CR    0.011098 0.006986 1.589000 0.000000 0.992900  0.561 0.165    \nV33RWH   0.010578 0.006786 1.559000 0.579800 1.512200  0.580 0.038 *  \nV16LR    0.010525 0.007973 1.320000 1.184900 0.576500  0.598 0.084 .  \nV3GF     0.010144 0.006630 1.530000 0.646500 1.330200  0.616 0.052 .  \nV29CST   0.009753 0.005485 1.778000 1.059100 0.256000  0.633 0.034 *  \nV23RBFT  0.009549 0.007039 1.357000 0.937700 0.320500  0.650 0.299    \nV43GCU   0.009226 0.005611 1.644000 0.000000 0.818400  0.667 0.052 .  \nV35STP   0.009032 0.007151 1.263000 0.993200 0.656700  0.683 0.226    \nV36YFHE  0.008694 0.006441 1.350000 1.243700 1.383500  0.698 0.639    \nV2EYR    0.008634 0.006941 1.244000 1.599500 0.874900  0.713 0.184    \nV5GWH    0.008626 0.006596 1.308000 0.674800 1.171900  0.729 0.110    \nV65LWB   0.008297 0.008644 0.960000 0.741500 0.000000  0.743 0.153    \nV63DWS   0.008199 0.007646 1.072000 0.724800 0.220000  0.758 0.049 *  \nV91NMIN  0.008072 0.008448 0.955000 0.719700 0.000000  0.772 0.067 .  \nV21ESP   0.007818 0.008323 0.939000 0.644000 0.000000  0.786 0.715    \nV7WEHE   0.007692 0.006430 1.196000 1.241500 1.296700  0.799 0.345    \nV4BTH    0.007495 0.007712 0.972000 1.184200 1.520900  0.813 0.737    \nV56FTC   0.007373 0.007510 0.982000 0.615700 0.281200  0.826 0.331    \nV66CBW   0.007079 0.007358 0.962000 0.000000 0.566600  0.838 0.004 ** \nV90WTG   0.006660 0.007437 0.896000 0.000000 0.542000  0.850 0.044 *  \nV70RSL   0.006539 0.006415 1.019000 0.236400 0.557400  0.862 0.271    \nV61SPW   0.006393 0.006678 0.957000 0.000000 0.593000  0.873 0.014 *  \nV19ER    0.006130 0.006577 0.932000 1.308600 0.846400  0.884 0.864    \nV14AUR   0.005432 0.006537 0.831000 0.962100 1.211600  0.893 0.642    \nV81SHBC  0.004610 0.006122 0.753000 0.271900 0.281200  0.902 0.923    \nV12LK    0.004495 0.005039 0.892000 0.801000 1.122800  0.910 0.337    \nV79YTHE  0.004087 0.007327 0.558000 0.000000 0.396100  0.917 0.042 *  \nV9SFW    0.004012 0.003799 1.056000 1.734700 1.472800  0.924 0.735    \nV30BTR   0.003950 0.007083 0.558000 0.000000 0.382900  0.931 0.627    \nV38GAL   0.003595 0.006455 0.557000 0.323400 0.000000  0.937 0.770    \nV13RWB   0.003563 0.001691 2.107000 1.922100 1.792200  0.944 0.998    \nV41SPP   0.003386 0.002315 1.463000 1.200500 1.440100  0.950 0.887    \nV87LFC   0.003273 0.005878 0.557000 0.289600 0.000000  0.955 0.552    \nV45MGLK  0.003023 0.005428 0.557000 0.271900 0.000000  0.961 0.759    \nV8WNHE   0.002915 0.000912 3.194000 1.843200 1.595400  0.966 0.930    \nV53MB    0.002779 0.004990 0.557000 0.250000 0.000000  0.971 0.762    \nV94RBEE  0.002642 0.004736 0.558000 0.000000 0.256000  0.976 0.414    \nV96DF    0.002642 0.004736 0.558000 0.000000 0.256000  0.980 0.358    \nV76GBB   0.002548 0.004573 0.557000 0.000000 0.210200  0.985 0.397    \nV1GST    0.002444 0.001755 1.393000 1.493300 1.425700  0.989 0.321    \nV88WG    0.002395 0.004296 0.557000 0.000000 0.210200  0.993 0.381    \nV95HBC   0.002395 0.004296 0.557000 0.000000 0.210200  0.997 0.050 *  \nV24BFCS  0.001441 0.001298 1.110000 1.112500 1.014400  1.000 0.985    \nV20PCU   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV31AMAG  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV32SCC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV37WHIP  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV47RFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV48YTBC  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV49LYRE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV50CHE   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV51OWH   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV52TRM   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV54STHR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV55LHE   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV57PINK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV60LFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV62RBTR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV64BELL  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV67GGC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV68PIL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV69SKF   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV71PDOV  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV77RRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV78LLOR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV80RF    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV82AZKF  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV83SFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV84YRTH  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV85ROSE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV86BCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV89PCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV92NFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV93DB    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV97PCL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV98FLAME 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV100WBWS 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV102KING 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Gippsland Ma_River Red Gu \n\n           average        sd     ratio       ava       avb cumsum     p    \nV32SCC    0.025596  0.002660  9.624000  0.000000  2.225000  0.041 0.001 ***\nV13RWB    0.022078  0.001581 13.960000  1.922100  0.000000  0.077 0.001 ***\nV10WBSW   0.020296  0.002871  7.069000  1.760400  0.000000  0.110 0.001 ***\nV27NHHE   0.018894  0.005104  3.702000  1.646300  0.000000  0.140 0.003 ** \nV2EYR     0.018457  0.002843  6.492000  1.599500  0.000000  0.170 0.001 ***\nV26WWCH   0.018125  0.001284 14.119000  0.000000  1.578000  0.199 0.004 ** \nV8WNHE    0.018039  0.006464  2.791000  1.843200  0.289600  0.228 0.001 ***\nV30BTR    0.017484  0.001776  9.844000  0.000000  1.520300  0.257 0.002 ** \nV31AMAG   0.016036  0.001581 10.146000  0.000000  1.393900  0.283 0.002 ** \nV60LFB    0.015147  0.003696  4.098000  0.000000  1.312300  0.307 0.002 ** \nV38GAL    0.014698  0.007189  2.044000  0.323400  1.582800  0.331 0.002 ** \nV47RFC    0.014352  0.001420 10.106000  0.000000  1.251400  0.354 0.001 ***\nV36YFHE   0.014260  0.009360  1.524000  1.243700  0.000000  0.377 0.016 *  \nV7WEHE    0.013756  0.008586  1.602000  1.241500  0.000000  0.399 0.006 ** \nV41SPP    0.013744  0.002390  5.750000  1.200500  0.000000  0.421 0.002 ** \nV69SKF    0.013273  0.001217 10.902000  0.000000  1.156400  0.443 0.002 ** \nV4BTH     0.013108  0.008293  1.581000  1.184200  0.000000  0.464 0.083 .  \nV70RSL    0.011108  0.007818  1.421000  0.236400  1.085200  0.482 0.009 ** \nV45MGLK   0.010974  0.005854  1.875000  0.271900  1.216900  0.500 0.002 ** \nV98FLAME  0.010959  0.006960  1.575000  0.000000  0.964700  0.517 0.003 ** \nV73JW     0.010616  0.006635  1.600000  0.000000  0.934100  0.534 0.001 ***\nV34WSW    0.010143  0.007370  1.376000  1.856000  0.961700  0.551 0.175    \nV52TRM    0.009996  0.006422  1.556000  0.000000  0.871300  0.567 0.007 ** \nV18YTH    0.009754  0.010215  0.955000  0.000000  0.845000  0.583 0.250    \nV93DB     0.009507  0.005742  1.656000  0.000000  0.829800  0.598 0.001 ***\nV91NMIN   0.008898  0.008710  1.022000  0.719700  0.767400  0.612 0.018 *  \nV15STTH   0.008597  0.008559  1.004000  0.323400  0.745400  0.626 0.482    \nV3GF      0.008595  0.004954  1.735000  0.646500  1.077300  0.640 0.101    \nV6WTTR    0.008498  0.008891  0.956000  0.000000  0.736100  0.654 0.327    \nV94RBEE   0.008405  0.005103  1.647000  0.000000  0.732200  0.667 0.001 ***\nV65LWB    0.008183  0.008484  0.965000  0.741500  0.000000  0.681 0.143    \nV23RBFT   0.008089  0.005959  1.357000  0.937700  0.456500  0.694 0.568    \nV63DWS    0.008008  0.008299  0.965000  0.724800  0.000000  0.707 0.059 .  \nV28VS     0.007914  0.008215  0.963000  0.000000  0.686000  0.720 0.599    \nV35STP    0.007822  0.007933  0.986000  0.993200  1.638300  0.732 0.572    \nV5GWH     0.007810  0.005625  1.388000  0.674800  0.765000  0.745 0.169    \nV21ESP    0.007696  0.008143  0.945000  0.644000  0.000000  0.757 0.732    \nV59YR     0.007520  0.007810  0.963000  0.000000  0.651800  0.769 0.001 ***\nV42SIL    0.007277  0.009602  0.758000  0.420400  0.442300  0.781 0.772    \nV56FTC    0.007269  0.006882  1.056000  0.615700  0.220000  0.793 0.338    \nV88WG     0.007075  0.007399  0.956000  0.000000  0.612900  0.804 0.002 ** \nV90WTG    0.007075  0.007399  0.956000  0.000000  0.612900  0.816 0.020 *  \nV99WWT    0.007063  0.007361  0.959000  0.000000  0.615300  0.827 0.001 ***\nV58OBO    0.007005  0.007268  0.964000  0.000000  0.611700  0.838 0.092 .  \nV9SFW     0.006866  0.003914  1.754000  1.734700  1.145500  0.849 0.328    \nV33RWH    0.006745  0.006466  1.043000  0.579800  0.532700  0.860 0.615    \nV20PCU    0.006711  0.007071  0.949000  0.000000  0.602800  0.871 0.494    \nV16LR     0.006244  0.004942  1.264000  1.184900  1.252000  0.881 0.846    \nV72CRP    0.006087  0.006321  0.963000  0.000000  0.545600  0.891 0.001 ***\nV92NFB    0.006062  0.006313  0.960000  0.000000  0.513100  0.901 0.011 *  \nV71PDOV   0.006011  0.006489  0.926000  0.000000  0.530500  0.910 0.001 ***\nV12LK     0.005937  0.005887  1.009000  0.801000  0.910300  0.920 0.074 .  \nV14AUR    0.005129  0.006478  0.792000  0.962100  1.237700  0.928 0.683    \nV87LFC    0.005124  0.006716  0.763000  0.289600  0.314400  0.937 0.222    \nV84YRTH   0.004535  0.008137  0.557000  0.000000  0.389600  0.944 0.040 *  \nV101LCOR  0.004040  0.007245  0.558000  0.000000  0.370000  0.951 0.003 ** \nV53MB     0.004022  0.005167  0.778000  0.250000  0.210200  0.957 0.583    \nV29CST    0.003883  0.005041  0.770000  1.059100  0.776000  0.963 0.983    \nV46BHHE   0.003418  0.006118  0.559000  0.307900  0.000000  0.969 0.983    \nV74BCHE   0.003144  0.005642  0.557000  0.000000  0.261700  0.974 0.001 ***\nV81SHBC   0.003030  0.005425  0.559000  0.271900  0.000000  0.979 0.996    \nV17WPHE   0.002749  0.001706  1.611000  2.247700  2.034900  0.983 0.981    \nV24BFCS   0.002646  0.002193  1.206000  1.112500  1.328700  0.988 0.913    \nV95HBC    0.002389  0.004287  0.557000  0.000000  0.198800  0.991 0.049 *  \nV1GST     0.002019  0.001634  1.236000  1.493300  1.343300  0.995 0.574    \nV19ER     0.001923  0.001172  1.640000  1.308600  1.397300  0.998 0.999    \nV25WAG    0.001422  0.001101  1.292000  1.376000  1.365200  1.000 0.964    \nV11CR     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV22SCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV37WHIP   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV40BRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV43GCU    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV48YTBC   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV49LYRE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV50CHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV51OWH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV54STHR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV55LHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV57PINK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV61SPW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV62RBTR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV66CBW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV67GGC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV68PIL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV76GBB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV80RF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV83SFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV85ROSE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV86BCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV96DF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV102KING  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Montane Fore_Foothills Wo \n\n          average       sd    ratio      ava      avb cumsum     p   \nV40BRTH  0.014435 0.009183 1.571800 0.000000 1.423800  0.042 0.008 **\nV18YTH   0.012164 0.007560 1.609000 0.000000 1.199300  0.077 0.054 . \nV26WWCH  0.010745 0.008003 1.342600 0.462400 1.210300  0.108 0.159   \nV22SCR   0.010509 0.005505 1.909200 0.317500 1.364500  0.138 0.043 * \nV37WHIP  0.010114 0.006054 1.670700 1.020400 0.000000  0.168 0.018 * \nV48YTBC  0.009915 0.005918 1.675400 0.958400 0.000000  0.196 0.021 * \nV83SFC   0.009752 0.004877 1.999500 1.220000 0.261700  0.224 0.029 * \nV10WBSW  0.009395 0.007408 1.268200 1.622600 0.773600  0.251 0.369   \nV67GGC   0.009162 0.005646 1.622600 0.926600 0.000000  0.278 0.007 **\nV69SKF   0.009034 0.005447 1.658400 0.000000 0.890600  0.304 0.036 * \nV53MB    0.008754 0.005324 1.644300 0.000000 0.851600  0.329 0.023 * \nV33RWH   0.008585 0.007066 1.215100 0.753700 1.597500  0.354 0.278   \nV46BHHE  0.008556 0.006287 1.361000 0.828800 0.931700  0.379 0.496   \nV13RWB   0.008482 0.004846 1.750400 0.662400 1.170600  0.403 0.713   \nV49LYRE  0.007927 0.004849 1.634700 0.803100 0.000000  0.426 0.024 * \nV68PIL   0.007799 0.004781 1.631300 0.785200 0.000000  0.449 0.019 * \nV80RF    0.007785 0.006023 1.292600 0.913500 0.304500  0.471 0.131   \nV54STHR  0.007684 0.004615 1.665200 0.758900 0.000000  0.493 0.007 **\nV23RBFT  0.007670 0.005612 1.366800 0.883700 0.243500  0.516 0.700   \nV28VS    0.007670 0.007611 1.007700 0.709200 1.088900  0.538 0.660   \nV43GCU   0.007662 0.005636 1.359400 1.256100 0.583000  0.560 0.197   \nV16LR    0.007478 0.006026 1.241100 1.023200 1.479800  0.582 0.644   \nV20PCU   0.006977 0.006456 1.080800 1.268100 0.673000  0.602 0.378   \nV19ER    0.006769 0.006493 1.042500 0.266900 0.654600  0.621 0.755   \nV29CST   0.006698 0.006812 0.983200 0.314400 0.659900  0.641 0.537   \nV50CHE   0.006550 0.006045 1.083500 0.509600 0.384600  0.659 0.508   \nV35STP   0.006186 0.006202 0.997400 0.944200 1.011200  0.677 0.856   \nV42SIL   0.006120 0.004742 1.290500 1.363600 0.761200  0.695 0.914   \nV24BFCS  0.005843 0.005215 1.120300 0.496700 0.794400  0.712 0.426   \nV14AUR   0.005747 0.006056 0.948900 0.919800 0.987000  0.729 0.586   \nV55LHE   0.005677 0.005865 0.967900 0.590500 0.000000  0.745 0.283   \nV36YFHE  0.005166 0.005585 0.925100 1.134500 1.319700  0.760 0.966   \nV9SFW    0.005041 0.006178 0.815900 1.525000 1.081900  0.774 0.599   \nV64BELL  0.004931 0.008845 0.557500 0.000000 0.492000  0.789 0.365   \nV86BCOO  0.004867 0.005040 0.965600 0.481700 0.000000  0.803 0.115   \nV81SHBC  0.004290 0.004697 0.913300 1.191300 0.807100  0.815 0.960   \nV62RBTR  0.004175 0.005389 0.774800 0.281200 0.220000  0.827 0.174   \nV21ESP   0.003642 0.002780 1.310000 1.502100 1.346100  0.838 0.989   \nV96DF    0.003541 0.006353 0.557400 0.000000 0.349000  0.848 0.228   \nV4BTH    0.003496 0.002575 1.358000 2.294000 1.975000  0.858 0.951   \nV61SPW   0.003310 0.005938 0.557400 0.000000 0.326200  0.868 0.250   \nV8WNHE   0.003301 0.002559 1.290000 1.709100 1.582200  0.877 0.895   \nV85ROSE  0.003237 0.005792 0.558900 0.331700 0.000000  0.886 0.358   \nV7WEHE   0.002775 0.001747 1.588400 1.224500 1.318200  0.894 0.957   \nV27NHHE  0.002698 0.004842 0.557300 0.000000 0.256000  0.902 0.969   \nV58OBO   0.002685 0.004816 0.557400 0.000000 0.261700  0.910 0.869   \nV11CR    0.002557 0.001689 1.513300 1.710400 1.484400  0.917 0.992   \nV63DWS   0.002440 0.004377 0.557500 0.000000 0.243500  0.924 0.834   \nV57PINK  0.002440 0.004365 0.558900 0.250000 0.000000  0.931 0.465   \nV76GBB   0.002313 0.004139 0.558900 0.220000 0.000000  0.938 0.588   \nV66CBW   0.002257 0.004050 0.557400 0.000000 0.220000  0.945 0.602   \nV102KING 0.002240 0.004008 0.558900 0.236400 0.000000  0.951 0.322   \nV41SPP   0.002181 0.001351 1.614700 1.352200 1.526000  0.958 0.935   \nV51OWH   0.002147 0.003842 0.558900 0.220000 0.000000  0.964 0.595   \nV1GST    0.002076 0.002289 0.906700 1.379200 1.577800  0.970 0.524   \nV2EYR    0.002039 0.001545 1.319900 1.440100 1.316800  0.976 0.965   \nV15STTH  0.001903 0.001654 1.150800 1.857500 1.788200  0.981 0.993   \nV5GWH    0.001891 0.001382 1.367800 1.486100 1.309900  0.987 0.975   \nV3GF     0.001741 0.001033 1.685600 1.724600 1.569600  0.992 0.991   \nV6WTTR   0.001138 0.000630 1.805600 1.567100 1.579900  0.995 0.993   \nV56FTC   0.001005 0.000717 1.402700 1.231300 1.244700  0.998 1.000   \nV12LK    0.000762 0.000683 1.116400 1.324100 1.350100  1.000 1.000   \nV17WPHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV25WAG   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV30BTR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV31AMAG  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV32SCC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV34WSW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV38GAL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV39FHE   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV44MUSK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV45MGLK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV47RFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV52TRM   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV60LFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV65LWB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV70RSL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV71PDOV  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV77RRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV78LLOR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV79YTHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV82AZKF  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV84YRTH  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV87LFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV88WG    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV89PCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV90WTG   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV91NMIN  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV92NFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV93DB    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV94RBEE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV95HBC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV97PCL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV98FLAME 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV100WBWS 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Montane Fore_Box-Ironbark \n\n           average        sd     ratio       ava       avb cumsum     p    \nV18YTH    0.019614  0.002267  8.650000  0.000000  1.909900  0.042 0.002 ** \nV10WBSW   0.016737  0.002289  7.312000  1.622600  0.000000  0.079 0.004 ** \nV39FHE    0.015508  0.003007  5.157000  0.000000  1.532700  0.112 0.002 ** \nV21ESP    0.015277  0.002850  5.360000  1.502100  0.000000  0.145 0.005 ** \nV40BRTH   0.013739  0.008267  1.662000  0.000000  1.379300  0.175 0.019 *  \nV20PCU    0.013073  0.001670  7.829000  1.268100  0.000000  0.203 0.007 ** \nV26WWCH   0.012932  0.006735  1.920000  0.462400  1.655500  0.231 0.030 *  \nV13RWB    0.012500  0.008069  1.549000  0.662400  1.792200  0.258 0.053 .  \nV83SFC    0.012497  0.001221 10.232000  1.220000  0.000000  0.285 0.004 ** \nV44MUSK   0.011793  0.007641  1.543000  0.000000  1.180800  0.310 0.010 ** \nV58OBO    0.010960  0.002648  4.139000  0.000000  1.059200  0.334 0.002 ** \nV22SCR    0.010674  0.005558  1.920000  0.317500  1.350800  0.357 0.035 *  \nV46BHHE   0.010527  0.009610  1.095000  0.828800  1.798900  0.380 0.113    \nV75RCR    0.010410  0.003002  3.468000  0.000000  1.000900  0.402 0.001 ***\nV37WHIP   0.010161  0.006135  1.656000  1.020400  0.000000  0.424 0.022 *  \nV56FTC    0.010047  0.005497  1.828000  1.231300  0.281200  0.446 0.042 *  \nV48YTBC   0.009965  0.006005  1.659000  0.958400  0.000000  0.468 0.026 *  \nV81SHBC   0.009699  0.005338  1.817000  1.191300  0.281200  0.489 0.011 *  \nV67GGC    0.009203  0.005719  1.609000  0.926600  0.000000  0.508 0.006 ** \nV80RF     0.009127  0.005635  1.620000  0.913500  0.000000  0.528 0.056 .  \nV16LR     0.008206  0.006541  1.255000  1.023200  0.576500  0.546 0.472    \nV11CR     0.008012  0.007131  1.124000  1.710400  0.992900  0.563 0.747    \nV49LYRE   0.007963  0.004911  1.621000  0.803100  0.000000  0.581 0.018 *  \nV4BTH     0.007900  0.002729  2.895000  2.294000  1.520900  0.598 0.655    \nV33RWH    0.007855  0.007080  1.109000  0.753700  1.512200  0.615 0.401    \nV23RBFT   0.007852  0.005901  1.331000  0.883700  0.320500  0.631 0.678    \nV68PIL    0.007835  0.004844  1.617000  0.785200  0.000000  0.648 0.013 *  \nV35STP    0.007792  0.006378  1.222000  0.944200  0.656700  0.665 0.606    \nV54STHR   0.007721  0.004680  1.650000  0.758900  0.000000  0.682 0.005 ** \nV28VS     0.007362  0.006716  1.096000  0.709200  0.971800  0.698 0.756    \nV19ER     0.007289  0.005572  1.308000  0.266900  0.846400  0.714 0.599    \nV36YFHE   0.006921  0.005444  1.271000  1.134500  1.383500  0.729 0.900    \nV66CBW    0.006194  0.006437  0.962000  0.000000  0.566600  0.742 0.024 *  \nV24BFCS   0.005995  0.005034  1.191000  0.496700  1.014400  0.755 0.357    \nV2EYR     0.005841  0.005810  1.005000  1.440100  0.874900  0.768 0.594    \nV90WTG    0.005838  0.006497  0.899000  0.000000  0.542000  0.780 0.096 .  \nV70RSL    0.005780  0.006013  0.961000  0.000000  0.557400  0.793 0.340    \nV55LHE    0.005702  0.005915  0.964000  0.590500  0.000000  0.805 0.256    \nV61SPW    0.005691  0.005950  0.956000  0.000000  0.593000  0.817 0.051 .  \nV50CHE    0.005164  0.005411  0.954000  0.509600  0.000000  0.829 0.653    \nV86BCOO   0.004890  0.005087  0.961000  0.481700  0.000000  0.839 0.080 .  \nV43GCU    0.004840  0.005651  0.857000  1.256100  0.818400  0.850 0.869    \nV29CST    0.004252  0.005499  0.773000  0.314400  0.256000  0.859 0.960    \nV14AUR    0.004172  0.005415  0.771000  0.919800  1.211600  0.868 0.847    \nV3GF      0.004016  0.002231  1.800000  1.724600  1.330200  0.876 0.685    \nV79YTHE   0.003656  0.006555  0.558000  0.000000  0.396100  0.884 0.200    \nV30BTR    0.003534  0.006336  0.558000  0.000000  0.382900  0.892 0.802    \nV76GBB    0.003414  0.004520  0.755000  0.220000  0.210200  0.899 0.290    \nV5GWH     0.003383  0.002341  1.445000  1.486100  1.171900  0.907 0.783    \nV85ROSE   0.003252  0.005834  0.557000  0.331700  0.000000  0.914 0.336    \nV62RBTR   0.003172  0.005698  0.557000  0.281200  0.000000  0.920 0.373    \nV8WNHE    0.002763  0.002154  1.282000  1.709100  1.595400  0.926 0.959    \nV15STTH   0.002604  0.001970  1.321000  1.857500  1.627000  0.932 0.960    \nV57PINK   0.002450  0.004397  0.557000  0.250000  0.000000  0.937 0.451    \nV9SFW     0.002418  0.002010  1.203000  1.525000  1.472800  0.943 0.960    \nV94RBEE   0.002363  0.004237  0.558000  0.000000  0.256000  0.948 0.572    \nV96DF     0.002363  0.004237  0.558000  0.000000  0.256000  0.953 0.529    \nV102KING  0.002249  0.004035  0.557000  0.236400  0.000000  0.958 0.313    \nV7WEHE    0.002224  0.001655  1.344000  1.224500  1.296700  0.963 0.988    \nV51OWH    0.002157  0.003870  0.557000  0.220000  0.000000  0.967 0.576    \nV88WG     0.002119  0.003801  0.557000  0.000000  0.210200  0.972 0.477    \nV95HBC    0.002119  0.003801  0.557000  0.000000  0.210200  0.976 0.311    \nV12LK     0.002086  0.000765  2.726000  1.324100  1.122800  0.981 0.603    \nV63DWS    0.002031  0.003641  0.558000  0.000000  0.220000  0.985 0.925    \nV42SIL    0.002007  0.001772  1.133000  1.363600  1.255000  0.990 1.000    \nV1GST     0.001927  0.000613  3.143000  1.379200  1.425700  0.994 0.633    \nV41SPP    0.001767  0.001227  1.439000  1.352200  1.440100  0.998 0.951    \nV6WTTR    0.001142  0.000614  1.860000  1.567100  1.525100  1.000 0.996    \nV17WPHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV25WAG    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV27NHHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV31AMAG   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV32SCC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV34WSW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV38GAL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV45MGLK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV47RFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV52TRM    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV53MB     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV59YR     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV60LFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV65LWB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV69SKF    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV71PDOV   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV72CRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV73JW     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV74BCHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV84YRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV87LFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV91NMIN   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV92NFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV93DB     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV98FLAME  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV99WWT    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV101LCOR  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Montane Fore_River Red Gu \n\n           average        sd     ratio       ava       avb cumsum     p    \nV4BTH     0.023240  0.001423 16.334000  2.294000  0.000000  0.032 0.001 ***\nV32SCC    0.022619  0.002319  9.752000  0.000000  2.225000  0.062 0.002 ** \nV17WPHE   0.020648  0.001817 11.364000  0.000000  2.034900  0.090 0.005 ** \nV11CR     0.017360  0.001975  8.790000  1.710400  0.000000  0.114 0.003 ** \nV10WBSW   0.016529  0.001948  8.485000  1.622600  0.000000  0.136 0.005 ** \nV38GAL    0.016118  0.001999  8.065000  0.000000  1.582800  0.158 0.001 ***\nV30BTR    0.015451  0.001552  9.955000  0.000000  1.520300  0.179 0.001 ***\nV21ESP    0.015090  0.002641  5.713000  1.502100  0.000000  0.200 0.005 ** \nV2EYR     0.014573  0.001010 14.428000  1.440100  0.000000  0.220 0.001 ***\nV8WNHE    0.014442  0.006232  2.317000  1.709100  0.289600  0.239 0.007 ** \nV31AMAG   0.014171  0.001380 10.268000  0.000000  1.393900  0.258 0.002 ** \nV25WAG    0.013859  0.001151 12.039000  0.000000  1.365200  0.277 0.003 ** \nV41SPP    0.013725  0.001684  8.149000  1.352200  0.000000  0.296 0.005 ** \nV60LFB    0.013381  0.003234  4.138000  0.000000  1.312300  0.314 0.001 ***\nV47RFC    0.012687  0.001289  9.845000  0.000000  1.251400  0.331 0.002 ** \nV43GCU    0.012658  0.001321  9.581000  1.256100  0.000000  0.348 0.002 ** \nV7WEHE    0.012401  0.000859 14.431000  1.224500  0.000000  0.365 0.010 ** \nV26WWCH   0.012397  0.005637  2.199000  0.462400  1.578000  0.382 0.056 .  \nV83SFC    0.012343  0.000888 13.906000  1.220000  0.000000  0.399 0.003 ** \nV45MGLK   0.012337  0.001123 10.986000  0.000000  1.216900  0.416 0.003 ** \nV81SHBC   0.012054  0.001019 11.835000  1.191300  0.000000  0.432 0.005 ** \nV69SKF    0.011732  0.001094 10.720000  0.000000  1.156400  0.448 0.004 ** \nV42SIL    0.011523  0.004695  2.455000  1.363600  0.442300  0.464 0.040 *  \nV19ER     0.011415  0.005033  2.268000  0.266900  1.397300  0.479 0.028 *  \nV36YFHE   0.011410  0.006858  1.664000  1.134500  0.000000  0.495 0.112    \nV15STTH   0.011292  0.008100  1.394000  1.857500  0.745400  0.510 0.142    \nV70RSL    0.010957  0.007153  1.532000  0.000000  1.085200  0.525 0.011 *  \nV56FTC    0.010350  0.004225  2.450000  1.231300  0.220000  0.539 0.037 *  \nV37WHIP   0.010040  0.006016  1.669000  1.020400  0.000000  0.552 0.019 *  \nV48YTBC   0.009839  0.005880  1.673000  0.958400  0.000000  0.566 0.024 *  \nV34WSW    0.009813  0.006345  1.546000  0.000000  0.961700  0.579 0.185    \nV98FLAME  0.009699  0.006152  1.577000  0.000000  0.964700  0.592 0.013 *  \nV73JW     0.009395  0.005864  1.602000  0.000000  0.934100  0.605 0.001 ***\nV67GGC    0.009094  0.005611  1.621000  0.926600  0.000000  0.618 0.007 ** \nV80RF     0.009018  0.005525  1.632000  0.913500  0.000000  0.630 0.068 .  \nV52TRM    0.008836  0.005669  1.559000  0.000000  0.871300  0.642 0.029 *  \nV6WTTR    0.008775  0.007475  1.174000  1.567100  0.736100  0.654 0.281    \nV18YTH    0.008617  0.009023  0.955000  0.000000  0.845000  0.665 0.500    \nV24BFCS   0.008611  0.005836  1.475000  0.496700  1.328700  0.677 0.022 *  \nV93DB     0.008405  0.005073  1.657000  0.000000  0.829800  0.689 0.003 ** \nV91NMIN   0.007934  0.008238  0.963000  0.000000  0.767400  0.699 0.061 .  \nV46BHHE   0.007914  0.008183  0.967000  0.828800  0.000000  0.710 0.657    \nV49LYRE   0.007869  0.004819  1.633000  0.803100  0.000000  0.721 0.024 *  \nV33RWH    0.007804  0.006307  1.237000  0.753700  0.532700  0.731 0.430    \nV68PIL    0.007742  0.004750  1.630000  0.785200  0.000000  0.742 0.020 *  \nV54STHR   0.007627  0.004585  1.663000  0.758900  0.000000  0.752 0.008 ** \nV20PCU    0.007523  0.006020  1.250000  1.268100  0.602800  0.762 0.220    \nV94RBEE   0.007429  0.004516  1.645000  0.000000  0.732200  0.772 0.005 ** \nV5GWH     0.007231  0.004781  1.512000  1.486100  0.765000  0.782 0.216    \nV28VS     0.007207  0.007247  0.994000  0.709200  0.686000  0.792 0.759    \nV29CST    0.007165  0.004999  1.433000  0.314400  0.776000  0.802 0.338    \nV35STP    0.007050  0.006207  1.136000  0.944200  1.638300  0.811 0.760    \nV59YR     0.006644  0.006900  0.963000  0.000000  0.651800  0.820 0.013 *  \nV23RBFT   0.006615  0.005151  1.284000  0.883700  0.456500  0.829 0.883    \nV3GF      0.006534  0.001821  3.589000  1.724600  1.077300  0.838 0.416    \nV13RWB    0.006293  0.007149  0.880000  0.662400  0.000000  0.847 0.941    \nV16LR     0.006278  0.005139  1.222000  1.023200  1.252000  0.855 0.854    \nV88WG     0.006250  0.006536  0.956000  0.000000  0.612900  0.864 0.013 *  \nV90WTG    0.006250  0.006536  0.956000  0.000000  0.612900  0.872 0.070 .  \nV99WWT    0.006243  0.006502  0.960000  0.000000  0.615300  0.881 0.019 *  \nV58OBO    0.006193  0.006425  0.964000  0.000000  0.611700  0.889 0.257    \nV55LHE    0.005637  0.005827  0.967000  0.590500  0.000000  0.897 0.284    \nV72CRP    0.005399  0.005608  0.963000  0.000000  0.545600  0.904 0.016 *  \nV92NFB    0.005341  0.005564  0.960000  0.000000  0.513100  0.911 0.093 .  \nV71PDOV   0.005320  0.005756  0.924000  0.000000  0.530500  0.919 0.036 *  \nV50CHE    0.005101  0.005325  0.958000  0.509600  0.000000  0.926 0.686    \nV86BCOO   0.004831  0.005006  0.965000  0.481700  0.000000  0.932 0.120    \nV9SFW     0.004284  0.002149  1.993000  1.525000  1.145500  0.938 0.683    \nV12LK     0.004232  0.005626  0.752000  1.324100  0.910300  0.944 0.376    \nV14AUR    0.004098  0.005269  0.778000  0.919800  1.237700  0.949 0.845    \nV84YRTH   0.004003  0.007182  0.557000  0.000000  0.389600  0.955 0.190    \nV101LCOR  0.003592  0.006442  0.558000  0.000000  0.370000  0.960 0.183    \nV22SCR    0.003532  0.006323  0.559000  0.317500  0.000000  0.964 0.890    \nV85ROSE   0.003214  0.005752  0.559000  0.331700  0.000000  0.969 0.392    \nV62RBTR   0.003128  0.005600  0.559000  0.281200  0.000000  0.973 0.389    \nV87LFC    0.003051  0.005472  0.558000  0.000000  0.314400  0.977 0.613    \nV74BCHE   0.002765  0.004961  0.557000  0.000000  0.261700  0.981 0.180    \nV57PINK   0.002422  0.004335  0.559000  0.250000  0.000000  0.984 0.514    \nV76GBB    0.002296  0.004109  0.559000  0.220000  0.000000  0.987 0.607    \nV102KING  0.002224  0.003981  0.559000  0.236400  0.000000  0.990 0.369    \nV53MB     0.002160  0.003875  0.557000  0.000000  0.210200  0.993 0.874    \nV51OWH    0.002132  0.003815  0.559000  0.220000  0.000000  0.996 0.634    \nV95HBC    0.002101  0.003770  0.557000  0.000000  0.198800  0.999 0.348    \nV1GST     0.000715  0.000313  2.283000  1.379200  1.343300  1.000 0.998    \nV27NHHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV40BRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV61SPW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV63DWS    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV65LWB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV66CBW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV96DF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Foothills Wo_Box-Ironbark \n\n          average       sd    ratio      ava      avb cumsum     p    \nV39FHE   0.015833 0.002871 5.514000 0.000000 1.532700  0.047 0.001 ***\nV21ESP   0.014135 0.003111 4.543000 1.346100 0.000000  0.089 0.010 ** \nV44MUSK  0.012037 0.007747 1.554000 0.000000 1.180800  0.125 0.007 ** \nV75RCR   0.010634 0.002985 3.562000 0.000000 1.000900  0.157 0.001 ***\nV56FTC   0.010423 0.005633 1.850000 1.244700 0.281200  0.188 0.019 *  \nV16LR    0.009727 0.006963 1.397000 1.479800 0.576500  0.217 0.174    \nV40BRTH  0.009298 0.009205 1.010000 1.423800 1.379300  0.245 0.220    \nV69SKF   0.009269 0.005598 1.656000 0.890600 0.000000  0.273 0.023 *  \nV46BHHE  0.009105 0.006329 1.439000 0.931700 1.798900  0.300 0.334    \nV53MB    0.008985 0.005474 1.641000 0.851600 0.000000  0.327 0.018 *  \nV58OBO   0.008774 0.004921 1.783000 0.261700 1.059200  0.353 0.026 *  \nV35STP   0.008296 0.006636 1.250000 1.011200 0.656700  0.378 0.426    \nV10WBSW  0.008277 0.008818 0.939000 0.773600 0.000000  0.402 0.647    \nV18YTH   0.007632 0.008144 0.937000 1.199300 1.909900  0.425 0.577    \nV81SHBC  0.007489 0.005740 1.305000 0.807100 0.281200  0.447 0.169    \nV20PCU   0.006999 0.007268 0.963000 0.673000 0.000000  0.468 0.368    \nV19ER    0.006999 0.005848 1.197000 0.654600 0.846400  0.489 0.726    \nV29CST   0.006862 0.006603 1.039000 0.659900 0.256000  0.510 0.444    \nV43GCU   0.006594 0.005495 1.200000 0.583000 0.818400  0.529 0.536    \nV13RWB   0.006454 0.003518 1.834000 1.170600 1.792200  0.549 0.940    \nV11CR    0.006420 0.006540 0.982000 1.484400 0.992900  0.568 0.930    \nV28VS    0.006385 0.006620 0.964000 1.088900 0.971800  0.587 0.876    \nV61SPW   0.006374 0.006302 1.011000 0.326200 0.593000  0.606 0.025 *  \nV66CBW   0.006186 0.005875 1.053000 0.220000 0.566600  0.624 0.028 *  \nV26WWCH  0.006051 0.007359 0.822000 1.210300 1.655500  0.642 0.870    \nV90WTG   0.005966 0.006614 0.902000 0.000000 0.542000  0.660 0.089 .  \nV70RSL   0.005903 0.006116 0.965000 0.000000 0.557400  0.678 0.323    \nV9SFW    0.005785 0.005795 0.998000 1.081900 1.472800  0.695 0.493    \nV42SIL   0.005356 0.004742 1.130000 0.761200 1.255000  0.711 0.959    \nV2EYR    0.005255 0.005672 0.927000 1.316800 0.874900  0.727 0.749    \nV64BELL  0.005057 0.009076 0.557000 0.492000 0.000000  0.742 0.325    \nV4BTH    0.005028 0.003157 1.593000 1.975000 1.520900  0.757 0.842    \nV96DF    0.004848 0.006302 0.769000 0.349000 0.256000  0.771 0.084 .  \nV23RBFT  0.004574 0.005837 0.784000 0.243500 0.320500  0.785 0.981    \nV36YFHE  0.004518 0.002675 1.689000 1.319700 1.383500  0.798 0.977    \nV14AUR   0.004445 0.005204 0.854000 0.987000 1.211600  0.812 0.801    \nV50CHE   0.004163 0.007474 0.557000 0.384600 0.000000  0.824 0.792    \nV79YTHE  0.003726 0.006667 0.559000 0.000000 0.396100  0.835 0.176    \nV30BTR   0.003602 0.006445 0.559000 0.000000 0.382900  0.846 0.800    \nV63DWS   0.003558 0.004682 0.760000 0.243500 0.220000  0.857 0.689    \nV24BFCS  0.003325 0.004364 0.762000 0.794400 1.014400  0.867 0.880    \nV80RF    0.003296 0.005917 0.557000 0.304500 0.000000  0.876 0.871    \nV7WEHE   0.003045 0.002323 1.311000 1.318200 1.296700  0.885 0.920    \nV1GST    0.002847 0.001972 1.444000 1.577800 1.425700  0.894 0.132    \nV27NHHE  0.002771 0.004976 0.557000 0.256000 0.000000  0.902 0.964    \nV83SFC   0.002755 0.004946 0.557000 0.261700 0.000000  0.910 0.919    \nV15STTH  0.002694 0.001759 1.532000 1.788200 1.627000  0.918 0.954    \nV3GF     0.002586 0.002274 1.137000 1.569600 1.330200  0.926 0.947    \nV94RBEE  0.002409 0.004310 0.559000 0.000000 0.256000  0.933 0.560    \nV12LK    0.002400 0.001272 1.887000 1.350100 1.122800  0.941 0.526    \nV62RBTR  0.002317 0.004159 0.557000 0.220000 0.000000  0.947 0.593    \nV8WNHE   0.002311 0.000652 3.548000 1.582200 1.595400  0.954 0.978    \nV76GBB   0.002287 0.004092 0.559000 0.000000 0.210200  0.961 0.617    \nV22SCR   0.002258 0.001532 1.474000 1.364500 1.350800  0.968 0.972    \nV5GWH    0.002206 0.001744 1.265000 1.309900 1.171900  0.975 0.940    \nV88WG    0.002163 0.003870 0.559000 0.000000 0.210200  0.981 0.469    \nV95HBC   0.002163 0.003870 0.559000 0.000000 0.210200  0.987 0.234    \nV33RWH   0.001840 0.001210 1.520000 1.597500 1.512200  0.993 0.996    \nV41SPP   0.001677 0.001086 1.544000 1.526000 1.440100  0.998 0.968    \nV6WTTR   0.000709 0.000533 1.332000 1.579900 1.525100  1.000 1.000    \nV17WPHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV25WAG   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV31AMAG  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV32SCC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV34WSW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV37WHIP  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV38GAL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV45MGLK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV47RFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV48YTBC  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV49LYRE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV51OWH   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV52TRM   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV54STHR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV55LHE   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV57PINK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV60LFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV65LWB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV67GGC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV68PIL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV71PDOV  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV77RRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV78LLOR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV82AZKF  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV84YRTH  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV85ROSE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV86BCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV87LFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV89PCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV91NMIN  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV92NFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV93DB    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV97PCL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV98FLAME 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV100WBWS 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV102KING 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Foothills Wo_River Red Gu \n\n           average        sd     ratio       ava       avb cumsum     p    \nV32SCC    0.023093  0.001828 12.633000  0.000000  2.225000  0.035 0.001 ***\nV17WPHE   0.021080  0.001241 16.985000  0.000000  2.034900  0.067 0.008 ** \nV4BTH     0.020489  0.002659  7.707000  1.975000  0.000000  0.098 0.001 ***\nV38GAL    0.016457  0.001736  9.477000  0.000000  1.582800  0.123 0.001 ***\nV41SPP    0.015828  0.001134 13.956000  1.526000  0.000000  0.146 0.001 ***\nV30BTR    0.015775  0.001205 13.087000  0.000000  1.520300  0.170 0.002 ** \nV11CR     0.015410  0.001314 11.727000  1.484400  0.000000  0.194 0.010 ** \nV40BRTH   0.014624  0.009247  1.582000  1.423800  0.000000  0.216 0.009 ** \nV31AMAG   0.014468  0.001047 13.821000  0.000000  1.393900  0.238 0.001 ***\nV25WAG    0.014149  0.000727 19.475000  0.000000  1.365200  0.259 0.003 ** \nV22SCR    0.014138  0.001253 11.280000  1.364500  0.000000  0.280 0.001 ***\nV21ESP    0.013956  0.002919  4.781000  1.346100  0.000000  0.301 0.009 ** \nV7WEHE    0.013692  0.003128  4.377000  1.318200  0.000000  0.322 0.003 ** \nV2EYR     0.013677  0.002086  6.556000  1.316800  0.000000  0.343 0.001 ***\nV60LFB    0.013662  0.003176  4.302000  0.000000  1.312300  0.363 0.001 ***\nV36YFHE   0.013654  0.002185  6.250000  1.319700  0.000000  0.384 0.037 *  \nV8WNHE    0.013515  0.005982  2.259000  1.582200  0.289600  0.404 0.012 *  \nV47RFC    0.012953  0.001004 12.896000  0.000000  1.251400  0.424 0.002 ** \nV45MGLK   0.012595  0.000795 15.834000  0.000000  1.216900  0.443 0.002 ** \nV13RWB    0.012138  0.001836  6.609000  1.170600  0.000000  0.461 0.063 .  \nV70RSL    0.011186  0.007252  1.543000  0.000000  1.085200  0.478 0.006 ** \nV15STTH   0.011042  0.008000  1.380000  1.788200  0.745400  0.495 0.167    \nV33RWH    0.011004  0.005906  1.863000  1.597500  0.532700  0.511 0.037 *  \nV56FTC    0.010724  0.004330  2.477000  1.244700  0.220000  0.528 0.018 *  \nV34WSW    0.010020  0.006434  1.557000  0.000000  0.961700  0.543 0.166    \nV98FLAME  0.009901  0.006236  1.588000  0.000000  0.964700  0.558 0.008 ** \nV46BHHE   0.009696  0.006173  1.571000  0.931700  0.000000  0.573 0.242    \nV73JW     0.009590  0.005943  1.614000  0.000000  0.934100  0.587 0.001 ***\nV18YTH    0.009296  0.008040  1.156000  1.199300  0.845000  0.601 0.361    \nV52TRM    0.009021  0.005747  1.570000  0.000000  0.871300  0.615 0.023 *  \nV6WTTR    0.008899  0.007759  1.147000  1.579900  0.736100  0.628 0.292    \nV93DB     0.008581  0.005139  1.670000  0.000000  0.829800  0.641 0.003 ** \nV42SIL    0.008525  0.004802  1.776000  0.761200  0.442300  0.654 0.486    \nV81SHBC   0.008439  0.005218  1.617000  0.807100  0.000000  0.667 0.065 .  \nV10WBSW   0.008169  0.008666  0.943000  0.773600  0.000000  0.679 0.631    \nV91NMIN   0.008103  0.008379  0.967000  0.000000  0.767400  0.691 0.076 .  \nV19ER     0.007916  0.006665  1.188000  0.654600  1.397300  0.703 0.441    \nV53MB     0.007757  0.005181  1.497000  0.851600  0.210200  0.715 0.046 *  \nV28VS     0.007735  0.007384  1.048000  1.088900  0.686000  0.727 0.636    \nV94RBEE   0.007585  0.004574  1.658000  0.000000  0.732200  0.738 0.002 ** \nV20PCU    0.007072  0.006526  1.084000  0.673000  0.602800  0.749 0.326    \nV29CST    0.006866  0.005049  1.360000  0.659900  0.776000  0.759 0.462    \nV59YR     0.006784  0.007017  0.967000  0.000000  0.651800  0.769 0.010 ** \nV35STP    0.006652  0.006625  1.004000  1.011200  1.638300  0.779 0.785    \nV88WG     0.006382  0.006647  0.960000  0.000000  0.612900  0.789 0.010 ** \nV90WTG    0.006382  0.006647  0.960000  0.000000  0.612900  0.799 0.053 .  \nV99WWT    0.006373  0.006612  0.964000  0.000000  0.615300  0.808 0.013 *  \nV58OBO    0.006330  0.006124  1.034000  0.261700  0.611700  0.818 0.199    \nV26WWCH   0.005950  0.006536  0.910000  1.210300  1.578000  0.827 0.872    \nV43GCU    0.005949  0.006303  0.944000  0.583000  0.000000  0.836 0.716    \nV9SFW     0.005874  0.004177  1.406000  1.081900  1.145500  0.845 0.508    \nV5GWH     0.005644  0.004760  1.186000  1.309900  0.765000  0.853 0.521    \nV72CRP    0.005509  0.005702  0.966000  0.000000  0.545600  0.862 0.017 *  \nV24BFCS   0.005494  0.005150  1.067000  0.794400  1.328700  0.870 0.489    \nV92NFB    0.005455  0.005659  0.964000  0.000000  0.513100  0.878 0.079 .  \nV71PDOV   0.005431  0.005852  0.928000  0.000000  0.530500  0.886 0.023 *  \nV3GF      0.005121  0.001989  2.575000  1.569600  1.077300  0.894 0.543    \nV64BELL   0.004995  0.008940  0.559000  0.492000  0.000000  0.902 0.353    \nV23RBFT   0.004889  0.004924  0.993000  0.243500  0.456500  0.909 0.985    \nV12LK     0.004650  0.005759  0.808000  1.350100  0.910300  0.916 0.290    \nV14AUR    0.004157  0.005215  0.797000  0.987000  1.237700  0.922 0.856    \nV50CHE    0.004108  0.007354  0.559000  0.384600  0.000000  0.928 0.790    \nV84YRTH   0.004088  0.007314  0.559000  0.000000  0.389600  0.935 0.150    \nV69SKF    0.003666  0.005226  0.701000  0.890600  1.156400  0.940 0.812    \nV101LCOR  0.003664  0.006556  0.559000  0.000000  0.370000  0.946 0.155    \nV96DF     0.003588  0.006422  0.559000  0.349000  0.000000  0.951 0.229    \nV61SPW    0.003353  0.006002  0.559000  0.326200  0.000000  0.956 0.239    \nV80RF     0.003252  0.005822  0.559000  0.304500  0.000000  0.961 0.892    \nV87LFC    0.003113  0.005569  0.559000  0.000000  0.314400  0.966 0.622    \nV74BCHE   0.002825  0.005054  0.559000  0.000000  0.261700  0.970 0.144    \nV27NHHE   0.002735  0.004896  0.559000  0.256000  0.000000  0.974 0.975    \nV83SFC    0.002720  0.004869  0.559000  0.261700  0.000000  0.978 0.934    \nV16LR     0.002626  0.002077  1.264000  1.479800  1.252000  0.982 0.995    \nV1GST     0.002484  0.002388  1.040000  1.577800  1.343300  0.986 0.304    \nV63DWS    0.002472  0.004425  0.559000  0.243500  0.000000  0.990 0.829    \nV62RBTR   0.002287  0.004094  0.559000  0.220000  0.000000  0.993 0.596    \nV66CBW    0.002287  0.004094  0.559000  0.220000  0.000000  0.997 0.614    \nV95HBC    0.002146  0.003840  0.559000  0.000000  0.198800  1.000 0.302    \nV37WHIP   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV48YTBC   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV49LYRE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV51OWH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV54STHR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV55LHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV57PINK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV65LWB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV67GGC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV68PIL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV76GBB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV85ROSE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV86BCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV102KING  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Box-Ironbark_River Red Gu \n\n           average        sd     ratio       ava       avb cumsum     p    \nV32SCC    0.023209  0.002485  9.340000  0.000000  2.225000  0.036 0.003 ** \nV17WPHE   0.021185  0.001961 10.802000  0.000000  2.034900  0.068 0.005 ** \nV46BHHE   0.018779  0.002092  8.977000  1.798900  0.000000  0.097 0.001 ***\nV13RWB    0.018603  0.002860  6.505000  1.792200  0.000000  0.126 0.001 ***\nV38GAL    0.016539  0.002119  7.804000  0.000000  1.582800  0.151 0.002 ** \nV4BTH     0.015839  0.002532  6.255000  1.520900  0.000000  0.175 0.014 *  \nV39FHE    0.015712  0.002885  5.447000  1.532700  0.000000  0.200 0.001 ***\nV41SPP    0.014960  0.001224 12.222000  1.440100  0.000000  0.222 0.001 ***\nV31AMAG   0.014541  0.001484  9.795000  0.000000  1.393900  0.245 0.004 ** \nV36YFHE   0.014477  0.004922  2.941000  1.383500  0.000000  0.267 0.013 *  \nV25WAG    0.014220  0.001253 11.353000  0.000000  1.365200  0.289 0.001 ***\nV22SCR    0.014102  0.002784  5.066000  1.350800  0.000000  0.311 0.002 ** \nV40BRTH   0.013917  0.008320  1.673000  1.379300  0.000000  0.332 0.012 *  \nV8WNHE    0.013741  0.005715  2.405000  1.595400  0.289600  0.353 0.012 *  \nV60LFB    0.013731  0.003352  4.097000  0.000000  1.312300  0.374 0.002 ** \nV7WEHE    0.013625  0.003185  4.278000  1.296700  0.000000  0.395 0.006 ** \nV47RFC    0.013017  0.001371  9.494000  0.000000  1.251400  0.415 0.001 ***\nV45MGLK   0.012658  0.001205 10.507000  0.000000  1.216900  0.435 0.001 ***\nV30BTR    0.012531  0.007039  1.780000  0.382900  1.520300  0.454 0.012 *  \nV69SKF    0.012038  0.001175 10.245000  0.000000  1.156400  0.472 0.007 ** \nV44MUSK   0.011946  0.007696  1.552000  1.180800  0.000000  0.491 0.010 ** \nV18YTH    0.011253  0.009055  1.243000  1.909900  0.845000  0.508 0.103    \nV42SIL    0.011190  0.003985  2.808000  1.255000  0.442300  0.525 0.060 .  \nV35STP    0.010880  0.007777  1.399000  0.656700  1.638300  0.542 0.052 .  \nV75RCR    0.010550  0.002971  3.552000  1.000900  0.000000  0.558 0.001 ***\nV33RWH    0.010203  0.005935  1.719000  1.512200  0.532700  0.574 0.031 *  \nV34WSW    0.010070  0.006524  1.544000  0.000000  0.961700  0.589 0.158    \nV11CR     0.009963  0.006247  1.595000  0.992900  0.000000  0.605 0.368    \nV98FLAME  0.009949  0.006321  1.574000  0.000000  0.964700  0.620 0.005 ** \nV73JW     0.009637  0.006026  1.599000  0.000000  0.934100  0.635 0.001 ***\nV15STTH   0.009594  0.007905  1.214000  1.627000  0.745400  0.649 0.357    \nV52TRM    0.009066  0.005827  1.556000  0.000000  0.871300  0.663 0.026 *  \nV2EYR     0.009033  0.005592  1.615000  0.874900  0.000000  0.677 0.103    \nV93DB     0.008624  0.005214  1.654000  0.000000  0.829800  0.690 0.001 ***\nV70RSL    0.008584  0.006538  1.313000  0.557400  1.085200  0.704 0.064 .  \nV6WTTR    0.008576  0.007595  1.129000  1.525100  0.736100  0.717 0.308    \nV43GCU    0.008275  0.004993  1.657000  0.818400  0.000000  0.730 0.118    \nV91NMIN   0.008145  0.008464  0.962000  0.000000  0.767400  0.742 0.063 .  \nV16LR     0.007429  0.006376  1.165000  0.576500  1.252000  0.753 0.641    \nV28VS     0.007148  0.006803  1.051000  0.971800  0.686000  0.764 0.785    \nV29CST    0.006978  0.005342  1.306000  0.256000  0.776000  0.775 0.372    \nV59YR     0.006818  0.007086  0.962000  0.000000  0.651800  0.786 0.011 *  \nV58OBO    0.006742  0.004954  1.361000  1.059200  0.611700  0.796 0.138    \nV90WTG    0.006683  0.006034  1.107000  0.542000  0.612900  0.806 0.038 *  \nV94RBEE   0.006671  0.005063  1.318000  0.256000  0.732200  0.817 0.007 ** \nV88WG     0.006408  0.005997  1.069000  0.210200  0.612900  0.826 0.012 *  \nV99WWT    0.006405  0.006678  0.959000  0.000000  0.615300  0.836 0.010 ** \nV66CBW    0.006280  0.006500  0.966000  0.566600  0.000000  0.846 0.021 *  \nV19ER     0.006192  0.005991  1.034000  0.846400  1.397300  0.855 0.834    \nV20PCU    0.006104  0.006440  0.948000  0.000000  0.602800  0.865 0.720    \nV61SPW    0.005762  0.006007  0.959000  0.593000  0.000000  0.874 0.041 *  \nV23RBFT   0.005699  0.005229  1.090000  0.320500  0.456500  0.882 0.959    \nV72CRP    0.005535  0.005754  0.962000  0.000000  0.545600  0.891 0.015 *  \nV92NFB    0.005483  0.005717  0.959000  0.000000  0.513100  0.899 0.068 .  \nV71PDOV   0.005457  0.005906  0.924000  0.000000  0.530500  0.908 0.025 *  \nV5GWH     0.004967  0.004736  1.049000  1.171900  0.765000  0.915 0.667    \nV9SFW     0.004265  0.002744  1.554000  1.472800  1.145500  0.922 0.697    \nV84YRTH   0.004108  0.007375  0.557000  0.000000  0.389600  0.928 0.142    \nV56FTC    0.003830  0.004875  0.786000  0.281200  0.220000  0.934 0.956    \nV12LK     0.003782  0.004861  0.778000  1.122800  0.910300  0.940 0.381    \nV79YTHE   0.003700  0.006623  0.559000  0.396100  0.000000  0.946 0.176    \nV101LCOR  0.003681  0.006605  0.557000  0.000000  0.370000  0.951 0.143    \nV24BFCS   0.003288  0.002264  1.452000  1.014400  1.328700  0.956 0.907    \nV95HBC    0.003249  0.004283  0.759000  0.210200  0.198800  0.961 0.055 .  \nV3GF      0.003169  0.001979  1.601000  1.330200  1.077300  0.966 0.835    \nV87LFC    0.003127  0.005611  0.557000  0.000000  0.314400  0.971 0.613    \nV74BCHE   0.002840  0.005099  0.557000  0.000000  0.261700  0.975 0.131    \nV81SHBC   0.002627  0.004702  0.559000  0.281200  0.000000  0.979 0.990    \nV96DF     0.002392  0.004281  0.559000  0.256000  0.000000  0.983 0.522    \nV76GBB    0.002269  0.004061  0.559000  0.210200  0.000000  0.986 0.606    \nV53MB     0.002217  0.003979  0.557000  0.000000  0.210200  0.990 0.873    \nV63DWS    0.002056  0.003679  0.559000  0.220000  0.000000  0.993 0.896    \nV1GST     0.001950  0.001137  1.714000  1.425700  1.343300  0.996 0.631    \nV26WWCH   0.001348  0.001039  1.297000  1.655500  1.578000  0.998 0.987    \nV14AUR    0.001268  0.000601  2.111000  1.211600  1.237700  1.000 0.992    \nV10WBSW   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV21ESP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV27NHHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV37WHIP   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV48YTBC   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV49LYRE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV50CHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV51OWH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV54STHR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV55LHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV57PINK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV62RBTR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV65LWB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV67GGC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV68PIL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV80RF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV83SFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV85ROSE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV86BCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV102KING  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 999\n\n\nThe SIMPER analysis shows which species contribute most to the dissimilarity between habitat groups. This helps identify indicator species or species that drive community differences."
  },
  {
    "objectID": "tutorials/tutorial12.html#setup",
    "href": "tutorials/tutorial12.html#setup",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "To do these in R, you need to use the vegan package, so first load it and the MASS package:\n\nlibrary(vegan)\nlibrary(MASS)\nlibrary(ggplot2)"
  },
  {
    "objectID": "tutorials/tutorial12.html#load-the-data",
    "href": "tutorials/tutorial12.html#load-the-data",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "Next load the bird data:\n\nBirds &lt;- read.csv(\"data/macnally.csv\", header = TRUE)"
  },
  {
    "objectID": "tutorials/tutorial12.html#transform-the-data",
    "href": "tutorials/tutorial12.html#transform-the-data",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "Let’s transform the data using a fourth-root transformation:\n\nTransBirdsData &lt;- (Birds[, 3:104])^(1/4)"
  },
  {
    "objectID": "tutorials/tutorial12.html#bray-curtis-dissimilarity-matrix",
    "href": "tutorials/tutorial12.html#bray-curtis-dissimilarity-matrix",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "Now let’s generate a Bray-Curtis dissimilarity matrix on this data and perform a nMDS. Note this data has already been standardised.\n\nBird.dis &lt;- vegdist(TransBirdsData, method = \"bray\")"
  },
  {
    "objectID": "tutorials/tutorial12.html#nmds-ordination",
    "href": "tutorials/tutorial12.html#nmds-ordination",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "Let’s plot the nMDS and label with Habitat Type:\n\npchs &lt;- c(0:5)\nBird_Factor &lt;- factor(Birds$HABITAT)\nnMDS_Bird &lt;- metaMDS(TransBirdsData, distance = \"bray\", k = 2)\n\nRun 0 stress 0.1112209 \nRun 1 stress 0.1112209 \n... Procrustes: rmse 2.378988e-05  max resid 0.0001124603 \n... Similar to previous best\nRun 2 stress 0.1112209 \n... Procrustes: rmse 9.96519e-06  max resid 4.122183e-05 \n... Similar to previous best\nRun 3 stress 0.1268505 \nRun 4 stress 0.1112209 \n... Procrustes: rmse 3.758145e-05  max resid 0.0001780056 \n... Similar to previous best\nRun 5 stress 0.1417702 \nRun 6 stress 0.1112209 \n... Procrustes: rmse 4.312368e-05  max resid 0.0002048902 \n... Similar to previous best\nRun 7 stress 0.111221 \n... Procrustes: rmse 0.0001033651  max resid 0.0004882732 \n... Similar to previous best\nRun 8 stress 0.1112209 \n... Procrustes: rmse 2.339721e-05  max resid 0.000103012 \n... Similar to previous best\nRun 9 stress 0.1417215 \nRun 10 stress 0.1112209 \n... Procrustes: rmse 3.180471e-06  max resid 1.386164e-05 \n... Similar to previous best\nRun 11 stress 0.1112209 \n... Procrustes: rmse 5.780775e-06  max resid 2.582454e-05 \n... Similar to previous best\nRun 12 stress 0.1181205 \nRun 13 stress 0.1627193 \nRun 14 stress 0.1112209 \n... Procrustes: rmse 1.680034e-05  max resid 7.951508e-05 \n... Similar to previous best\nRun 15 stress 0.1112209 \n... Procrustes: rmse 1.63033e-05  max resid 7.724869e-05 \n... Similar to previous best\nRun 16 stress 0.1145319 \nRun 17 stress 0.1112209 \n... Procrustes: rmse 3.321037e-05  max resid 0.0001568831 \n... Similar to previous best\nRun 18 stress 0.1415031 \nRun 19 stress 0.1112209 \n... Procrustes: rmse 1.625044e-05  max resid 7.894352e-05 \n... Similar to previous best\nRun 20 stress 0.1389817 \n*** Best solution repeated 12 times\n\nplot_Bird &lt;- ordiplot(nMDS_Bird, display = \"sites\", type = \"n\")\npoints(nMDS_Bird, col = \"black\", pch = pchs[Bird_Factor])\nlegend(\"topright\", bty = \"n\", legend = levels(Bird_Factor), pch = pchs)\n\n\n\n\n\n\n\n\n\nnMDS_Bird\n\n\nCall:\nmetaMDS(comm = TransBirdsData, distance = \"bray\", k = 2) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     TransBirdsData \nDistance: bray \n\nDimensions: 2 \nStress:     0.1112209 \nStress type 1, weak ties\nBest solution was repeated 12 times in 20 tries\nThe best solution was from try 0 (metric scaling or null solution)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'TransBirdsData'"
  },
  {
    "objectID": "tutorials/tutorial12.html#anosim",
    "href": "tutorials/tutorial12.html#anosim",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "Let’s do an ANOSIM, using Habitat as the Factor:\n\nBird.anosim &lt;- with(TransBirdsData, anosim(Bird.dis, Birds$HABITAT))\nsummary(Bird.anosim)\n\n\nCall:\nanosim(x = Bird.dis, grouping = Birds$HABITAT) \nDissimilarity: bray \n\nANOSIM statistic R: 0.3936 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.135 0.179 0.223 0.264 \n\nDissimilarity ranks between and within classes:\n             0%    25%   50%    75% 100%   N\nBetween       1 224.25 380.5 520.25  665 500\nBox-Ironbark  6  22.50  29.5  45.50   78   6\nFoothills Wo 10  47.00  54.0  71.50  145   6\nGippsland Ma 51  72.25  90.0  95.00  130   6\nMixed         4 124.00 220.5 410.25  666 136\nMontane Fore  3  17.75  54.5  87.50   97   6\nRiver Red Gu 12  33.75  96.5 179.50  227   6"
  },
  {
    "objectID": "tutorials/tutorial12.html#permanova",
    "href": "tutorials/tutorial12.html#permanova",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "Let’s do a PERMANOVA, with HABITAT as a factor. PERMANOVA is called adonis in the vegan package. Note adonis is being replaced by adonis2, which is basically the same.\n\nadonis2(TransBirdsData ~ HABITAT, data = Birds, permutations = 999)\n\nPermutation test for adonis under reduced model\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = TransBirdsData ~ HABITAT, data = Birds, permutations = 999)\n         Df SumOfSqs      R2      F Pr(&gt;F)    \nModel     5   2.2749 0.50098 6.2245  0.001 ***\nResidual 31   2.2660 0.49902                  \nTotal    36   4.5409 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "tutorials/tutorial12.html#pairwise-comparisons",
    "href": "tutorials/tutorial12.html#pairwise-comparisons",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "If we find an overall significant p-value, we need to see which level is significant from which other level. Here we can do pairwise tests (a bit like post-hoc tests in ANOVAs), with the p-value adjusted with a Bonferroni correction.\nTo do the pairwise tests, there is a new R command on GitHub. Note the first time you do this, you need to first install Rtools from here.\n\nlibrary(devtools)\ninstall_github(\"pmartinezarbizu/pairwiseAdonis/pairwiseAdonis\")\n\n\nlibrary(pairwiseAdonis)\npairwise.adonis(TransBirdsData, Birds$HABITAT)"
  },
  {
    "objectID": "tutorials/tutorial12.html#simper-analysis",
    "href": "tutorials/tutorial12.html#simper-analysis",
    "title": "Tutorial 12: nMDS and PERMANOVA",
    "section": "",
    "text": "To see what species are contributing to the differences between the vegetation communities, we can use a SIMPER analysis (Similarity Percentages):\n\nBirdsim &lt;- simper(TransBirdsData, Birds$HABITAT)\nsummary(Birdsim)\n\n\nContrast: Mixed_Gippsland Ma \n\n          average       sd    ratio      ava      avb cumsum     p    \nV17WPHE  0.019779 0.011557 1.711400 0.602000 2.247700  0.040 0.001 ***\nV34WSW   0.018979 0.007112 2.668500 0.247500 1.856000  0.079 0.001 ***\nV11CR    0.016850 0.007496 2.247800 1.443400 0.000000  0.114 0.001 ***\nV6WTTR   0.015456 0.006359 2.430500 1.325800 0.000000  0.145 0.001 ***\nV15STTH  0.014940 0.009012 1.657800 1.475400 0.323400  0.176 0.002 ** \nV25WAG   0.013966 0.005802 2.407300 0.205100 1.376000  0.204 0.001 ***\nV27NHHE  0.012769 0.008435 1.513800 0.689200 1.646300  0.231 0.029 *  \nV3GF     0.012483 0.008488 1.470700 1.569300 0.646500  0.256 0.002 ** \nV4BTH    0.011472 0.009944 1.153700 1.946400 1.184200  0.280 0.096 .  \nV36YFHE  0.010708 0.008006 1.337500 1.121400 1.243700  0.302 0.100 .  \nV42SIL   0.010651 0.008608 1.237400 0.952800 0.420400  0.323 0.021 *  \nV13RWB   0.010585 0.007337 1.442700 1.080500 1.922100  0.345 0.133    \nV33RWH   0.010302 0.006859 1.501900 1.318700 0.579800  0.366 0.026 *  \nV5GWH    0.010203 0.008334 1.224100 1.325200 0.674800  0.387 0.004 ** \nV16LR    0.009557 0.008123 1.176500 0.852900 1.184900  0.407 0.117    \nV65LWB   0.009521 0.009391 1.013900 0.258700 0.741500  0.426 0.038 *  \nV21ESP   0.009484 0.007739 1.225400 1.236300 0.644000  0.445 0.139    \nV23RBFT  0.009142 0.006803 1.343900 0.990600 0.937700  0.464 0.241    \nV63DWS   0.008333 0.007483 1.113600 0.313100 0.724800  0.481 0.021 *  \nV35STP   0.008318 0.007536 1.103700 0.700100 0.993200  0.498 0.356    \nV7WEHE   0.008195 0.007490 1.094100 1.252500 1.241500  0.515 0.140    \nV91NMIN  0.008158 0.007918 1.030300 0.103300 0.719700  0.532 0.025 *  \nV19ER    0.007905 0.006671 1.184900 0.816000 1.308600  0.548 0.394    \nV29CST   0.007715 0.005456 1.414100 0.548200 1.059100  0.564 0.075 .  \nV56FTC   0.007610 0.006779 1.122500 0.948600 0.615700  0.579 0.105    \nV14AUR   0.007440 0.007024 1.059200 0.893600 0.962100  0.594 0.169    \nV81SHBC  0.007124 0.006361 1.120000 0.641600 0.271900  0.609 0.108    \nV28VS    0.007070 0.008926 0.792000 0.599300 0.000000  0.624 0.926    \nV12LK    0.006733 0.005709 1.179300 1.372500 0.801000  0.637 0.011 *  \nV80RF    0.006689 0.007273 0.919800 0.568000 0.000000  0.651 0.251    \nV50CHE   0.006645 0.008280 0.802400 0.580200 0.000000  0.664 0.355    \nV46BHHE  0.006634 0.006911 0.959900 0.491200 0.307900  0.678 0.960    \nV9SFW    0.006024 0.006604 0.912200 1.384400 1.734700  0.690 0.373    \nV48YTBC  0.005842 0.007167 0.815200 0.511500 0.000000  0.702 0.383    \nV41SPP   0.005728 0.005620 1.019200 1.058800 1.200500  0.714 0.716    \nV8WNHE   0.005294 0.006131 0.863400 1.588600 1.843200  0.725 0.820    \nV24BFCS  0.005165 0.005210 0.991300 0.751600 1.112500  0.736 0.535    \nV37WHIP  0.005130 0.007087 0.723800 0.441900 0.000000  0.746 0.522    \nV83SFC   0.005026 0.007077 0.710200 0.424600 0.000000  0.756 0.700    \nV43GCU   0.004955 0.006908 0.717300 0.423100 0.000000  0.766 0.966    \nV32SCC   0.004898 0.010846 0.451600 0.441300 0.000000  0.776 0.798    \nV20PCU   0.004806 0.006607 0.727400 0.411800 0.000000  0.786 0.948    \nV38GAL   0.004767 0.007107 0.670700 0.178700 0.323400  0.796 0.564    \nV87LFC   0.004719 0.006574 0.717800 0.224900 0.289600  0.806 0.141    \nV10WBSW  0.004538 0.005738 0.791000 1.426000 1.760400  0.815 0.998    \nV2EYR    0.004523 0.005813 0.778100 1.285300 1.599500  0.824 0.890    \nV53MB    0.004509 0.006393 0.705200 0.224200 0.250000  0.833 0.444    \nV31AMAG  0.004040 0.007500 0.538600 0.370900 0.000000  0.842 0.770    \nV55LHE   0.004024 0.006359 0.632900 0.354900 0.000000  0.850 0.512    \nV45MGLK  0.003945 0.005906 0.668000 0.142600 0.271900  0.858 0.505    \nV69SKF   0.003692 0.005819 0.634500 0.334500 0.000000  0.866 0.953    \nV70RSL   0.003469 0.005750 0.603200 0.094600 0.236400  0.873 0.676    \nV49LYRE  0.003229 0.005942 0.543500 0.268000 0.000000  0.879 0.683    \nV85ROSE  0.002816 0.005168 0.545000 0.239700 0.000000  0.885 0.466    \nV39FHE   0.002732 0.007692 0.355200 0.248500 0.000000  0.891 0.848    \nV68PIL   0.002684 0.005897 0.455200 0.232900 0.000000  0.896 0.677    \nV44MUSK  0.002563 0.007101 0.361000 0.236300 0.000000  0.901 0.757    \nV98FLAME 0.002557 0.005598 0.456700 0.220500 0.000000  0.907 0.691    \nV51OWH   0.002408 0.005315 0.453100 0.211500 0.000000  0.911 0.401    \nV1GST    0.002323 0.001704 1.363300 1.466600 1.493300  0.916 0.327    \nV22SCR   0.002318 0.005111 0.453500 0.210400 0.000000  0.921 0.999    \nV52TRM   0.002247 0.006427 0.349700 0.202800 0.000000  0.926 0.692    \nV47RFC   0.002229 0.004899 0.455000 0.214300 0.000000  0.930 0.866    \nV30BTR   0.002214 0.006274 0.352900 0.200700 0.000000  0.935 0.942    \nV58OBO   0.002124 0.004652 0.456500 0.198800 0.000000  0.939 0.987    \nV26WWCH  0.002007 0.005550 0.361500 0.180300 0.000000  0.943 1.000    \nV77RRP   0.001900 0.005290 0.359200 0.174100 0.000000  0.947 0.202    \nV76GBB   0.001874 0.004111 0.455800 0.166700 0.000000  0.951 0.593    \nV18YTH   0.001726 0.004872 0.354300 0.171800 0.000000  0.954 1.000    \nV86BCOO  0.001595 0.004421 0.360800 0.142300 0.000000  0.958 0.587    \nV67GGC   0.001558 0.004376 0.356000 0.135400 0.000000  0.961 0.780    \nV64BELL  0.001553 0.006272 0.247500 0.127500 0.000000  0.964 0.421    \nV62RBTR  0.001522 0.004260 0.357300 0.127400 0.000000  0.967 0.587    \nV57PINK  0.001480 0.004113 0.359800 0.127300 0.000000  0.970 0.409    \nV92NFB   0.001412 0.003904 0.361800 0.141400 0.000000  0.973 0.595    \nV97PCL   0.001237 0.004997 0.247500 0.102200 0.000000  0.976 0.095 .  \nV40BRTH  0.001151 0.004652 0.247400 0.088800 0.000000  0.978 0.995    \nV78LLOR  0.001038 0.004191 0.247800 0.106900 0.000000  0.980 0.119    \nV60LFB   0.000929 0.003754 0.247500 0.076800 0.000000  0.982 0.946    \nV84YRTH  0.000851 0.003434 0.247700 0.082700 0.000000  0.984 0.609    \nV89PCOO  0.000836 0.003378 0.247500 0.069100 0.000000  0.985 0.095 .  \nV54STHR  0.000766 0.003093 0.247700 0.073200 0.000000  0.987 0.827    \nV66CBW   0.000735 0.002968 0.247500 0.058800 0.000000  0.989 0.875    \nV79YTHE  0.000733 0.002957 0.247800 0.075400 0.000000  0.990 0.654    \nV96DF    0.000688 0.002777 0.247800 0.070800 0.000000  0.991 0.763    \nV82AZKF  0.000680 0.002747 0.247500 0.053800 0.000000  0.993 0.102    \nV100WBWS 0.000643 0.002594 0.247800 0.066200 0.000000  0.994 0.119    \nV93DB    0.000634 0.002557 0.247700 0.061600 0.000000  0.995 0.927    \nV102KING 0.000608 0.002454 0.247600 0.053800 0.000000  0.997 0.422    \nV90WTG   0.000572 0.002311 0.247700 0.055600 0.000000  0.998 0.957    \nV71PDOV  0.000540 0.002181 0.247800 0.055600 0.000000  0.999 0.789    \nV94RBEE  0.000523 0.002110 0.247800 0.053800 0.000000  1.000 0.978    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV61SPW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV88WG    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV95HBC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Mixed_Montane Fore \n\n          average       sd    ratio      ava      avb cumsum     p    \nV13RWB   0.009338 0.006981 1.337700 1.080500 0.662400  0.025 0.512    \nV43GCU   0.009042 0.005499 1.644400 0.423100 1.256100  0.049 0.015 *  \nV83SFC   0.008981 0.004949 1.814500 0.424600 1.220000  0.073 0.018 *  \nV20PCU   0.008970 0.005919 1.515400 0.411800 1.268100  0.097 0.019 *  \nV67GGC   0.008783 0.005683 1.545500 0.135400 0.926600  0.121 0.005 ** \nV16LR    0.008555 0.006520 1.312100 0.852900 1.023200  0.144 0.350    \nV46BHHE  0.008352 0.007086 1.178600 0.491200 0.828800  0.166 0.535    \nV33RWH   0.008286 0.006874 1.205500 1.318700 0.753700  0.188 0.210    \nV37WHIP  0.008217 0.006567 1.251200 0.441900 1.020400  0.211 0.039 *  \nV23RBFT  0.008056 0.006201 1.299200 0.990600 0.883700  0.232 0.729    \nV36YFHE  0.008026 0.007167 1.120000 1.121400 1.134500  0.254 0.865    \nV28VS    0.007929 0.007261 1.091900 0.599300 0.709200  0.275 0.613    \nV19ER    0.007909 0.006742 1.173100 0.816000 0.266900  0.296 0.374    \nV48YTBC  0.007759 0.006195 1.252500 0.511500 0.958400  0.317 0.054 .  \nV54STHR  0.007620 0.004661 1.634900 0.073200 0.758900  0.337 0.001 ***\nV68PIL   0.007514 0.004974 1.510900 0.232900 0.785200  0.358 0.015 *  \nV35STP   0.007322 0.006426 1.139400 0.700100 0.944200  0.377 0.776    \nV27NHHE  0.007298 0.008654 0.843300 0.689200 0.000000  0.397 0.673    \nV49LYRE  0.007170 0.005092 1.408100 0.268000 0.803100  0.416 0.021 *  \nV50CHE   0.006930 0.005950 1.164700 0.580200 0.509600  0.435 0.284    \nV80RF    0.006902 0.006031 1.144500 0.568000 0.913500  0.453 0.178    \nV42SIL   0.006868 0.005994 1.145900 0.952800 1.363600  0.472 0.969    \nV14AUR   0.006236 0.005868 1.062700 0.893600 0.919800  0.488 0.414    \nV55LHE   0.006136 0.005947 1.031900 0.354900 0.590500  0.505 0.115    \nV29CST   0.006120 0.006185 0.989500 0.548200 0.314400  0.521 0.880    \nV24BFCS  0.006092 0.005192 1.173200 0.751600 0.496700  0.537 0.259    \nV26WWCH  0.006052 0.009091 0.665800 0.180300 0.462400  0.554 0.972    \nV17WPHE  0.005965 0.009471 0.629800 0.602000 0.000000  0.570 0.991    \nV81SHBC  0.005961 0.005479 1.088100 0.641600 1.191300  0.586 0.883    \nV8WNHE   0.005525 0.005093 1.084800 1.588600 1.709100  0.601 0.778    \nV21ESP   0.005370 0.004726 1.136100 1.236300 1.502100  0.615 0.998    \nV15STTH  0.005290 0.006652 0.795300 1.475400 1.857500  0.629 0.947    \nV86BCOO  0.005217 0.005078 1.027400 0.142300 0.481700  0.643 0.054 .  \nV11CR    0.004834 0.005601 0.863000 1.443400 1.710400  0.656 1.000    \nV41SPP   0.004678 0.005784 0.808700 1.058800 1.352200  0.669 0.873    \nV22SCR   0.004598 0.006404 0.718100 0.210400 0.317500  0.681 0.923    \nV85ROSE  0.004568 0.005803 0.787200 0.239700 0.331700  0.693 0.087 .  \nV9SFW    0.004450 0.004930 0.902600 1.384400 1.525000  0.705 0.806    \nV32SCC   0.004345 0.009627 0.451300 0.441300 0.000000  0.717 0.854    \nV4BTH    0.004124 0.005531 0.745600 1.946400 2.294000  0.728 0.998    \nV56FTC   0.004054 0.005016 0.808300 0.948600 1.231300  0.739 0.990    \nV6WTTR   0.003848 0.005024 0.765900 1.325800 1.567100  0.749 0.974    \nV62RBTR  0.003800 0.005744 0.661600 0.127400 0.281200  0.759 0.122    \nV7WEHE   0.003646 0.003640 1.001600 1.252500 1.224500  0.769 0.990    \nV31AMAG  0.003590 0.006655 0.539500 0.370900 0.000000  0.779 0.824    \nV51OWH   0.003548 0.004941 0.718000 0.211500 0.220000  0.788 0.258    \nV5GWH    0.003405 0.004259 0.799600 1.325200 1.486100  0.797 0.954    \nV2EYR    0.003300 0.004546 0.725800 1.285300 1.440100  0.806 0.992    \nV69SKF   0.003277 0.005158 0.635200 0.334500 0.000000  0.815 0.981    \nV57PINK  0.003208 0.004800 0.668200 0.127300 0.250000  0.824 0.146    \nV76GBB   0.003203 0.004482 0.714700 0.166700 0.220000  0.832 0.177    \nV63DWS   0.003119 0.004987 0.625500 0.313100 0.000000  0.840 0.837    \nV10WBSW  0.002993 0.004868 0.614800 1.426000 1.622600  0.849 1.000    \nV3GF     0.002942 0.004175 0.704800 1.569300 1.724600  0.856 0.987    \nV65LWB   0.002877 0.008006 0.359400 0.258700 0.000000  0.864 0.599    \nV102KING 0.002549 0.004128 0.617500 0.053800 0.236400  0.871 0.088 .  \nV39FHE   0.002424 0.006810 0.356000 0.248500 0.000000  0.877 0.880    \nV34WSW   0.002385 0.005233 0.455700 0.247500 0.000000  0.884 0.996    \nV53MB    0.002366 0.005345 0.442500 0.224200 0.000000  0.890 0.897    \nV44MUSK  0.002278 0.006306 0.361200 0.236300 0.000000  0.896 0.802    \nV98FLAME 0.002256 0.004937 0.457000 0.220500 0.000000  0.902 0.802    \nV87LFC   0.002212 0.004906 0.450900 0.224900 0.000000  0.908 0.708    \nV52TRM   0.001993 0.005682 0.350700 0.202800 0.000000  0.914 0.744    \nV47RFC   0.001992 0.004375 0.455300 0.214300 0.000000  0.919 0.914    \nV30BTR   0.001964 0.005551 0.353800 0.200700 0.000000  0.924 0.947    \nV25WAG   0.001953 0.004286 0.455700 0.205100 0.000000  0.929 0.997    \nV58OBO   0.001891 0.004139 0.456800 0.198800 0.000000  0.934 0.987    \nV1GST    0.001754 0.001220 1.437400 1.466600 1.379200  0.939 0.874    \nV38GAL   0.001728 0.004792 0.360600 0.178700 0.000000  0.944 0.971    \nV77RRP   0.001687 0.004691 0.359700 0.174100 0.000000  0.948 0.377    \nV18YTH   0.001548 0.004368 0.354500 0.171800 0.000000  0.953 1.000    \nV45MGLK  0.001409 0.003896 0.361600 0.142600 0.000000  0.956 0.981    \nV64BELL  0.001363 0.005506 0.247500 0.127500 0.000000  0.960 0.584    \nV92NFB   0.001268 0.003504 0.361700 0.141400 0.000000  0.963 0.765    \nV12LK    0.001220 0.000891 1.368900 1.372500 1.324100  0.967 0.998    \nV97PCL   0.001087 0.004390 0.247500 0.102200 0.000000  0.970 0.477    \nV70RSL   0.001006 0.004066 0.247500 0.094600 0.000000  0.972 0.988    \nV40BRTH  0.001003 0.004051 0.247500 0.088800 0.000000  0.975 1.000    \nV91NMIN  0.000978 0.002735 0.357500 0.103300 0.000000  0.978 0.966    \nV78LLOR  0.000935 0.003773 0.247700 0.106900 0.000000  0.980 0.514    \nV60LFB   0.000816 0.003298 0.247500 0.076800 0.000000  0.982 0.953    \nV84YRTH  0.000761 0.003073 0.247700 0.082700 0.000000  0.984 0.791    \nV89PCOO  0.000734 0.002967 0.247500 0.069100 0.000000  0.986 0.477    \nV79YTHE  0.000659 0.002662 0.247700 0.075400 0.000000  0.988 0.815    \nV66CBW   0.000643 0.002597 0.247500 0.058800 0.000000  0.990 0.927    \nV96DF    0.000619 0.002499 0.247700 0.070800 0.000000  0.991 0.821    \nV82AZKF  0.000594 0.002400 0.247500 0.053800 0.000000  0.993 0.479    \nV100WBWS 0.000579 0.002335 0.247700 0.066200 0.000000  0.995 0.514    \nV93DB    0.000567 0.002289 0.247700 0.061600 0.000000  0.996 0.964    \nV90WTG   0.000512 0.002068 0.247700 0.055600 0.000000  0.997 0.987    \nV71PDOV  0.000486 0.001964 0.247700 0.055600 0.000000  0.999 0.888    \nV94RBEE  0.000470 0.001899 0.247700 0.053800 0.000000  1.000 0.984    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV61SPW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV88WG    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV95HBC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Mixed_Foothills Wo \n\n          average       sd    ratio      ava      avb cumsum     p   \nV40BRTH  0.014392 0.009230 1.559200 0.088800 1.423800  0.037 0.006 **\nV22SCR   0.012437 0.004863 2.557800 0.210400 1.364500  0.068 0.002 **\nV26WWCH  0.011907 0.007887 1.509700 0.180300 1.210300  0.099 0.021 * \nV18YTH   0.011886 0.007877 1.508900 0.171800 1.199300  0.129 0.039 * \nV10WBSW  0.009520 0.007570 1.257700 1.426000 0.773600  0.154 0.236   \nV23RBFT  0.009458 0.007442 1.270900 0.990600 0.243500  0.178 0.185   \nV28VS    0.009159 0.007107 1.288800 0.599300 1.088900  0.201 0.130   \nV53MB    0.008400 0.005483 1.532100 0.224200 0.851600  0.222 0.010 **\nV46BHHE  0.008159 0.006034 1.352200 0.491200 0.931700  0.243 0.638   \nV16LR    0.008067 0.006758 1.193700 0.852900 1.479800  0.264 0.534   \nV19ER    0.007778 0.006808 1.142400 0.816000 0.654600  0.284 0.438   \nV69SKF   0.007708 0.006107 1.262200 0.334500 0.890600  0.304 0.036 * \nV35STP   0.007702 0.006775 1.136900 0.700100 1.011200  0.323 0.643   \nV42SIL   0.007673 0.005328 1.440100 0.952800 0.761200  0.343 0.813   \nV27NHHE  0.007657 0.007842 0.976500 0.689200 0.256000  0.362 0.610   \nV50CHE   0.007225 0.007685 0.940200 0.580200 0.384600  0.381 0.227   \nV36YFHE  0.007150 0.005676 1.259600 1.121400 1.319700  0.399 0.980   \nV29CST   0.007090 0.006413 1.105700 0.548200 0.659900  0.417 0.289   \nV20PCU   0.007080 0.006583 1.075500 0.411800 0.673000  0.435 0.188   \nV9SFW    0.007048 0.006634 1.062400 1.384400 1.081900  0.453 0.190   \nV14AUR   0.006603 0.006170 1.070200 0.893600 0.987000  0.470 0.299   \nV43GCU   0.006573 0.006112 1.075300 0.423100 0.583000  0.487 0.485   \nV80RF    0.006348 0.006410 0.990200 0.568000 0.304500  0.503 0.380   \nV13RWB   0.006090 0.004647 1.310500 1.080500 1.170600  0.519 0.999   \nV17WPHE  0.006088 0.009642 0.631400 0.602000 0.000000  0.534 0.988   \nV64BELL  0.005858 0.009467 0.618800 0.127500 0.492000  0.549 0.144   \nV81SHBC  0.005798 0.005361 1.081400 0.641600 0.807100  0.564 0.929   \nV15STTH  0.005476 0.006465 0.847000 1.475400 1.788200  0.578 0.945   \nV83SFC   0.005382 0.006098 0.882700 0.424600 0.261700  0.592 0.588   \nV24BFCS  0.005280 0.004956 1.065500 0.751600 0.794400  0.605 0.499   \nV41SPP   0.005277 0.006727 0.784500 1.058800 1.526000  0.619 0.827   \nV48YTBC  0.005274 0.006448 0.817800 0.511500 0.000000  0.632 0.552   \nV8WNHE   0.005251 0.004942 1.062400 1.588600 1.582200  0.645 0.833   \nV11CR    0.004856 0.004784 1.015100 1.443400 1.484400  0.658 0.998   \nV21ESP   0.004753 0.004815 0.987100 1.236300 1.346100  0.670 0.998   \nV37WHIP  0.004623 0.006367 0.726100 0.441900 0.000000  0.682 0.681   \nV32SCC   0.004434 0.009801 0.452400 0.441300 0.000000  0.693 0.857   \nV63DWS   0.004331 0.005235 0.827300 0.313100 0.243500  0.704 0.579   \nV7WEHE   0.004274 0.004093 1.044200 1.252500 1.318200  0.715 0.970   \nV56FTC   0.004240 0.005159 0.821900 0.948600 1.244700  0.726 0.993   \nV33RWH   0.004130 0.005045 0.818600 1.318700 1.597500  0.736 0.993   \nV4BTH    0.004096 0.004957 0.826300 1.946400 1.975000  0.747 0.998   \nV96DF    0.003970 0.006435 0.617000 0.070800 0.349000  0.757 0.078 . \nV58OBO   0.003798 0.005307 0.715600 0.198800 0.261700  0.767 0.804   \nV6WTTR   0.003785 0.005196 0.728400 1.325800 1.579900  0.776 0.977   \nV2EYR    0.003733 0.004281 0.872000 1.285300 1.316800  0.786 0.969   \nV31AMAG  0.003663 0.006775 0.540700 0.370900 0.000000  0.795 0.824   \nV55LHE   0.003636 0.005729 0.634700 0.354900 0.000000  0.804 0.588   \nV5GWH    0.003634 0.003729 0.974400 1.325200 1.309900  0.814 0.927   \nV61SPW   0.003414 0.005979 0.571000 0.000000 0.326200  0.823 0.206   \nV3GF     0.003253 0.003748 0.868100 1.569300 1.569600  0.831 0.971   \nV62RBTR  0.003142 0.004683 0.670900 0.127400 0.220000  0.839 0.279   \nV65LWB   0.002942 0.008165 0.360300 0.258700 0.000000  0.846 0.618   \nV49LYRE  0.002899 0.005317 0.545200 0.268000 0.000000  0.854 0.704   \nV66CBW   0.002697 0.004351 0.619700 0.058800 0.220000  0.861 0.401   \nV85ROSE  0.002535 0.004640 0.546400 0.239700 0.000000  0.867 0.549   \nV39FHE   0.002474 0.006936 0.356600 0.248500 0.000000  0.874 0.870   \nV34WSW   0.002433 0.005328 0.456700 0.247500 0.000000  0.880 0.993   \nV1GST    0.002432 0.002103 1.156600 1.466600 1.577800  0.886 0.226   \nV68PIL   0.002421 0.005305 0.456500 0.232900 0.000000  0.892 0.704   \nV44MUSK  0.002324 0.006421 0.361900 0.236300 0.000000  0.898 0.785   \nV98FLAME 0.002304 0.005029 0.458100 0.220500 0.000000  0.904 0.767   \nV87LFC   0.002258 0.004995 0.452000 0.224900 0.000000  0.910 0.644   \nV51OWH   0.002175 0.004792 0.453900 0.211500 0.000000  0.915 0.440   \nV52TRM   0.002033 0.005787 0.351300 0.202800 0.000000  0.920 0.769   \nV47RFC   0.002030 0.004451 0.456200 0.214300 0.000000  0.926 0.915   \nV30BTR   0.002004 0.005654 0.354500 0.200700 0.000000  0.931 0.945   \nV25WAG   0.001992 0.004362 0.456700 0.205100 0.000000  0.936 0.996   \nV38GAL   0.001763 0.004879 0.361300 0.178700 0.000000  0.940 0.972   \nV77RRP   0.001722 0.004777 0.360400 0.174100 0.000000  0.945 0.379   \nV76GBB   0.001693 0.003703 0.457300 0.166700 0.000000  0.949 0.666   \nV86BCOO  0.001443 0.003991 0.361600 0.142300 0.000000  0.953 0.631   \nV45MGLK  0.001438 0.003967 0.362300 0.142600 0.000000  0.956 0.967   \nV12LK    0.001432 0.001053 1.360800 1.372500 1.350100  0.960 0.982   \nV67GGC   0.001406 0.003942 0.356500 0.135400 0.000000  0.964 0.868   \nV57PINK  0.001334 0.003700 0.360500 0.127300 0.000000  0.967 0.485   \nV92NFB   0.001291 0.003564 0.362400 0.141400 0.000000  0.970 0.752   \nV97PCL   0.001110 0.004475 0.248100 0.102200 0.000000  0.973 0.419   \nV70RSL   0.001028 0.004145 0.248100 0.094600 0.000000  0.976 0.991   \nV91NMIN  0.000997 0.002784 0.358300 0.103300 0.000000  0.978 0.975   \nV78LLOR  0.000952 0.003836 0.248100 0.106900 0.000000  0.981 0.423   \nV60LFB   0.000834 0.003362 0.248100 0.076800 0.000000  0.983 0.963   \nV84YRTH  0.000776 0.003127 0.248100 0.082700 0.000000  0.985 0.754   \nV89PCOO  0.000750 0.003025 0.248100 0.069100 0.000000  0.987 0.419   \nV54STHR  0.000698 0.002812 0.248100 0.073200 0.000000  0.989 0.855   \nV79YTHE  0.000672 0.002706 0.248100 0.075400 0.000000  0.990 0.759   \nV82AZKF  0.000607 0.002448 0.248100 0.053800 0.000000  0.992 0.414   \nV100WBWS 0.000589 0.002375 0.248100 0.066200 0.000000  0.993 0.422   \nV93DB    0.000578 0.002329 0.248100 0.061600 0.000000  0.995 0.961   \nV102KING 0.000549 0.002214 0.248100 0.053800 0.000000  0.996 0.597   \nV90WTG   0.000522 0.002104 0.248100 0.055600 0.000000  0.998 0.972   \nV71PDOV  0.000495 0.001997 0.248100 0.055600 0.000000  0.999 0.894   \nV94RBEE  0.000479 0.001931 0.248100 0.053800 0.000000  1.000 0.988   \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV88WG    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV95HBC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Mixed_Box-Ironbark \n\n          average       sd    ratio      ava      avb cumsum     p    \nV18YTH   0.018658 0.005852 3.188000 0.171800 1.909900  0.039 0.001 ***\nV26WWCH  0.015902 0.005536 2.872000 0.180300 1.655500  0.072 0.001 ***\nV10WBSW  0.015288 0.006045 2.529000 1.426000 0.000000  0.103 0.002 ** \nV39FHE   0.014967 0.004566 3.278000 0.248500 1.532700  0.134 0.001 ***\nV46BHHE  0.014098 0.007088 1.989000 0.491200 1.798900  0.163 0.001 ***\nV40BRTH  0.013699 0.008353 1.640000 0.088800 1.379300  0.192 0.007 ** \nV21ESP   0.013293 0.006583 2.019000 1.236300 0.000000  0.219 0.002 ** \nV22SCR   0.012484 0.005255 2.376000 0.210400 1.350800  0.245 0.002 ** \nV44MUSK  0.011843 0.007846 1.509000 0.236300 1.180800  0.269 0.006 ** \nV75RCR   0.010748 0.003075 3.495000 0.000000 1.000900  0.292 0.001 ***\nV58OBO   0.009787 0.004514 2.168000 0.198800 1.059200  0.312 0.003 ** \nV23RBFT  0.009490 0.007770 1.221000 0.990600 0.320500  0.332 0.181    \nV13RWB   0.008878 0.006402 1.387000 1.080500 1.792200  0.350 0.691    \nV56FTC   0.008670 0.006269 1.383000 0.948600 0.281200  0.368 0.030 *  \nV28VS    0.008644 0.006295 1.373000 0.599300 0.971800  0.386 0.264    \nV11CR    0.008437 0.007081 1.192000 1.443400 0.992900  0.403 0.736    \nV36YFHE  0.008272 0.006141 1.347000 1.121400 1.383500  0.420 0.825    \nV16LR    0.008079 0.006977 1.158000 0.852900 0.576500  0.437 0.511    \nV35STP   0.007717 0.006654 1.160000 0.700100 0.656700  0.453 0.648    \nV27NHHE  0.007497 0.008904 0.842000 0.689200 0.000000  0.468 0.635    \nV19ER    0.007343 0.005945 1.235000 0.816000 0.846400  0.484 0.652    \nV43GCU   0.007068 0.005570 1.269000 0.423100 0.818400  0.498 0.289    \nV42SIL   0.006994 0.005548 1.261000 0.952800 1.255000  0.513 0.948    \nV4BTH    0.006788 0.003994 1.699000 1.946400 1.520900  0.527 0.831    \nV2EYR    0.006709 0.005999 1.118000 1.285300 0.874900  0.541 0.402    \nV81SHBC  0.006489 0.005846 1.110000 0.641600 0.281200  0.554 0.416    \nV66CBW   0.006361 0.006379 0.997000 0.058800 0.566600  0.567 0.010 ** \nV70RSL   0.006278 0.006199 1.013000 0.094600 0.557400  0.580 0.144    \nV17WPHE  0.006117 0.009722 0.629000 0.602000 0.000000  0.593 0.984    \nV80RF    0.006050 0.006578 0.920000 0.568000 0.000000  0.605 0.498    \nV90WTG   0.006046 0.006464 0.935000 0.055600 0.542000  0.618 0.047 *  \nV50CHE   0.006025 0.007514 0.802000 0.580200 0.000000  0.630 0.485    \nV29CST   0.005983 0.006099 0.981000 0.548200 0.256000  0.643 0.929    \nV61SPW   0.005859 0.005982 0.980000 0.000000 0.593000  0.655 0.024 *  \nV14AUR   0.005388 0.005537 0.973000 0.893600 1.211600  0.666 0.717    \nV48YTBC  0.005300 0.006506 0.815000 0.511500 0.000000  0.677 0.562    \nV15STTH  0.005211 0.006022 0.865000 1.475400 1.627000  0.688 0.956    \nV9SFW    0.005137 0.005014 1.024000 1.384400 1.472800  0.698 0.586    \nV41SPP   0.004977 0.006330 0.786000 1.058800 1.440100  0.709 0.826    \nV30BTR   0.004843 0.007338 0.660000 0.200700 0.382900  0.719 0.539    \nV5GWH    0.004733 0.003669 1.290000 1.325200 1.171900  0.729 0.706    \nV3GF     0.004725 0.003539 1.335000 1.569300 1.330200  0.738 0.691    \nV8WNHE   0.004707 0.005030 0.936000 1.588600 1.595400  0.748 0.923    \nV37WHIP  0.004647 0.006424 0.723000 0.441900 0.000000  0.758 0.690    \nV83SFC   0.004543 0.006404 0.710000 0.424600 0.000000  0.767 0.826    \nV24BFCS  0.004457 0.004309 1.034000 0.751600 1.014400  0.776 0.803    \nV32SCC   0.004454 0.009873 0.451000 0.441300 0.000000  0.785 0.839    \nV20PCU   0.004351 0.005983 0.727000 0.411800 0.000000  0.794 0.980    \nV79YTHE  0.004127 0.006650 0.621000 0.075400 0.396100  0.803 0.029 *  \nV63DWS   0.004112 0.005037 0.816000 0.313100 0.220000  0.811 0.571    \nV7WEHE   0.004030 0.003983 1.012000 1.252500 1.296700  0.820 0.976    \nV33RWH   0.003758 0.004830 0.778000 1.318700 1.512200  0.828 0.996    \nV6WTTR   0.003705 0.004996 0.741000 1.325800 1.525100  0.835 0.979    \nV31AMAG  0.003680 0.006826 0.539000 0.370900 0.000000  0.843 0.818    \nV55LHE   0.003654 0.005776 0.633000 0.354900 0.000000  0.850 0.602    \nV69SKF   0.003359 0.005292 0.635000 0.334500 0.000000  0.857 0.977    \nV76GBB   0.003230 0.004486 0.720000 0.166700 0.210200  0.864 0.173    \nV65LWB   0.002960 0.008240 0.359000 0.258700 0.000000  0.870 0.603    \nV49LYRE  0.002915 0.005365 0.543000 0.268000 0.000000  0.876 0.705    \nV96DF    0.002818 0.004531 0.622000 0.070800 0.256000  0.882 0.205    \nV12LK    0.002704 0.001574 1.718000 1.372500 1.122800  0.888 0.590    \nV94RBEE  0.002692 0.004346 0.620000 0.053800 0.256000  0.893 0.418    \nV85ROSE  0.002548 0.004679 0.545000 0.239700 0.000000  0.898 0.540    \nV34WSW   0.002444 0.005368 0.455000 0.247500 0.000000  0.903 0.992    \nV68PIL   0.002434 0.005347 0.455000 0.232900 0.000000  0.908 0.732    \nV53MB    0.002430 0.005496 0.442000 0.224200 0.000000  0.914 0.885    \nV1GST    0.002363 0.001590 1.487000 1.466600 1.425700  0.918 0.277    \nV98FLAME 0.002315 0.005070 0.457000 0.220500 0.000000  0.923 0.801    \nV87LFC   0.002268 0.005033 0.451000 0.224900 0.000000  0.928 0.654    \nV51OWH   0.002186 0.004829 0.453000 0.211500 0.000000  0.932 0.453    \nV88WG    0.002185 0.003827 0.571000 0.000000 0.210200  0.937 0.286    \nV95HBC   0.002185 0.003827 0.571000 0.000000 0.210200  0.942 0.171    \nV52TRM   0.002043 0.005832 0.350000 0.202800 0.000000  0.946 0.747    \nV47RFC   0.002039 0.004481 0.455000 0.214300 0.000000  0.950 0.901    \nV25WAG   0.002001 0.004394 0.455000 0.205100 0.000000  0.954 0.997    \nV38GAL   0.001771 0.004914 0.360000 0.178700 0.000000  0.958 0.968    \nV77RRP   0.001730 0.004812 0.359000 0.174100 0.000000  0.961 0.301    \nV86BCOO  0.001450 0.004021 0.361000 0.142300 0.000000  0.964 0.645    \nV45MGLK  0.001444 0.003997 0.361000 0.142600 0.000000  0.967 0.966    \nV67GGC   0.001413 0.003972 0.356000 0.135400 0.000000  0.970 0.867    \nV64BELL  0.001400 0.005660 0.247000 0.127500 0.000000  0.973 0.549    \nV62RBTR  0.001374 0.003843 0.358000 0.127400 0.000000  0.976 0.684    \nV57PINK  0.001341 0.003729 0.359000 0.127300 0.000000  0.979 0.493    \nV92NFB   0.001296 0.003585 0.362000 0.141400 0.000000  0.981 0.709    \nV97PCL   0.001116 0.004512 0.247000 0.102200 0.000000  0.984 0.381    \nV91NMIN  0.001002 0.002802 0.357000 0.103300 0.000000  0.986 0.972    \nV78LLOR  0.000955 0.003858 0.248000 0.106900 0.000000  0.988 0.378    \nV60LFB   0.000839 0.003390 0.247000 0.076800 0.000000  0.990 0.956    \nV84YRTH  0.000779 0.003146 0.248000 0.082700 0.000000  0.991 0.724    \nV89PCOO  0.000755 0.003050 0.247000 0.069100 0.000000  0.993 0.381    \nV54STHR  0.000701 0.002830 0.248000 0.073200 0.000000  0.994 0.871    \nV82AZKF  0.000611 0.002470 0.247000 0.053800 0.000000  0.995 0.366    \nV100WBWS 0.000591 0.002388 0.248000 0.066200 0.000000  0.997 0.378    \nV93DB    0.000580 0.002343 0.248000 0.061600 0.000000  0.998 0.948    \nV102KING 0.000552 0.002230 0.247000 0.053800 0.000000  0.999 0.604    \nV71PDOV  0.000497 0.002008 0.248000 0.055600 0.000000  1.000 0.857    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Mixed_River Red Gu \n\n          average       sd    ratio      ava      avb cumsum     p    \nV4BTH    0.020445 0.006218 3.288000 1.946400 0.000000  0.030 0.001 ***\nV32SCC   0.020280 0.007521 2.697000 0.441300 2.225000  0.060 0.001 ***\nV17WPHE  0.015917 0.009167 1.736000 0.602000 2.034900  0.084 0.028 *  \nV10WBSW  0.015087 0.005860 2.574000 1.426000 0.000000  0.106 0.001 ***\nV11CR    0.015054 0.006597 2.282000 1.443400 0.000000  0.128 0.004 ** \nV38GAL   0.014937 0.005417 2.758000 0.178700 1.582800  0.151 0.002 ** \nV26WWCH  0.014881 0.004857 3.064000 0.180300 1.578000  0.173 0.002 ** \nV8WNHE   0.014481 0.007804 1.856000 1.588600 0.289600  0.194 0.002 ** \nV30BTR   0.014462 0.004730 3.057000 0.200700 1.520300  0.216 0.001 ***\nV2EYR    0.013450 0.005370 2.505000 1.285300 0.000000  0.236 0.001 ***\nV7WEHE   0.013287 0.005547 2.395000 1.252500 0.000000  0.255 0.001 ***\nV60LFB   0.013139 0.004117 3.192000 0.076800 1.312300  0.275 0.001 ***\nV21ESP   0.013118 0.006422 2.043000 1.236300 0.000000  0.294 0.001 ***\nV25WAG   0.012318 0.004959 2.484000 0.205100 1.365200  0.312 0.004 ** \nV31AMAG  0.011976 0.005536 2.163000 0.370900 1.393900  0.330 0.001 ***\nV45MGLK  0.011411 0.003990 2.860000 0.142600 1.216900  0.347 0.002 ** \nV36YFHE  0.011370 0.007879 1.443000 1.121400 0.000000  0.364 0.047 *  \nV13RWB   0.011326 0.007442 1.522000 1.080500 0.000000  0.381 0.040 *  \nV47RFC   0.011238 0.004799 2.342000 0.214300 1.251400  0.397 0.001 ***\nV70RSL   0.011018 0.007241 1.522000 0.094600 1.085200  0.414 0.003 ** \nV41SPP   0.010892 0.006237 1.746000 1.058800 0.000000  0.430 0.013 *  \nV15STTH  0.010499 0.008202 1.280000 1.475400 0.745400  0.445 0.137    \nV35STP   0.010090 0.007467 1.351000 0.700100 1.638300  0.460 0.026 *  \nV73JW    0.009687 0.005914 1.638000 0.000000 0.934100  0.474 0.001 ***\nV33RWH   0.009674 0.006163 1.570000 1.318700 0.532700  0.489 0.036 *  \nV42SIL   0.009667 0.007712 1.253000 0.952800 0.442300  0.503 0.112    \nV34WSW   0.009358 0.006570 1.424000 0.247500 0.961700  0.517 0.156    \nV52TRM   0.009124 0.005824 1.567000 0.202800 0.871300  0.531 0.009 ** \nV98FLAME 0.009079 0.006396 1.419000 0.220500 0.964700  0.544 0.006 ** \nV69SKF   0.008992 0.005367 1.675000 0.334500 1.156400  0.557 0.004 ** \nV18YTH   0.008930 0.008900 1.003000 0.171800 0.845000  0.571 0.288    \nV23RBFT  0.008751 0.006449 1.357000 0.990600 0.456500  0.584 0.398    \nV6WTTR   0.008747 0.007272 1.203000 1.325800 0.736100  0.597 0.182    \nV56FTC   0.008722 0.005609 1.555000 0.948600 0.220000  0.609 0.033 *  \nV93DB    0.008385 0.005276 1.589000 0.061600 0.829800  0.622 0.002 ** \nV91NMIN  0.008172 0.007902 1.034000 0.103300 0.767400  0.634 0.031 *  \nV28VS    0.007982 0.007161 1.115000 0.599300 0.686000  0.646 0.612    \nV5GWH    0.007767 0.005012 1.550000 1.325200 0.765000  0.657 0.102    \nV94RBEE  0.007444 0.004665 1.596000 0.053800 0.732200  0.668 0.003 ** \nV27NHHE  0.007397 0.008745 0.846000 0.689200 0.000000  0.679 0.662    \nV19ER    0.007375 0.006436 1.146000 0.816000 1.397300  0.690 0.649    \nV16LR    0.006856 0.005658 1.212000 0.852900 1.252000  0.700 0.858    \nV59YR    0.006854 0.006952 0.986000 0.000000 0.651800  0.711 0.007 ** \nV81SHBC  0.006705 0.005721 1.172000 0.641600 0.000000  0.720 0.277    \nV3GF     0.006585 0.003090 2.131000 1.569300 1.077300  0.730 0.303    \nV29CST   0.006553 0.005132 1.277000 0.548200 0.776000  0.740 0.612    \nV20PCU   0.006505 0.006055 1.074000 0.411800 0.602800  0.750 0.508    \nV88WG    0.006448 0.006586 0.979000 0.000000 0.612900  0.759 0.007 ** \nV90WTG   0.006446 0.006479 0.995000 0.055600 0.612900  0.769 0.021 *  \nV99WWT   0.006439 0.006551 0.983000 0.000000 0.615300  0.778 0.007 ** \nV58OBO   0.006407 0.006318 1.014000 0.198800 0.611700  0.788 0.108    \nV24BFCS  0.006242 0.005422 1.151000 0.751600 1.328700  0.797 0.201    \nV9SFW    0.006159 0.003969 1.552000 1.384400 1.145500  0.806 0.345    \nV80RF    0.005971 0.006465 0.924000 0.568000 0.000000  0.815 0.518    \nV50CHE   0.005950 0.007394 0.805000 0.580200 0.000000  0.824 0.534    \nV92NFB   0.005676 0.005583 1.017000 0.141400 0.513100  0.832 0.030 *  \nV72CRP   0.005564 0.005644 0.986000 0.000000 0.545600  0.840 0.007 ** \nV71PDOV  0.005528 0.005717 0.967000 0.055600 0.530500  0.849 0.010 ** \nV48YTBC  0.005233 0.006402 0.817000 0.511500 0.000000  0.856 0.563    \nV14AUR   0.005141 0.005536 0.929000 0.893600 1.237700  0.864 0.764    \nV12LK    0.005031 0.005700 0.883000 1.372500 0.910300  0.871 0.139    \nV46BHHE  0.004971 0.006225 0.799000 0.491200 0.000000  0.879 0.999    \nV37WHIP  0.004588 0.006321 0.726000 0.441900 0.000000  0.885 0.696    \nV84YRTH  0.004512 0.007311 0.617000 0.082700 0.389600  0.892 0.040 *  \nV83SFC   0.004484 0.006299 0.712000 0.424600 0.000000  0.899 0.823    \nV43GCU   0.004425 0.006145 0.720000 0.423100 0.000000  0.905 0.984    \nV87LFC   0.004386 0.006114 0.717000 0.224900 0.314400  0.912 0.210    \nV53MB    0.003834 0.005402 0.710000 0.224200 0.210200  0.917 0.625    \nV101LCOR 0.003699 0.006477 0.571000 0.000000 0.370000  0.923 0.072 .  \nV55LHE   0.003608 0.005688 0.634000 0.354900 0.000000  0.928 0.622    \nV63DWS   0.003160 0.005040 0.627000 0.313100 0.000000  0.933 0.844    \nV65LWB   0.002918 0.008100 0.360000 0.258700 0.000000  0.937 0.595    \nV49LYRE  0.002876 0.005277 0.545000 0.268000 0.000000  0.942 0.713    \nV74BCHE  0.002855 0.005003 0.571000 0.000000 0.261700  0.946 0.058 .  \nV85ROSE  0.002515 0.004606 0.546000 0.239700 0.000000  0.950 0.570    \nV39FHE   0.002455 0.006886 0.357000 0.248500 0.000000  0.953 0.880    \nV68PIL   0.002403 0.005266 0.456000 0.232900 0.000000  0.957 0.716    \nV44MUSK  0.002307 0.006375 0.362000 0.236300 0.000000  0.960 0.799    \nV95HBC   0.002170 0.003801 0.571000 0.000000 0.198800  0.963 0.156    \nV51OWH   0.002158 0.004757 0.454000 0.211500 0.000000  0.967 0.475    \nV22SCR   0.002084 0.004592 0.454000 0.210400 0.000000  0.970 1.000    \nV1GST    0.002006 0.001368 1.466000 1.466600 1.343300  0.973 0.641    \nV77RRP   0.001709 0.004743 0.360000 0.174100 0.000000  0.975 0.387    \nV76GBB   0.001680 0.003676 0.457000 0.166700 0.000000  0.978 0.673    \nV86BCOO  0.001432 0.003963 0.361000 0.142300 0.000000  0.980 0.619    \nV67GGC   0.001395 0.003914 0.356000 0.135400 0.000000  0.982 0.870    \nV64BELL  0.001382 0.005570 0.248000 0.127500 0.000000  0.984 0.550    \nV62RBTR  0.001356 0.003781 0.359000 0.127400 0.000000  0.986 0.659    \nV57PINK  0.001324 0.003673 0.360000 0.127300 0.000000  0.988 0.504    \nV97PCL   0.001101 0.004441 0.248000 0.102200 0.000000  0.990 0.430    \nV40BRTH  0.001017 0.004100 0.248000 0.088800 0.000000  0.991 0.995    \nV78LLOR  0.000946 0.003812 0.248000 0.106900 0.000000  0.992 0.472    \nV89PCOO  0.000744 0.003002 0.248000 0.069100 0.000000  0.994 0.430    \nV54STHR  0.000693 0.002793 0.248000 0.073200 0.000000  0.995 0.857    \nV79YTHE  0.000667 0.002689 0.248000 0.075400 0.000000  0.996 0.774    \nV66CBW   0.000652 0.002628 0.248000 0.058800 0.000000  0.997 0.924    \nV96DF    0.000626 0.002525 0.248000 0.070800 0.000000  0.997 0.826    \nV82AZKF  0.000602 0.002428 0.248000 0.053800 0.000000  0.998 0.453    \nV100WBWS 0.000585 0.002359 0.248000 0.066200 0.000000  0.999 0.472    \nV102KING 0.000545 0.002198 0.248000 0.053800 0.000000  1.000 0.601    \nV61SPW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Gippsland Ma_Montane Fore \n\n           average        sd     ratio       ava       avb cumsum     p    \nV17WPHE   0.025511  0.002976  8.572000  2.247700  0.000000  0.047 0.001 ***\nV34WSW    0.020923  0.002668  7.841000  1.856000  0.000000  0.086 0.001 ***\nV11CR     0.019365  0.002542  7.617000  0.000000  1.710400  0.121 0.001 ***\nV27NHHE   0.018629  0.005197  3.584000  1.646300  0.000000  0.156 0.001 ***\nV6WTTR    0.017764  0.002227  7.977000  0.000000  1.567100  0.189 0.002 ** \nV15STTH   0.017530  0.007233  2.423000  0.323400  1.857500  0.221 0.003 ** \nV25WAG    0.015594  0.002143  7.276000  1.376000  0.000000  0.250 0.002 ** \nV13RWB    0.014804  0.009495  1.559000  1.922100  0.662400  0.277 0.008 ** \nV20PCU    0.014406  0.001861  7.739000  0.000000  1.268100  0.304 0.001 ***\nV43GCU    0.014108  0.001624  8.689000  0.000000  1.256100  0.330 0.002 ** \nV83SFC    0.013763  0.001275 10.794000  0.000000  1.220000  0.355 0.001 ***\nV4BTH     0.012989  0.009916  1.310000  1.184200  2.294000  0.379 0.080 .  \nV42SIL    0.012586  0.005694  2.210000  0.420400  1.363600  0.402 0.022 *  \nV3GF      0.012521  0.008426  1.486000  0.646500  1.724600  0.425 0.002 ** \nV19ER     0.011718  0.005658  2.071000  1.308600  0.266900  0.447 0.023 *  \nV37WHIP   0.011155  0.006729  1.658000  0.000000  1.020400  0.468 0.002 ** \nV48YTBC   0.010991  0.006611  1.663000  0.000000  0.958400  0.488 0.002 ** \nV81SHBC   0.010480  0.005715  1.834000  0.271900  1.191300  0.507 0.011 *  \nV21ESP    0.010274  0.006547  1.569000  0.644000  1.502100  0.526 0.128    \nV67GGC    0.010101  0.006264  1.613000  0.000000  0.926600  0.545 0.001 ***\nV80RF     0.010024  0.006191  1.619000  0.000000  0.913500  0.563 0.021 *  \nV5GWH     0.009874  0.007909  1.248000  0.674800  1.486100  0.582 0.023 *  \nV29CST    0.009751  0.004639  2.102000  1.059100  0.314400  0.599 0.032 *  \nV36YFHE   0.009681  0.007030  1.377000  1.243700  1.134500  0.617 0.413    \nV16LR     0.009375  0.007159  1.310000  1.184900  1.023200  0.635 0.224    \nV46BHHE   0.008981  0.008370  1.073000  0.307900  0.828800  0.651 0.360    \nV49LYRE   0.008738  0.005375  1.626000  0.000000  0.803100  0.667 0.012 *  \nV33RWH    0.008651  0.007180  1.205000  0.579800  0.753700  0.683 0.255    \nV68PIL    0.008604  0.005317  1.618000  0.000000  0.785200  0.699 0.010 ** \nV54STHR   0.008499  0.005150  1.650000  0.000000  0.758900  0.715 0.002 ** \nV65LWB    0.008071  0.008400  0.961000  0.741500  0.000000  0.730 0.150    \nV63DWS    0.007899  0.008217  0.961000  0.724800  0.000000  0.744 0.051 .  \nV91NMIN   0.007851  0.008209  0.956000  0.719700  0.000000  0.759 0.073 .  \nV28VS     0.007829  0.008154  0.960000  0.000000  0.709200  0.773 0.606    \nV7WEHE    0.007421  0.005457  1.360000  1.241500  1.224500  0.787 0.363    \nV24BFCS   0.007277  0.005961  1.221000  1.112500  0.496700  0.800 0.130    \nV56FTC    0.007098  0.006534  1.086000  0.615700  1.231300  0.813 0.398    \nV35STP    0.006718  0.007148  0.940000  0.993200  0.944200  0.826 0.799    \nV23RBFT   0.006640  0.006306  1.053000  0.937700  0.883700  0.838 0.890    \nV14AUR    0.006610  0.006785  0.974000  0.962100  0.919800  0.850 0.411    \nV55LHE    0.006240  0.006468  0.965000  0.000000  0.590500  0.862 0.185    \nV12LK     0.005918  0.005486  1.079000  0.801000  1.324100  0.873 0.062 .  \nV26WWCH   0.005801  0.010413  0.557000  0.000000  0.462400  0.883 0.878    \nV50CHE    0.005678  0.005941  0.956000  0.000000  0.509600  0.894 0.599    \nV86BCOO   0.005379  0.005588  0.962000  0.000000  0.481700  0.904 0.078 .  \nV22SCR    0.003983  0.007149  0.557000  0.000000  0.317500  0.911 0.830    \nV8WNHE    0.003758  0.001583  2.374000  1.843200  1.709100  0.918 0.846    \nV85ROSE   0.003564  0.006391  0.558000  0.000000  0.331700  0.925 0.308    \nV62RBTR   0.003528  0.006332  0.557000  0.000000  0.281200  0.931 0.224    \nV38GAL    0.003497  0.006277  0.557000  0.323400  0.000000  0.938 0.800    \nV87LFC    0.003183  0.005712  0.557000  0.289600  0.000000  0.944 0.560    \nV45MGLK   0.002941  0.005278  0.557000  0.271900  0.000000  0.949 0.822    \nV41SPP    0.002919  0.001917  1.523000  1.200500  1.352200  0.954 0.894    \nV9SFW     0.002819  0.002623  1.075000  1.734700  1.525000  0.960 0.907    \nV53MB     0.002704  0.004852  0.557000  0.250000  0.000000  0.965 0.776    \nV57PINK   0.002686  0.004816  0.558000  0.000000  0.250000  0.970 0.280    \nV76GBB    0.002568  0.004606  0.557000  0.000000  0.220000  0.974 0.425    \nV70RSL    0.002557  0.004589  0.557000  0.236400  0.000000  0.979 0.834    \nV2EYR     0.002508  0.001941  1.292000  1.599500  1.440100  0.984 0.916    \nV102KING  0.002459  0.004408  0.558000  0.000000  0.236400  0.988 0.067 .  \nV51OWH    0.002364  0.004239  0.558000  0.000000  0.220000  0.993 0.508    \nV10WBSW   0.002324  0.001192  1.950000  1.760400  1.622600  0.997 0.966    \nV1GST     0.001754  0.001373  1.278000  1.493300  1.379200  1.000 0.734    \nV18YTH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV30BTR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV31AMAG   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV32SCC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV40BRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV47RFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV52TRM    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV58OBO    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV59YR     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV60LFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV61SPW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV66CBW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV69SKF    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV71PDOV   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV72CRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV73JW     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV74BCHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV84YRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV88WG     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV90WTG    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV92NFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV93DB     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV94RBEE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV95HBC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV96DF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV98FLAME  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV99WWT    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV101LCOR  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Gippsland Ma_Foothills Wo \n\n           average        sd     ratio       ava       avb cumsum     p    \nV17WPHE   0.026097  0.002383 10.951000  2.247700  0.000000  0.052 0.001 ***\nV34WSW    0.021402  0.002230  9.597000  1.856000  0.000000  0.094 0.001 ***\nV6WTTR    0.018324  0.001427 12.839000  0.000000  1.579900  0.130 0.001 ***\nV11CR     0.017226  0.001813  9.499000  0.000000  1.484400  0.164 0.005 ** \nV15STTH   0.017112  0.007416  2.307000  0.323400  1.788200  0.198 0.003 ** \nV40BRTH   0.016327  0.010384  1.572000  0.000000  1.423800  0.230 0.001 ***\nV27NHHE   0.015988  0.007215  2.216000  1.646300  0.256000  0.262 0.006 ** \nV25WAG    0.015952  0.001857  8.589000  1.376000  0.000000  0.293 0.001 ***\nV22SCR    0.015801  0.001670  9.460000  0.000000  1.364500  0.325 0.001 ***\nV26WWCH   0.013884  0.008667  1.602000  0.000000  1.210300  0.352 0.022 *  \nV18YTH    0.013759  0.008550  1.609000  0.000000  1.199300  0.379 0.024 *  \nV28VS     0.012735  0.007685  1.657000  0.000000  1.088900  0.404 0.019 *  \nV10WBSW   0.011739  0.009043  1.298000  1.760400  0.773600  0.428 0.081 .  \nV3GF      0.011589  0.007893  1.468000  0.646500  1.569600  0.451 0.005 ** \nV33RWH    0.011462  0.006709  1.709000  0.579800  1.597500  0.473 0.026 *  \nV69SKF    0.010219  0.006162  1.658000  0.000000  0.890600  0.494 0.004 ** \nV4BTH     0.010179  0.009715  1.048000  1.184200  1.975000  0.514 0.369    \nV23RBFT   0.009465  0.006668  1.419000  0.937700  0.243500  0.532 0.316    \nV46BHHE   0.009439  0.007230  1.305000  0.307900  0.931700  0.551 0.292    \nV42SIL    0.009295  0.005353  1.736000  0.420400  0.761200  0.569 0.284    \nV21ESP    0.009082  0.006693  1.357000  0.644000  1.346100  0.587 0.302    \nV5GWH     0.008800  0.007323  1.202000  0.674800  1.309900  0.605 0.102    \nV13RWB    0.008699  0.002320  3.750000  1.922100  1.170600  0.622 0.665    \nV53MB     0.008525  0.006365  1.339000  0.250000  0.851600  0.639 0.033 *  \nV36YFHE   0.008313  0.004873  1.706000  1.243700  1.319700  0.655 0.704    \nV65LWB    0.008250  0.008548  0.965000  0.741500  0.000000  0.671 0.131    \nV63DWS    0.008179  0.007487  1.092000  0.724800  0.243500  0.688 0.047 *  \nV81SHBC   0.008171  0.006178  1.323000  0.271900  0.807100  0.704 0.099 .  \nV19ER     0.008140  0.007125  1.143000  1.308600  0.654600  0.720 0.385    \nV91NMIN   0.008026  0.008354  0.961000  0.719700  0.000000  0.736 0.069 .  \nV7WEHE    0.007740  0.006507  1.189000  1.241500  1.318200  0.751 0.340    \nV29CST    0.007736  0.004933  1.568000  1.059100  0.659900  0.766 0.208    \nV20PCU    0.007716  0.008005  0.964000  0.000000  0.673000  0.782 0.161    \nV9SFW     0.007643  0.007705  0.992000  1.734700  1.081900  0.797 0.201    \nV56FTC    0.007448  0.006647  1.120000  0.615700  1.244700  0.811 0.293    \nV14AUR    0.006977  0.007154  0.975000  0.962100  0.987000  0.825 0.375    \nV35STP    0.006914  0.007551  0.916000  0.993200  1.011200  0.839 0.805    \nV43GCU    0.006636  0.007051  0.941000  0.000000  0.583000  0.852 0.524    \nV12LK     0.006350  0.005685  1.117000  0.801000  1.350100  0.865 0.043 *  \nV16LR     0.006246  0.006500  0.961000  1.184900  1.479800  0.877 0.845    \nV64BELL   0.005568  0.009988  0.557000  0.000000  0.492000  0.888 0.219    \nV50CHE    0.004608  0.008269  0.557000  0.000000  0.384600  0.897 0.706    \nV96DF     0.004006  0.007186  0.557000  0.000000  0.349000  0.905 0.104    \nV41SPP    0.003877  0.002906  1.334000  1.200500  1.526000  0.913 0.906    \nV24BFCS   0.003777  0.005287  0.714000  1.112500  0.794400  0.920 0.850    \nV61SPW    0.003744  0.006716  0.557000  0.000000  0.326200  0.927 0.104    \nV80RF     0.003648  0.006547  0.557000  0.000000  0.304500  0.935 0.847    \nV2EYR     0.003593  0.002576  1.394000  1.599500  1.316800  0.942 0.865    \nV38GAL    0.003574  0.006396  0.559000  0.323400  0.000000  0.949 0.780    \nV87LFC    0.003254  0.005822  0.559000  0.289600  0.000000  0.955 0.538    \nV8WNHE    0.003139  0.002678  1.172000  1.843200  1.582200  0.962 0.919    \nV58OBO    0.003041  0.005456  0.557000  0.000000  0.261700  0.968 0.783    \nV83SFC    0.003041  0.005456  0.557000  0.000000  0.261700  0.974 0.879    \nV45MGLK   0.003006  0.005378  0.559000  0.271900  0.000000  0.980 0.762    \nV1GST     0.002663  0.001997  1.334000  1.493300  1.577800  0.985 0.218    \nV70RSL    0.002613  0.004676  0.559000  0.236400  0.000000  0.990 0.802    \nV62RBTR   0.002557  0.004588  0.557000  0.000000  0.220000  0.995 0.469    \nV66CBW    0.002557  0.004588  0.557000  0.000000  0.220000  1.000 0.541    \nV30BTR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV31AMAG   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV32SCC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV37WHIP   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV47RFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV48YTBC   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV49LYRE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV51OWH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV52TRM    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV54STHR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV55LHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV57PINK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV59YR     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV60LFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV67GGC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV68PIL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV71PDOV   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV72CRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV73JW     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV74BCHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV76GBB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV84YRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV85ROSE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV86BCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV88WG     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV90WTG    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV92NFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV93DB     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV94RBEE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV95HBC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV98FLAME  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV99WWT    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV101LCOR  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV102KING  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Gippsland Ma_Box-Ironbark \n\n          average       sd    ratio      ava      avb cumsum     p    \nV17WPHE  0.026261 0.003224 8.146000 2.247700 0.000000  0.046 0.001 ***\nV18YTH   0.022239 0.002654 8.378000 0.000000 1.909900  0.086 0.001 ***\nV34WSW   0.021534 0.002823 7.627000 1.856000 0.000000  0.124 0.001 ***\nV10WBSW  0.020600 0.003335 6.178000 1.760400 0.000000  0.160 0.001 ***\nV26WWCH  0.019377 0.002919 6.638000 0.000000 1.655500  0.195 0.001 ***\nV27NHHE  0.019175 0.005389 3.559000 1.646300 0.000000  0.229 0.003 ** \nV6WTTR   0.017796 0.001927 9.234000 0.000000 1.525100  0.260 0.001 ***\nV46BHHE  0.017556 0.007155 2.454000 0.307900 1.798900  0.291 0.001 ***\nV39FHE   0.017550 0.003257 5.388000 0.000000 1.532700  0.322 0.001 ***\nV25WAG   0.016052 0.002282 7.033000 1.376000 0.000000  0.351 0.001 ***\nV22SCR   0.015788 0.003354 4.707000 0.000000 1.350800  0.379 0.002 ** \nV40BRTH  0.015511 0.009337 1.661000 0.000000 1.379300  0.406 0.009 ** \nV15STTH  0.015362 0.007523 2.042000 0.323400 1.627000  0.433 0.009 ** \nV44MUSK  0.013318 0.008639 1.542000 0.000000 1.180800  0.457 0.003 ** \nV58OBO   0.012439 0.003104 4.007000 0.000000 1.059200  0.479 0.002 ** \nV42SIL   0.012248 0.004880 2.510000 0.420400 1.255000  0.500 0.026 *  \nV75RCR   0.011824 0.003514 3.365000 0.000000 1.000900  0.521 0.001 ***\nV28VS    0.011169 0.006772 1.649000 0.000000 0.971800  0.541 0.067 .  \nV11CR    0.011098 0.006986 1.589000 0.000000 0.992900  0.561 0.165    \nV33RWH   0.010578 0.006786 1.559000 0.579800 1.512200  0.580 0.038 *  \nV16LR    0.010525 0.007973 1.320000 1.184900 0.576500  0.598 0.084 .  \nV3GF     0.010144 0.006630 1.530000 0.646500 1.330200  0.616 0.052 .  \nV29CST   0.009753 0.005485 1.778000 1.059100 0.256000  0.633 0.034 *  \nV23RBFT  0.009549 0.007039 1.357000 0.937700 0.320500  0.650 0.299    \nV43GCU   0.009226 0.005611 1.644000 0.000000 0.818400  0.667 0.052 .  \nV35STP   0.009032 0.007151 1.263000 0.993200 0.656700  0.683 0.226    \nV36YFHE  0.008694 0.006441 1.350000 1.243700 1.383500  0.698 0.639    \nV2EYR    0.008634 0.006941 1.244000 1.599500 0.874900  0.713 0.184    \nV5GWH    0.008626 0.006596 1.308000 0.674800 1.171900  0.729 0.110    \nV65LWB   0.008297 0.008644 0.960000 0.741500 0.000000  0.743 0.153    \nV63DWS   0.008199 0.007646 1.072000 0.724800 0.220000  0.758 0.049 *  \nV91NMIN  0.008072 0.008448 0.955000 0.719700 0.000000  0.772 0.067 .  \nV21ESP   0.007818 0.008323 0.939000 0.644000 0.000000  0.786 0.715    \nV7WEHE   0.007692 0.006430 1.196000 1.241500 1.296700  0.799 0.345    \nV4BTH    0.007495 0.007712 0.972000 1.184200 1.520900  0.813 0.737    \nV56FTC   0.007373 0.007510 0.982000 0.615700 0.281200  0.826 0.331    \nV66CBW   0.007079 0.007358 0.962000 0.000000 0.566600  0.838 0.004 ** \nV90WTG   0.006660 0.007437 0.896000 0.000000 0.542000  0.850 0.044 *  \nV70RSL   0.006539 0.006415 1.019000 0.236400 0.557400  0.862 0.271    \nV61SPW   0.006393 0.006678 0.957000 0.000000 0.593000  0.873 0.014 *  \nV19ER    0.006130 0.006577 0.932000 1.308600 0.846400  0.884 0.864    \nV14AUR   0.005432 0.006537 0.831000 0.962100 1.211600  0.893 0.642    \nV81SHBC  0.004610 0.006122 0.753000 0.271900 0.281200  0.902 0.923    \nV12LK    0.004495 0.005039 0.892000 0.801000 1.122800  0.910 0.337    \nV79YTHE  0.004087 0.007327 0.558000 0.000000 0.396100  0.917 0.042 *  \nV9SFW    0.004012 0.003799 1.056000 1.734700 1.472800  0.924 0.735    \nV30BTR   0.003950 0.007083 0.558000 0.000000 0.382900  0.931 0.627    \nV38GAL   0.003595 0.006455 0.557000 0.323400 0.000000  0.937 0.770    \nV13RWB   0.003563 0.001691 2.107000 1.922100 1.792200  0.944 0.998    \nV41SPP   0.003386 0.002315 1.463000 1.200500 1.440100  0.950 0.887    \nV87LFC   0.003273 0.005878 0.557000 0.289600 0.000000  0.955 0.552    \nV45MGLK  0.003023 0.005428 0.557000 0.271900 0.000000  0.961 0.759    \nV8WNHE   0.002915 0.000912 3.194000 1.843200 1.595400  0.966 0.930    \nV53MB    0.002779 0.004990 0.557000 0.250000 0.000000  0.971 0.762    \nV94RBEE  0.002642 0.004736 0.558000 0.000000 0.256000  0.976 0.414    \nV96DF    0.002642 0.004736 0.558000 0.000000 0.256000  0.980 0.358    \nV76GBB   0.002548 0.004573 0.557000 0.000000 0.210200  0.985 0.397    \nV1GST    0.002444 0.001755 1.393000 1.493300 1.425700  0.989 0.321    \nV88WG    0.002395 0.004296 0.557000 0.000000 0.210200  0.993 0.381    \nV95HBC   0.002395 0.004296 0.557000 0.000000 0.210200  0.997 0.050 *  \nV24BFCS  0.001441 0.001298 1.110000 1.112500 1.014400  1.000 0.985    \nV20PCU   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV31AMAG  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV32SCC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV37WHIP  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV47RFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV48YTBC  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV49LYRE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV50CHE   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV51OWH   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV52TRM   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV54STHR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV55LHE   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV57PINK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV60LFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV62RBTR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV64BELL  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV67GGC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV68PIL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV69SKF   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV71PDOV  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV77RRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV78LLOR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV80RF    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV82AZKF  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV83SFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV84YRTH  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV85ROSE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV86BCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV89PCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV92NFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV93DB    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV97PCL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV98FLAME 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV100WBWS 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV102KING 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Gippsland Ma_River Red Gu \n\n           average        sd     ratio       ava       avb cumsum     p    \nV32SCC    0.025596  0.002660  9.624000  0.000000  2.225000  0.041 0.001 ***\nV13RWB    0.022078  0.001581 13.960000  1.922100  0.000000  0.077 0.001 ***\nV10WBSW   0.020296  0.002871  7.069000  1.760400  0.000000  0.110 0.001 ***\nV27NHHE   0.018894  0.005104  3.702000  1.646300  0.000000  0.140 0.003 ** \nV2EYR     0.018457  0.002843  6.492000  1.599500  0.000000  0.170 0.001 ***\nV26WWCH   0.018125  0.001284 14.119000  0.000000  1.578000  0.199 0.004 ** \nV8WNHE    0.018039  0.006464  2.791000  1.843200  0.289600  0.228 0.001 ***\nV30BTR    0.017484  0.001776  9.844000  0.000000  1.520300  0.257 0.002 ** \nV31AMAG   0.016036  0.001581 10.146000  0.000000  1.393900  0.283 0.002 ** \nV60LFB    0.015147  0.003696  4.098000  0.000000  1.312300  0.307 0.002 ** \nV38GAL    0.014698  0.007189  2.044000  0.323400  1.582800  0.331 0.002 ** \nV47RFC    0.014352  0.001420 10.106000  0.000000  1.251400  0.354 0.001 ***\nV36YFHE   0.014260  0.009360  1.524000  1.243700  0.000000  0.377 0.016 *  \nV7WEHE    0.013756  0.008586  1.602000  1.241500  0.000000  0.399 0.006 ** \nV41SPP    0.013744  0.002390  5.750000  1.200500  0.000000  0.421 0.002 ** \nV69SKF    0.013273  0.001217 10.902000  0.000000  1.156400  0.443 0.002 ** \nV4BTH     0.013108  0.008293  1.581000  1.184200  0.000000  0.464 0.083 .  \nV70RSL    0.011108  0.007818  1.421000  0.236400  1.085200  0.482 0.009 ** \nV45MGLK   0.010974  0.005854  1.875000  0.271900  1.216900  0.500 0.002 ** \nV98FLAME  0.010959  0.006960  1.575000  0.000000  0.964700  0.517 0.003 ** \nV73JW     0.010616  0.006635  1.600000  0.000000  0.934100  0.534 0.001 ***\nV34WSW    0.010143  0.007370  1.376000  1.856000  0.961700  0.551 0.175    \nV52TRM    0.009996  0.006422  1.556000  0.000000  0.871300  0.567 0.007 ** \nV18YTH    0.009754  0.010215  0.955000  0.000000  0.845000  0.583 0.250    \nV93DB     0.009507  0.005742  1.656000  0.000000  0.829800  0.598 0.001 ***\nV91NMIN   0.008898  0.008710  1.022000  0.719700  0.767400  0.612 0.018 *  \nV15STTH   0.008597  0.008559  1.004000  0.323400  0.745400  0.626 0.482    \nV3GF      0.008595  0.004954  1.735000  0.646500  1.077300  0.640 0.101    \nV6WTTR    0.008498  0.008891  0.956000  0.000000  0.736100  0.654 0.327    \nV94RBEE   0.008405  0.005103  1.647000  0.000000  0.732200  0.667 0.001 ***\nV65LWB    0.008183  0.008484  0.965000  0.741500  0.000000  0.681 0.143    \nV23RBFT   0.008089  0.005959  1.357000  0.937700  0.456500  0.694 0.568    \nV63DWS    0.008008  0.008299  0.965000  0.724800  0.000000  0.707 0.059 .  \nV28VS     0.007914  0.008215  0.963000  0.000000  0.686000  0.720 0.599    \nV35STP    0.007822  0.007933  0.986000  0.993200  1.638300  0.732 0.572    \nV5GWH     0.007810  0.005625  1.388000  0.674800  0.765000  0.745 0.169    \nV21ESP    0.007696  0.008143  0.945000  0.644000  0.000000  0.757 0.732    \nV59YR     0.007520  0.007810  0.963000  0.000000  0.651800  0.769 0.001 ***\nV42SIL    0.007277  0.009602  0.758000  0.420400  0.442300  0.781 0.772    \nV56FTC    0.007269  0.006882  1.056000  0.615700  0.220000  0.793 0.338    \nV88WG     0.007075  0.007399  0.956000  0.000000  0.612900  0.804 0.002 ** \nV90WTG    0.007075  0.007399  0.956000  0.000000  0.612900  0.816 0.020 *  \nV99WWT    0.007063  0.007361  0.959000  0.000000  0.615300  0.827 0.001 ***\nV58OBO    0.007005  0.007268  0.964000  0.000000  0.611700  0.838 0.092 .  \nV9SFW     0.006866  0.003914  1.754000  1.734700  1.145500  0.849 0.328    \nV33RWH    0.006745  0.006466  1.043000  0.579800  0.532700  0.860 0.615    \nV20PCU    0.006711  0.007071  0.949000  0.000000  0.602800  0.871 0.494    \nV16LR     0.006244  0.004942  1.264000  1.184900  1.252000  0.881 0.846    \nV72CRP    0.006087  0.006321  0.963000  0.000000  0.545600  0.891 0.001 ***\nV92NFB    0.006062  0.006313  0.960000  0.000000  0.513100  0.901 0.011 *  \nV71PDOV   0.006011  0.006489  0.926000  0.000000  0.530500  0.910 0.001 ***\nV12LK     0.005937  0.005887  1.009000  0.801000  0.910300  0.920 0.074 .  \nV14AUR    0.005129  0.006478  0.792000  0.962100  1.237700  0.928 0.683    \nV87LFC    0.005124  0.006716  0.763000  0.289600  0.314400  0.937 0.222    \nV84YRTH   0.004535  0.008137  0.557000  0.000000  0.389600  0.944 0.040 *  \nV101LCOR  0.004040  0.007245  0.558000  0.000000  0.370000  0.951 0.003 ** \nV53MB     0.004022  0.005167  0.778000  0.250000  0.210200  0.957 0.583    \nV29CST    0.003883  0.005041  0.770000  1.059100  0.776000  0.963 0.983    \nV46BHHE   0.003418  0.006118  0.559000  0.307900  0.000000  0.969 0.983    \nV74BCHE   0.003144  0.005642  0.557000  0.000000  0.261700  0.974 0.001 ***\nV81SHBC   0.003030  0.005425  0.559000  0.271900  0.000000  0.979 0.996    \nV17WPHE   0.002749  0.001706  1.611000  2.247700  2.034900  0.983 0.981    \nV24BFCS   0.002646  0.002193  1.206000  1.112500  1.328700  0.988 0.913    \nV95HBC    0.002389  0.004287  0.557000  0.000000  0.198800  0.991 0.049 *  \nV1GST     0.002019  0.001634  1.236000  1.493300  1.343300  0.995 0.574    \nV19ER     0.001923  0.001172  1.640000  1.308600  1.397300  0.998 0.999    \nV25WAG    0.001422  0.001101  1.292000  1.376000  1.365200  1.000 0.964    \nV11CR     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV22SCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV37WHIP   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV40BRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV43GCU    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV48YTBC   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV49LYRE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV50CHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV51OWH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV54STHR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV55LHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV57PINK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV61SPW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV62RBTR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV66CBW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV67GGC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV68PIL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV76GBB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV80RF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV83SFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV85ROSE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV86BCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV96DF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV102KING  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Montane Fore_Foothills Wo \n\n          average       sd    ratio      ava      avb cumsum     p   \nV40BRTH  0.014435 0.009183 1.571800 0.000000 1.423800  0.042 0.008 **\nV18YTH   0.012164 0.007560 1.609000 0.000000 1.199300  0.077 0.054 . \nV26WWCH  0.010745 0.008003 1.342600 0.462400 1.210300  0.108 0.159   \nV22SCR   0.010509 0.005505 1.909200 0.317500 1.364500  0.138 0.043 * \nV37WHIP  0.010114 0.006054 1.670700 1.020400 0.000000  0.168 0.018 * \nV48YTBC  0.009915 0.005918 1.675400 0.958400 0.000000  0.196 0.021 * \nV83SFC   0.009752 0.004877 1.999500 1.220000 0.261700  0.224 0.029 * \nV10WBSW  0.009395 0.007408 1.268200 1.622600 0.773600  0.251 0.369   \nV67GGC   0.009162 0.005646 1.622600 0.926600 0.000000  0.278 0.007 **\nV69SKF   0.009034 0.005447 1.658400 0.000000 0.890600  0.304 0.036 * \nV53MB    0.008754 0.005324 1.644300 0.000000 0.851600  0.329 0.023 * \nV33RWH   0.008585 0.007066 1.215100 0.753700 1.597500  0.354 0.278   \nV46BHHE  0.008556 0.006287 1.361000 0.828800 0.931700  0.379 0.496   \nV13RWB   0.008482 0.004846 1.750400 0.662400 1.170600  0.403 0.713   \nV49LYRE  0.007927 0.004849 1.634700 0.803100 0.000000  0.426 0.024 * \nV68PIL   0.007799 0.004781 1.631300 0.785200 0.000000  0.449 0.019 * \nV80RF    0.007785 0.006023 1.292600 0.913500 0.304500  0.471 0.131   \nV54STHR  0.007684 0.004615 1.665200 0.758900 0.000000  0.493 0.007 **\nV23RBFT  0.007670 0.005612 1.366800 0.883700 0.243500  0.516 0.700   \nV28VS    0.007670 0.007611 1.007700 0.709200 1.088900  0.538 0.660   \nV43GCU   0.007662 0.005636 1.359400 1.256100 0.583000  0.560 0.197   \nV16LR    0.007478 0.006026 1.241100 1.023200 1.479800  0.582 0.644   \nV20PCU   0.006977 0.006456 1.080800 1.268100 0.673000  0.602 0.378   \nV19ER    0.006769 0.006493 1.042500 0.266900 0.654600  0.621 0.755   \nV29CST   0.006698 0.006812 0.983200 0.314400 0.659900  0.641 0.537   \nV50CHE   0.006550 0.006045 1.083500 0.509600 0.384600  0.659 0.508   \nV35STP   0.006186 0.006202 0.997400 0.944200 1.011200  0.677 0.856   \nV42SIL   0.006120 0.004742 1.290500 1.363600 0.761200  0.695 0.914   \nV24BFCS  0.005843 0.005215 1.120300 0.496700 0.794400  0.712 0.426   \nV14AUR   0.005747 0.006056 0.948900 0.919800 0.987000  0.729 0.586   \nV55LHE   0.005677 0.005865 0.967900 0.590500 0.000000  0.745 0.283   \nV36YFHE  0.005166 0.005585 0.925100 1.134500 1.319700  0.760 0.966   \nV9SFW    0.005041 0.006178 0.815900 1.525000 1.081900  0.774 0.599   \nV64BELL  0.004931 0.008845 0.557500 0.000000 0.492000  0.789 0.365   \nV86BCOO  0.004867 0.005040 0.965600 0.481700 0.000000  0.803 0.115   \nV81SHBC  0.004290 0.004697 0.913300 1.191300 0.807100  0.815 0.960   \nV62RBTR  0.004175 0.005389 0.774800 0.281200 0.220000  0.827 0.174   \nV21ESP   0.003642 0.002780 1.310000 1.502100 1.346100  0.838 0.989   \nV96DF    0.003541 0.006353 0.557400 0.000000 0.349000  0.848 0.228   \nV4BTH    0.003496 0.002575 1.358000 2.294000 1.975000  0.858 0.951   \nV61SPW   0.003310 0.005938 0.557400 0.000000 0.326200  0.868 0.250   \nV8WNHE   0.003301 0.002559 1.290000 1.709100 1.582200  0.877 0.895   \nV85ROSE  0.003237 0.005792 0.558900 0.331700 0.000000  0.886 0.358   \nV7WEHE   0.002775 0.001747 1.588400 1.224500 1.318200  0.894 0.957   \nV27NHHE  0.002698 0.004842 0.557300 0.000000 0.256000  0.902 0.969   \nV58OBO   0.002685 0.004816 0.557400 0.000000 0.261700  0.910 0.869   \nV11CR    0.002557 0.001689 1.513300 1.710400 1.484400  0.917 0.992   \nV63DWS   0.002440 0.004377 0.557500 0.000000 0.243500  0.924 0.834   \nV57PINK  0.002440 0.004365 0.558900 0.250000 0.000000  0.931 0.465   \nV76GBB   0.002313 0.004139 0.558900 0.220000 0.000000  0.938 0.588   \nV66CBW   0.002257 0.004050 0.557400 0.000000 0.220000  0.945 0.602   \nV102KING 0.002240 0.004008 0.558900 0.236400 0.000000  0.951 0.322   \nV41SPP   0.002181 0.001351 1.614700 1.352200 1.526000  0.958 0.935   \nV51OWH   0.002147 0.003842 0.558900 0.220000 0.000000  0.964 0.595   \nV1GST    0.002076 0.002289 0.906700 1.379200 1.577800  0.970 0.524   \nV2EYR    0.002039 0.001545 1.319900 1.440100 1.316800  0.976 0.965   \nV15STTH  0.001903 0.001654 1.150800 1.857500 1.788200  0.981 0.993   \nV5GWH    0.001891 0.001382 1.367800 1.486100 1.309900  0.987 0.975   \nV3GF     0.001741 0.001033 1.685600 1.724600 1.569600  0.992 0.991   \nV6WTTR   0.001138 0.000630 1.805600 1.567100 1.579900  0.995 0.993   \nV56FTC   0.001005 0.000717 1.402700 1.231300 1.244700  0.998 1.000   \nV12LK    0.000762 0.000683 1.116400 1.324100 1.350100  1.000 1.000   \nV17WPHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV25WAG   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV30BTR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV31AMAG  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV32SCC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV34WSW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV38GAL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV39FHE   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV44MUSK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV45MGLK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV47RFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV52TRM   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV60LFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV65LWB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV70RSL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV71PDOV  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV75RCR   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV77RRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV78LLOR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV79YTHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV82AZKF  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV84YRTH  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV87LFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV88WG    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV89PCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV90WTG   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV91NMIN  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV92NFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV93DB    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV94RBEE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV95HBC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV97PCL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV98FLAME 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV100WBWS 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Montane Fore_Box-Ironbark \n\n           average        sd     ratio       ava       avb cumsum     p    \nV18YTH    0.019614  0.002267  8.650000  0.000000  1.909900  0.042 0.002 ** \nV10WBSW   0.016737  0.002289  7.312000  1.622600  0.000000  0.079 0.004 ** \nV39FHE    0.015508  0.003007  5.157000  0.000000  1.532700  0.112 0.002 ** \nV21ESP    0.015277  0.002850  5.360000  1.502100  0.000000  0.145 0.005 ** \nV40BRTH   0.013739  0.008267  1.662000  0.000000  1.379300  0.175 0.019 *  \nV20PCU    0.013073  0.001670  7.829000  1.268100  0.000000  0.203 0.007 ** \nV26WWCH   0.012932  0.006735  1.920000  0.462400  1.655500  0.231 0.030 *  \nV13RWB    0.012500  0.008069  1.549000  0.662400  1.792200  0.258 0.053 .  \nV83SFC    0.012497  0.001221 10.232000  1.220000  0.000000  0.285 0.004 ** \nV44MUSK   0.011793  0.007641  1.543000  0.000000  1.180800  0.310 0.010 ** \nV58OBO    0.010960  0.002648  4.139000  0.000000  1.059200  0.334 0.002 ** \nV22SCR    0.010674  0.005558  1.920000  0.317500  1.350800  0.357 0.035 *  \nV46BHHE   0.010527  0.009610  1.095000  0.828800  1.798900  0.380 0.113    \nV75RCR    0.010410  0.003002  3.468000  0.000000  1.000900  0.402 0.001 ***\nV37WHIP   0.010161  0.006135  1.656000  1.020400  0.000000  0.424 0.022 *  \nV56FTC    0.010047  0.005497  1.828000  1.231300  0.281200  0.446 0.042 *  \nV48YTBC   0.009965  0.006005  1.659000  0.958400  0.000000  0.468 0.026 *  \nV81SHBC   0.009699  0.005338  1.817000  1.191300  0.281200  0.489 0.011 *  \nV67GGC    0.009203  0.005719  1.609000  0.926600  0.000000  0.508 0.006 ** \nV80RF     0.009127  0.005635  1.620000  0.913500  0.000000  0.528 0.056 .  \nV16LR     0.008206  0.006541  1.255000  1.023200  0.576500  0.546 0.472    \nV11CR     0.008012  0.007131  1.124000  1.710400  0.992900  0.563 0.747    \nV49LYRE   0.007963  0.004911  1.621000  0.803100  0.000000  0.581 0.018 *  \nV4BTH     0.007900  0.002729  2.895000  2.294000  1.520900  0.598 0.655    \nV33RWH    0.007855  0.007080  1.109000  0.753700  1.512200  0.615 0.401    \nV23RBFT   0.007852  0.005901  1.331000  0.883700  0.320500  0.631 0.678    \nV68PIL    0.007835  0.004844  1.617000  0.785200  0.000000  0.648 0.013 *  \nV35STP    0.007792  0.006378  1.222000  0.944200  0.656700  0.665 0.606    \nV54STHR   0.007721  0.004680  1.650000  0.758900  0.000000  0.682 0.005 ** \nV28VS     0.007362  0.006716  1.096000  0.709200  0.971800  0.698 0.756    \nV19ER     0.007289  0.005572  1.308000  0.266900  0.846400  0.714 0.599    \nV36YFHE   0.006921  0.005444  1.271000  1.134500  1.383500  0.729 0.900    \nV66CBW    0.006194  0.006437  0.962000  0.000000  0.566600  0.742 0.024 *  \nV24BFCS   0.005995  0.005034  1.191000  0.496700  1.014400  0.755 0.357    \nV2EYR     0.005841  0.005810  1.005000  1.440100  0.874900  0.768 0.594    \nV90WTG    0.005838  0.006497  0.899000  0.000000  0.542000  0.780 0.096 .  \nV70RSL    0.005780  0.006013  0.961000  0.000000  0.557400  0.793 0.340    \nV55LHE    0.005702  0.005915  0.964000  0.590500  0.000000  0.805 0.256    \nV61SPW    0.005691  0.005950  0.956000  0.000000  0.593000  0.817 0.051 .  \nV50CHE    0.005164  0.005411  0.954000  0.509600  0.000000  0.829 0.653    \nV86BCOO   0.004890  0.005087  0.961000  0.481700  0.000000  0.839 0.080 .  \nV43GCU    0.004840  0.005651  0.857000  1.256100  0.818400  0.850 0.869    \nV29CST    0.004252  0.005499  0.773000  0.314400  0.256000  0.859 0.960    \nV14AUR    0.004172  0.005415  0.771000  0.919800  1.211600  0.868 0.847    \nV3GF      0.004016  0.002231  1.800000  1.724600  1.330200  0.876 0.685    \nV79YTHE   0.003656  0.006555  0.558000  0.000000  0.396100  0.884 0.200    \nV30BTR    0.003534  0.006336  0.558000  0.000000  0.382900  0.892 0.802    \nV76GBB    0.003414  0.004520  0.755000  0.220000  0.210200  0.899 0.290    \nV5GWH     0.003383  0.002341  1.445000  1.486100  1.171900  0.907 0.783    \nV85ROSE   0.003252  0.005834  0.557000  0.331700  0.000000  0.914 0.336    \nV62RBTR   0.003172  0.005698  0.557000  0.281200  0.000000  0.920 0.373    \nV8WNHE    0.002763  0.002154  1.282000  1.709100  1.595400  0.926 0.959    \nV15STTH   0.002604  0.001970  1.321000  1.857500  1.627000  0.932 0.960    \nV57PINK   0.002450  0.004397  0.557000  0.250000  0.000000  0.937 0.451    \nV9SFW     0.002418  0.002010  1.203000  1.525000  1.472800  0.943 0.960    \nV94RBEE   0.002363  0.004237  0.558000  0.000000  0.256000  0.948 0.572    \nV96DF     0.002363  0.004237  0.558000  0.000000  0.256000  0.953 0.529    \nV102KING  0.002249  0.004035  0.557000  0.236400  0.000000  0.958 0.313    \nV7WEHE    0.002224  0.001655  1.344000  1.224500  1.296700  0.963 0.988    \nV51OWH    0.002157  0.003870  0.557000  0.220000  0.000000  0.967 0.576    \nV88WG     0.002119  0.003801  0.557000  0.000000  0.210200  0.972 0.477    \nV95HBC    0.002119  0.003801  0.557000  0.000000  0.210200  0.976 0.311    \nV12LK     0.002086  0.000765  2.726000  1.324100  1.122800  0.981 0.603    \nV63DWS    0.002031  0.003641  0.558000  0.000000  0.220000  0.985 0.925    \nV42SIL    0.002007  0.001772  1.133000  1.363600  1.255000  0.990 1.000    \nV1GST     0.001927  0.000613  3.143000  1.379200  1.425700  0.994 0.633    \nV41SPP    0.001767  0.001227  1.439000  1.352200  1.440100  0.998 0.951    \nV6WTTR    0.001142  0.000614  1.860000  1.567100  1.525100  1.000 0.996    \nV17WPHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV25WAG    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV27NHHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV31AMAG   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV32SCC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV34WSW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV38GAL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV45MGLK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV47RFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV52TRM    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV53MB     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV59YR     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV60LFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV65LWB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV69SKF    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV71PDOV   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV72CRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV73JW     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV74BCHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV84YRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV87LFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV91NMIN   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV92NFB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV93DB     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV98FLAME  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV99WWT    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV101LCOR  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Montane Fore_River Red Gu \n\n           average        sd     ratio       ava       avb cumsum     p    \nV4BTH     0.023240  0.001423 16.334000  2.294000  0.000000  0.032 0.001 ***\nV32SCC    0.022619  0.002319  9.752000  0.000000  2.225000  0.062 0.002 ** \nV17WPHE   0.020648  0.001817 11.364000  0.000000  2.034900  0.090 0.005 ** \nV11CR     0.017360  0.001975  8.790000  1.710400  0.000000  0.114 0.003 ** \nV10WBSW   0.016529  0.001948  8.485000  1.622600  0.000000  0.136 0.005 ** \nV38GAL    0.016118  0.001999  8.065000  0.000000  1.582800  0.158 0.001 ***\nV30BTR    0.015451  0.001552  9.955000  0.000000  1.520300  0.179 0.001 ***\nV21ESP    0.015090  0.002641  5.713000  1.502100  0.000000  0.200 0.005 ** \nV2EYR     0.014573  0.001010 14.428000  1.440100  0.000000  0.220 0.001 ***\nV8WNHE    0.014442  0.006232  2.317000  1.709100  0.289600  0.239 0.007 ** \nV31AMAG   0.014171  0.001380 10.268000  0.000000  1.393900  0.258 0.002 ** \nV25WAG    0.013859  0.001151 12.039000  0.000000  1.365200  0.277 0.003 ** \nV41SPP    0.013725  0.001684  8.149000  1.352200  0.000000  0.296 0.005 ** \nV60LFB    0.013381  0.003234  4.138000  0.000000  1.312300  0.314 0.001 ***\nV47RFC    0.012687  0.001289  9.845000  0.000000  1.251400  0.331 0.002 ** \nV43GCU    0.012658  0.001321  9.581000  1.256100  0.000000  0.348 0.002 ** \nV7WEHE    0.012401  0.000859 14.431000  1.224500  0.000000  0.365 0.010 ** \nV26WWCH   0.012397  0.005637  2.199000  0.462400  1.578000  0.382 0.056 .  \nV83SFC    0.012343  0.000888 13.906000  1.220000  0.000000  0.399 0.003 ** \nV45MGLK   0.012337  0.001123 10.986000  0.000000  1.216900  0.416 0.003 ** \nV81SHBC   0.012054  0.001019 11.835000  1.191300  0.000000  0.432 0.005 ** \nV69SKF    0.011732  0.001094 10.720000  0.000000  1.156400  0.448 0.004 ** \nV42SIL    0.011523  0.004695  2.455000  1.363600  0.442300  0.464 0.040 *  \nV19ER     0.011415  0.005033  2.268000  0.266900  1.397300  0.479 0.028 *  \nV36YFHE   0.011410  0.006858  1.664000  1.134500  0.000000  0.495 0.112    \nV15STTH   0.011292  0.008100  1.394000  1.857500  0.745400  0.510 0.142    \nV70RSL    0.010957  0.007153  1.532000  0.000000  1.085200  0.525 0.011 *  \nV56FTC    0.010350  0.004225  2.450000  1.231300  0.220000  0.539 0.037 *  \nV37WHIP   0.010040  0.006016  1.669000  1.020400  0.000000  0.552 0.019 *  \nV48YTBC   0.009839  0.005880  1.673000  0.958400  0.000000  0.566 0.024 *  \nV34WSW    0.009813  0.006345  1.546000  0.000000  0.961700  0.579 0.185    \nV98FLAME  0.009699  0.006152  1.577000  0.000000  0.964700  0.592 0.013 *  \nV73JW     0.009395  0.005864  1.602000  0.000000  0.934100  0.605 0.001 ***\nV67GGC    0.009094  0.005611  1.621000  0.926600  0.000000  0.618 0.007 ** \nV80RF     0.009018  0.005525  1.632000  0.913500  0.000000  0.630 0.068 .  \nV52TRM    0.008836  0.005669  1.559000  0.000000  0.871300  0.642 0.029 *  \nV6WTTR    0.008775  0.007475  1.174000  1.567100  0.736100  0.654 0.281    \nV18YTH    0.008617  0.009023  0.955000  0.000000  0.845000  0.665 0.500    \nV24BFCS   0.008611  0.005836  1.475000  0.496700  1.328700  0.677 0.022 *  \nV93DB     0.008405  0.005073  1.657000  0.000000  0.829800  0.689 0.003 ** \nV91NMIN   0.007934  0.008238  0.963000  0.000000  0.767400  0.699 0.061 .  \nV46BHHE   0.007914  0.008183  0.967000  0.828800  0.000000  0.710 0.657    \nV49LYRE   0.007869  0.004819  1.633000  0.803100  0.000000  0.721 0.024 *  \nV33RWH    0.007804  0.006307  1.237000  0.753700  0.532700  0.731 0.430    \nV68PIL    0.007742  0.004750  1.630000  0.785200  0.000000  0.742 0.020 *  \nV54STHR   0.007627  0.004585  1.663000  0.758900  0.000000  0.752 0.008 ** \nV20PCU    0.007523  0.006020  1.250000  1.268100  0.602800  0.762 0.220    \nV94RBEE   0.007429  0.004516  1.645000  0.000000  0.732200  0.772 0.005 ** \nV5GWH     0.007231  0.004781  1.512000  1.486100  0.765000  0.782 0.216    \nV28VS     0.007207  0.007247  0.994000  0.709200  0.686000  0.792 0.759    \nV29CST    0.007165  0.004999  1.433000  0.314400  0.776000  0.802 0.338    \nV35STP    0.007050  0.006207  1.136000  0.944200  1.638300  0.811 0.760    \nV59YR     0.006644  0.006900  0.963000  0.000000  0.651800  0.820 0.013 *  \nV23RBFT   0.006615  0.005151  1.284000  0.883700  0.456500  0.829 0.883    \nV3GF      0.006534  0.001821  3.589000  1.724600  1.077300  0.838 0.416    \nV13RWB    0.006293  0.007149  0.880000  0.662400  0.000000  0.847 0.941    \nV16LR     0.006278  0.005139  1.222000  1.023200  1.252000  0.855 0.854    \nV88WG     0.006250  0.006536  0.956000  0.000000  0.612900  0.864 0.013 *  \nV90WTG    0.006250  0.006536  0.956000  0.000000  0.612900  0.872 0.070 .  \nV99WWT    0.006243  0.006502  0.960000  0.000000  0.615300  0.881 0.019 *  \nV58OBO    0.006193  0.006425  0.964000  0.000000  0.611700  0.889 0.257    \nV55LHE    0.005637  0.005827  0.967000  0.590500  0.000000  0.897 0.284    \nV72CRP    0.005399  0.005608  0.963000  0.000000  0.545600  0.904 0.016 *  \nV92NFB    0.005341  0.005564  0.960000  0.000000  0.513100  0.911 0.093 .  \nV71PDOV   0.005320  0.005756  0.924000  0.000000  0.530500  0.919 0.036 *  \nV50CHE    0.005101  0.005325  0.958000  0.509600  0.000000  0.926 0.686    \nV86BCOO   0.004831  0.005006  0.965000  0.481700  0.000000  0.932 0.120    \nV9SFW     0.004284  0.002149  1.993000  1.525000  1.145500  0.938 0.683    \nV12LK     0.004232  0.005626  0.752000  1.324100  0.910300  0.944 0.376    \nV14AUR    0.004098  0.005269  0.778000  0.919800  1.237700  0.949 0.845    \nV84YRTH   0.004003  0.007182  0.557000  0.000000  0.389600  0.955 0.190    \nV101LCOR  0.003592  0.006442  0.558000  0.000000  0.370000  0.960 0.183    \nV22SCR    0.003532  0.006323  0.559000  0.317500  0.000000  0.964 0.890    \nV85ROSE   0.003214  0.005752  0.559000  0.331700  0.000000  0.969 0.392    \nV62RBTR   0.003128  0.005600  0.559000  0.281200  0.000000  0.973 0.389    \nV87LFC    0.003051  0.005472  0.558000  0.000000  0.314400  0.977 0.613    \nV74BCHE   0.002765  0.004961  0.557000  0.000000  0.261700  0.981 0.180    \nV57PINK   0.002422  0.004335  0.559000  0.250000  0.000000  0.984 0.514    \nV76GBB    0.002296  0.004109  0.559000  0.220000  0.000000  0.987 0.607    \nV102KING  0.002224  0.003981  0.559000  0.236400  0.000000  0.990 0.369    \nV53MB     0.002160  0.003875  0.557000  0.000000  0.210200  0.993 0.874    \nV51OWH    0.002132  0.003815  0.559000  0.220000  0.000000  0.996 0.634    \nV95HBC    0.002101  0.003770  0.557000  0.000000  0.198800  0.999 0.348    \nV1GST     0.000715  0.000313  2.283000  1.379200  1.343300  1.000 0.998    \nV27NHHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV40BRTH   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV61SPW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV63DWS    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV65LWB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV66CBW    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV96DF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Foothills Wo_Box-Ironbark \n\n          average       sd    ratio      ava      avb cumsum     p    \nV39FHE   0.015833 0.002871 5.514000 0.000000 1.532700  0.047 0.001 ***\nV21ESP   0.014135 0.003111 4.543000 1.346100 0.000000  0.089 0.010 ** \nV44MUSK  0.012037 0.007747 1.554000 0.000000 1.180800  0.125 0.007 ** \nV75RCR   0.010634 0.002985 3.562000 0.000000 1.000900  0.157 0.001 ***\nV56FTC   0.010423 0.005633 1.850000 1.244700 0.281200  0.188 0.019 *  \nV16LR    0.009727 0.006963 1.397000 1.479800 0.576500  0.217 0.174    \nV40BRTH  0.009298 0.009205 1.010000 1.423800 1.379300  0.245 0.220    \nV69SKF   0.009269 0.005598 1.656000 0.890600 0.000000  0.273 0.023 *  \nV46BHHE  0.009105 0.006329 1.439000 0.931700 1.798900  0.300 0.334    \nV53MB    0.008985 0.005474 1.641000 0.851600 0.000000  0.327 0.018 *  \nV58OBO   0.008774 0.004921 1.783000 0.261700 1.059200  0.353 0.026 *  \nV35STP   0.008296 0.006636 1.250000 1.011200 0.656700  0.378 0.426    \nV10WBSW  0.008277 0.008818 0.939000 0.773600 0.000000  0.402 0.647    \nV18YTH   0.007632 0.008144 0.937000 1.199300 1.909900  0.425 0.577    \nV81SHBC  0.007489 0.005740 1.305000 0.807100 0.281200  0.447 0.169    \nV20PCU   0.006999 0.007268 0.963000 0.673000 0.000000  0.468 0.368    \nV19ER    0.006999 0.005848 1.197000 0.654600 0.846400  0.489 0.726    \nV29CST   0.006862 0.006603 1.039000 0.659900 0.256000  0.510 0.444    \nV43GCU   0.006594 0.005495 1.200000 0.583000 0.818400  0.529 0.536    \nV13RWB   0.006454 0.003518 1.834000 1.170600 1.792200  0.549 0.940    \nV11CR    0.006420 0.006540 0.982000 1.484400 0.992900  0.568 0.930    \nV28VS    0.006385 0.006620 0.964000 1.088900 0.971800  0.587 0.876    \nV61SPW   0.006374 0.006302 1.011000 0.326200 0.593000  0.606 0.025 *  \nV66CBW   0.006186 0.005875 1.053000 0.220000 0.566600  0.624 0.028 *  \nV26WWCH  0.006051 0.007359 0.822000 1.210300 1.655500  0.642 0.870    \nV90WTG   0.005966 0.006614 0.902000 0.000000 0.542000  0.660 0.089 .  \nV70RSL   0.005903 0.006116 0.965000 0.000000 0.557400  0.678 0.323    \nV9SFW    0.005785 0.005795 0.998000 1.081900 1.472800  0.695 0.493    \nV42SIL   0.005356 0.004742 1.130000 0.761200 1.255000  0.711 0.959    \nV2EYR    0.005255 0.005672 0.927000 1.316800 0.874900  0.727 0.749    \nV64BELL  0.005057 0.009076 0.557000 0.492000 0.000000  0.742 0.325    \nV4BTH    0.005028 0.003157 1.593000 1.975000 1.520900  0.757 0.842    \nV96DF    0.004848 0.006302 0.769000 0.349000 0.256000  0.771 0.084 .  \nV23RBFT  0.004574 0.005837 0.784000 0.243500 0.320500  0.785 0.981    \nV36YFHE  0.004518 0.002675 1.689000 1.319700 1.383500  0.798 0.977    \nV14AUR   0.004445 0.005204 0.854000 0.987000 1.211600  0.812 0.801    \nV50CHE   0.004163 0.007474 0.557000 0.384600 0.000000  0.824 0.792    \nV79YTHE  0.003726 0.006667 0.559000 0.000000 0.396100  0.835 0.176    \nV30BTR   0.003602 0.006445 0.559000 0.000000 0.382900  0.846 0.800    \nV63DWS   0.003558 0.004682 0.760000 0.243500 0.220000  0.857 0.689    \nV24BFCS  0.003325 0.004364 0.762000 0.794400 1.014400  0.867 0.880    \nV80RF    0.003296 0.005917 0.557000 0.304500 0.000000  0.876 0.871    \nV7WEHE   0.003045 0.002323 1.311000 1.318200 1.296700  0.885 0.920    \nV1GST    0.002847 0.001972 1.444000 1.577800 1.425700  0.894 0.132    \nV27NHHE  0.002771 0.004976 0.557000 0.256000 0.000000  0.902 0.964    \nV83SFC   0.002755 0.004946 0.557000 0.261700 0.000000  0.910 0.919    \nV15STTH  0.002694 0.001759 1.532000 1.788200 1.627000  0.918 0.954    \nV3GF     0.002586 0.002274 1.137000 1.569600 1.330200  0.926 0.947    \nV94RBEE  0.002409 0.004310 0.559000 0.000000 0.256000  0.933 0.560    \nV12LK    0.002400 0.001272 1.887000 1.350100 1.122800  0.941 0.526    \nV62RBTR  0.002317 0.004159 0.557000 0.220000 0.000000  0.947 0.593    \nV8WNHE   0.002311 0.000652 3.548000 1.582200 1.595400  0.954 0.978    \nV76GBB   0.002287 0.004092 0.559000 0.000000 0.210200  0.961 0.617    \nV22SCR   0.002258 0.001532 1.474000 1.364500 1.350800  0.968 0.972    \nV5GWH    0.002206 0.001744 1.265000 1.309900 1.171900  0.975 0.940    \nV88WG    0.002163 0.003870 0.559000 0.000000 0.210200  0.981 0.469    \nV95HBC   0.002163 0.003870 0.559000 0.000000 0.210200  0.987 0.234    \nV33RWH   0.001840 0.001210 1.520000 1.597500 1.512200  0.993 0.996    \nV41SPP   0.001677 0.001086 1.544000 1.526000 1.440100  0.998 0.968    \nV6WTTR   0.000709 0.000533 1.332000 1.579900 1.525100  1.000 1.000    \nV17WPHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV25WAG   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV31AMAG  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV32SCC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV34WSW   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV37WHIP  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV38GAL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV45MGLK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV47RFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV48YTBC  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV49LYRE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV51OWH   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV52TRM   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV54STHR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV55LHE   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV57PINK  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV59YR    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV60LFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV65LWB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV67GGC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV68PIL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV71PDOV  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV72CRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV73JW    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV74BCHE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV77RRP   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV78LLOR  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV82AZKF  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV84YRTH  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV85ROSE  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV86BCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV87LFC   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV89PCOO  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV91NMIN  0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV92NFB   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV93DB    0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV97PCL   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV98FLAME 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV99WWT   0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV100WBWS 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV101LCOR 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \nV102KING 0.000000 0.000000      NaN 0.000000 0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Foothills Wo_River Red Gu \n\n           average        sd     ratio       ava       avb cumsum     p    \nV32SCC    0.023093  0.001828 12.633000  0.000000  2.225000  0.035 0.001 ***\nV17WPHE   0.021080  0.001241 16.985000  0.000000  2.034900  0.067 0.008 ** \nV4BTH     0.020489  0.002659  7.707000  1.975000  0.000000  0.098 0.001 ***\nV38GAL    0.016457  0.001736  9.477000  0.000000  1.582800  0.123 0.001 ***\nV41SPP    0.015828  0.001134 13.956000  1.526000  0.000000  0.146 0.001 ***\nV30BTR    0.015775  0.001205 13.087000  0.000000  1.520300  0.170 0.002 ** \nV11CR     0.015410  0.001314 11.727000  1.484400  0.000000  0.194 0.010 ** \nV40BRTH   0.014624  0.009247  1.582000  1.423800  0.000000  0.216 0.009 ** \nV31AMAG   0.014468  0.001047 13.821000  0.000000  1.393900  0.238 0.001 ***\nV25WAG    0.014149  0.000727 19.475000  0.000000  1.365200  0.259 0.003 ** \nV22SCR    0.014138  0.001253 11.280000  1.364500  0.000000  0.280 0.001 ***\nV21ESP    0.013956  0.002919  4.781000  1.346100  0.000000  0.301 0.009 ** \nV7WEHE    0.013692  0.003128  4.377000  1.318200  0.000000  0.322 0.003 ** \nV2EYR     0.013677  0.002086  6.556000  1.316800  0.000000  0.343 0.001 ***\nV60LFB    0.013662  0.003176  4.302000  0.000000  1.312300  0.363 0.001 ***\nV36YFHE   0.013654  0.002185  6.250000  1.319700  0.000000  0.384 0.037 *  \nV8WNHE    0.013515  0.005982  2.259000  1.582200  0.289600  0.404 0.012 *  \nV47RFC    0.012953  0.001004 12.896000  0.000000  1.251400  0.424 0.002 ** \nV45MGLK   0.012595  0.000795 15.834000  0.000000  1.216900  0.443 0.002 ** \nV13RWB    0.012138  0.001836  6.609000  1.170600  0.000000  0.461 0.063 .  \nV70RSL    0.011186  0.007252  1.543000  0.000000  1.085200  0.478 0.006 ** \nV15STTH   0.011042  0.008000  1.380000  1.788200  0.745400  0.495 0.167    \nV33RWH    0.011004  0.005906  1.863000  1.597500  0.532700  0.511 0.037 *  \nV56FTC    0.010724  0.004330  2.477000  1.244700  0.220000  0.528 0.018 *  \nV34WSW    0.010020  0.006434  1.557000  0.000000  0.961700  0.543 0.166    \nV98FLAME  0.009901  0.006236  1.588000  0.000000  0.964700  0.558 0.008 ** \nV46BHHE   0.009696  0.006173  1.571000  0.931700  0.000000  0.573 0.242    \nV73JW     0.009590  0.005943  1.614000  0.000000  0.934100  0.587 0.001 ***\nV18YTH    0.009296  0.008040  1.156000  1.199300  0.845000  0.601 0.361    \nV52TRM    0.009021  0.005747  1.570000  0.000000  0.871300  0.615 0.023 *  \nV6WTTR    0.008899  0.007759  1.147000  1.579900  0.736100  0.628 0.292    \nV93DB     0.008581  0.005139  1.670000  0.000000  0.829800  0.641 0.003 ** \nV42SIL    0.008525  0.004802  1.776000  0.761200  0.442300  0.654 0.486    \nV81SHBC   0.008439  0.005218  1.617000  0.807100  0.000000  0.667 0.065 .  \nV10WBSW   0.008169  0.008666  0.943000  0.773600  0.000000  0.679 0.631    \nV91NMIN   0.008103  0.008379  0.967000  0.000000  0.767400  0.691 0.076 .  \nV19ER     0.007916  0.006665  1.188000  0.654600  1.397300  0.703 0.441    \nV53MB     0.007757  0.005181  1.497000  0.851600  0.210200  0.715 0.046 *  \nV28VS     0.007735  0.007384  1.048000  1.088900  0.686000  0.727 0.636    \nV94RBEE   0.007585  0.004574  1.658000  0.000000  0.732200  0.738 0.002 ** \nV20PCU    0.007072  0.006526  1.084000  0.673000  0.602800  0.749 0.326    \nV29CST    0.006866  0.005049  1.360000  0.659900  0.776000  0.759 0.462    \nV59YR     0.006784  0.007017  0.967000  0.000000  0.651800  0.769 0.010 ** \nV35STP    0.006652  0.006625  1.004000  1.011200  1.638300  0.779 0.785    \nV88WG     0.006382  0.006647  0.960000  0.000000  0.612900  0.789 0.010 ** \nV90WTG    0.006382  0.006647  0.960000  0.000000  0.612900  0.799 0.053 .  \nV99WWT    0.006373  0.006612  0.964000  0.000000  0.615300  0.808 0.013 *  \nV58OBO    0.006330  0.006124  1.034000  0.261700  0.611700  0.818 0.199    \nV26WWCH   0.005950  0.006536  0.910000  1.210300  1.578000  0.827 0.872    \nV43GCU    0.005949  0.006303  0.944000  0.583000  0.000000  0.836 0.716    \nV9SFW     0.005874  0.004177  1.406000  1.081900  1.145500  0.845 0.508    \nV5GWH     0.005644  0.004760  1.186000  1.309900  0.765000  0.853 0.521    \nV72CRP    0.005509  0.005702  0.966000  0.000000  0.545600  0.862 0.017 *  \nV24BFCS   0.005494  0.005150  1.067000  0.794400  1.328700  0.870 0.489    \nV92NFB    0.005455  0.005659  0.964000  0.000000  0.513100  0.878 0.079 .  \nV71PDOV   0.005431  0.005852  0.928000  0.000000  0.530500  0.886 0.023 *  \nV3GF      0.005121  0.001989  2.575000  1.569600  1.077300  0.894 0.543    \nV64BELL   0.004995  0.008940  0.559000  0.492000  0.000000  0.902 0.353    \nV23RBFT   0.004889  0.004924  0.993000  0.243500  0.456500  0.909 0.985    \nV12LK     0.004650  0.005759  0.808000  1.350100  0.910300  0.916 0.290    \nV14AUR    0.004157  0.005215  0.797000  0.987000  1.237700  0.922 0.856    \nV50CHE    0.004108  0.007354  0.559000  0.384600  0.000000  0.928 0.790    \nV84YRTH   0.004088  0.007314  0.559000  0.000000  0.389600  0.935 0.150    \nV69SKF    0.003666  0.005226  0.701000  0.890600  1.156400  0.940 0.812    \nV101LCOR  0.003664  0.006556  0.559000  0.000000  0.370000  0.946 0.155    \nV96DF     0.003588  0.006422  0.559000  0.349000  0.000000  0.951 0.229    \nV61SPW    0.003353  0.006002  0.559000  0.326200  0.000000  0.956 0.239    \nV80RF     0.003252  0.005822  0.559000  0.304500  0.000000  0.961 0.892    \nV87LFC    0.003113  0.005569  0.559000  0.000000  0.314400  0.966 0.622    \nV74BCHE   0.002825  0.005054  0.559000  0.000000  0.261700  0.970 0.144    \nV27NHHE   0.002735  0.004896  0.559000  0.256000  0.000000  0.974 0.975    \nV83SFC    0.002720  0.004869  0.559000  0.261700  0.000000  0.978 0.934    \nV16LR     0.002626  0.002077  1.264000  1.479800  1.252000  0.982 0.995    \nV1GST     0.002484  0.002388  1.040000  1.577800  1.343300  0.986 0.304    \nV63DWS    0.002472  0.004425  0.559000  0.243500  0.000000  0.990 0.829    \nV62RBTR   0.002287  0.004094  0.559000  0.220000  0.000000  0.993 0.596    \nV66CBW    0.002287  0.004094  0.559000  0.220000  0.000000  0.997 0.614    \nV95HBC    0.002146  0.003840  0.559000  0.000000  0.198800  1.000 0.302    \nV37WHIP   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV39FHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV44MUSK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV48YTBC   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV49LYRE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV51OWH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV54STHR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV55LHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV57PINK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV65LWB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV67GGC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV68PIL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV75RCR    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV76GBB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV79YTHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV85ROSE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV86BCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV102KING  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: Box-Ironbark_River Red Gu \n\n           average        sd     ratio       ava       avb cumsum     p    \nV32SCC    0.023209  0.002485  9.340000  0.000000  2.225000  0.036 0.003 ** \nV17WPHE   0.021185  0.001961 10.802000  0.000000  2.034900  0.068 0.005 ** \nV46BHHE   0.018779  0.002092  8.977000  1.798900  0.000000  0.097 0.001 ***\nV13RWB    0.018603  0.002860  6.505000  1.792200  0.000000  0.126 0.001 ***\nV38GAL    0.016539  0.002119  7.804000  0.000000  1.582800  0.151 0.002 ** \nV4BTH     0.015839  0.002532  6.255000  1.520900  0.000000  0.175 0.014 *  \nV39FHE    0.015712  0.002885  5.447000  1.532700  0.000000  0.200 0.001 ***\nV41SPP    0.014960  0.001224 12.222000  1.440100  0.000000  0.222 0.001 ***\nV31AMAG   0.014541  0.001484  9.795000  0.000000  1.393900  0.245 0.004 ** \nV36YFHE   0.014477  0.004922  2.941000  1.383500  0.000000  0.267 0.013 *  \nV25WAG    0.014220  0.001253 11.353000  0.000000  1.365200  0.289 0.001 ***\nV22SCR    0.014102  0.002784  5.066000  1.350800  0.000000  0.311 0.002 ** \nV40BRTH   0.013917  0.008320  1.673000  1.379300  0.000000  0.332 0.012 *  \nV8WNHE    0.013741  0.005715  2.405000  1.595400  0.289600  0.353 0.012 *  \nV60LFB    0.013731  0.003352  4.097000  0.000000  1.312300  0.374 0.002 ** \nV7WEHE    0.013625  0.003185  4.278000  1.296700  0.000000  0.395 0.006 ** \nV47RFC    0.013017  0.001371  9.494000  0.000000  1.251400  0.415 0.001 ***\nV45MGLK   0.012658  0.001205 10.507000  0.000000  1.216900  0.435 0.001 ***\nV30BTR    0.012531  0.007039  1.780000  0.382900  1.520300  0.454 0.012 *  \nV69SKF    0.012038  0.001175 10.245000  0.000000  1.156400  0.472 0.007 ** \nV44MUSK   0.011946  0.007696  1.552000  1.180800  0.000000  0.491 0.010 ** \nV18YTH    0.011253  0.009055  1.243000  1.909900  0.845000  0.508 0.103    \nV42SIL    0.011190  0.003985  2.808000  1.255000  0.442300  0.525 0.060 .  \nV35STP    0.010880  0.007777  1.399000  0.656700  1.638300  0.542 0.052 .  \nV75RCR    0.010550  0.002971  3.552000  1.000900  0.000000  0.558 0.001 ***\nV33RWH    0.010203  0.005935  1.719000  1.512200  0.532700  0.574 0.031 *  \nV34WSW    0.010070  0.006524  1.544000  0.000000  0.961700  0.589 0.158    \nV11CR     0.009963  0.006247  1.595000  0.992900  0.000000  0.605 0.368    \nV98FLAME  0.009949  0.006321  1.574000  0.000000  0.964700  0.620 0.005 ** \nV73JW     0.009637  0.006026  1.599000  0.000000  0.934100  0.635 0.001 ***\nV15STTH   0.009594  0.007905  1.214000  1.627000  0.745400  0.649 0.357    \nV52TRM    0.009066  0.005827  1.556000  0.000000  0.871300  0.663 0.026 *  \nV2EYR     0.009033  0.005592  1.615000  0.874900  0.000000  0.677 0.103    \nV93DB     0.008624  0.005214  1.654000  0.000000  0.829800  0.690 0.001 ***\nV70RSL    0.008584  0.006538  1.313000  0.557400  1.085200  0.704 0.064 .  \nV6WTTR    0.008576  0.007595  1.129000  1.525100  0.736100  0.717 0.308    \nV43GCU    0.008275  0.004993  1.657000  0.818400  0.000000  0.730 0.118    \nV91NMIN   0.008145  0.008464  0.962000  0.000000  0.767400  0.742 0.063 .  \nV16LR     0.007429  0.006376  1.165000  0.576500  1.252000  0.753 0.641    \nV28VS     0.007148  0.006803  1.051000  0.971800  0.686000  0.764 0.785    \nV29CST    0.006978  0.005342  1.306000  0.256000  0.776000  0.775 0.372    \nV59YR     0.006818  0.007086  0.962000  0.000000  0.651800  0.786 0.011 *  \nV58OBO    0.006742  0.004954  1.361000  1.059200  0.611700  0.796 0.138    \nV90WTG    0.006683  0.006034  1.107000  0.542000  0.612900  0.806 0.038 *  \nV94RBEE   0.006671  0.005063  1.318000  0.256000  0.732200  0.817 0.007 ** \nV88WG     0.006408  0.005997  1.069000  0.210200  0.612900  0.826 0.012 *  \nV99WWT    0.006405  0.006678  0.959000  0.000000  0.615300  0.836 0.010 ** \nV66CBW    0.006280  0.006500  0.966000  0.566600  0.000000  0.846 0.021 *  \nV19ER     0.006192  0.005991  1.034000  0.846400  1.397300  0.855 0.834    \nV20PCU    0.006104  0.006440  0.948000  0.000000  0.602800  0.865 0.720    \nV61SPW    0.005762  0.006007  0.959000  0.593000  0.000000  0.874 0.041 *  \nV23RBFT   0.005699  0.005229  1.090000  0.320500  0.456500  0.882 0.959    \nV72CRP    0.005535  0.005754  0.962000  0.000000  0.545600  0.891 0.015 *  \nV92NFB    0.005483  0.005717  0.959000  0.000000  0.513100  0.899 0.068 .  \nV71PDOV   0.005457  0.005906  0.924000  0.000000  0.530500  0.908 0.025 *  \nV5GWH     0.004967  0.004736  1.049000  1.171900  0.765000  0.915 0.667    \nV9SFW     0.004265  0.002744  1.554000  1.472800  1.145500  0.922 0.697    \nV84YRTH   0.004108  0.007375  0.557000  0.000000  0.389600  0.928 0.142    \nV56FTC    0.003830  0.004875  0.786000  0.281200  0.220000  0.934 0.956    \nV12LK     0.003782  0.004861  0.778000  1.122800  0.910300  0.940 0.381    \nV79YTHE   0.003700  0.006623  0.559000  0.396100  0.000000  0.946 0.176    \nV101LCOR  0.003681  0.006605  0.557000  0.000000  0.370000  0.951 0.143    \nV24BFCS   0.003288  0.002264  1.452000  1.014400  1.328700  0.956 0.907    \nV95HBC    0.003249  0.004283  0.759000  0.210200  0.198800  0.961 0.055 .  \nV3GF      0.003169  0.001979  1.601000  1.330200  1.077300  0.966 0.835    \nV87LFC    0.003127  0.005611  0.557000  0.000000  0.314400  0.971 0.613    \nV74BCHE   0.002840  0.005099  0.557000  0.000000  0.261700  0.975 0.131    \nV81SHBC   0.002627  0.004702  0.559000  0.281200  0.000000  0.979 0.990    \nV96DF     0.002392  0.004281  0.559000  0.256000  0.000000  0.983 0.522    \nV76GBB    0.002269  0.004061  0.559000  0.210200  0.000000  0.986 0.606    \nV53MB     0.002217  0.003979  0.557000  0.000000  0.210200  0.990 0.873    \nV63DWS    0.002056  0.003679  0.559000  0.220000  0.000000  0.993 0.896    \nV1GST     0.001950  0.001137  1.714000  1.425700  1.343300  0.996 0.631    \nV26WWCH   0.001348  0.001039  1.297000  1.655500  1.578000  0.998 0.987    \nV14AUR    0.001268  0.000601  2.111000  1.211600  1.237700  1.000 0.992    \nV10WBSW   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV21ESP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV27NHHE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV37WHIP   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV48YTBC   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV49LYRE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV50CHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV51OWH    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV54STHR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV55LHE    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV57PINK   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV62RBTR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV64BELL   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV65LWB    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV67GGC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV68PIL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV77RRP    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV78LLOR   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV80RF     0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV82AZKF   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV83SFC    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV85ROSE   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV86BCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV89PCOO   0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV97PCL    0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV100WBWS  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \nV102KING  0.000000  0.000000       NaN  0.000000  0.000000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 999\n\n\nThe SIMPER analysis shows which species contribute most to the dissimilarity between habitat groups. This helps identify indicator species or species that drive community differences."
  },
  {
    "objectID": "tutorials/tutorial10.html",
    "href": "tutorials/tutorial10.html",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "Students in class gave “Characteristics of a good ecology lecturer”. They scored from 1-10 on a range of characteristics:\n\naccessible\nattractive\nWide range of topics\nDo teaching research\nDress well\nComplex issues\nExample exam questions\nNotes before\nhumour\nEasy marking\nMove around lecture\nDetailed lecture notes\nClear objective to lecture\nSolicit questions\nUse textbook closely\nFocus on primary research\nUse their own research\nTeach in team\nUnderstand principles of teaching\nUse pointers\nrespected\n\nThe students also indicated whether they were:\n\na gender (male/female)\nstudied mainly a system (marine/terrestrial)\nstudied mainly an organismal group (plants/animals)\n\nWe are going to use principal components analysis and factor analysis to interpret this dataset.\n\n\nLoad these packages to make the plots look better:\n\nlibrary(EFAtools)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\n\n\n\nFirst open your dataset:\n\nlecturers &lt;- read.csv(\"data/lecturers.csv\", header = TRUE)\n\n\n\n\nLet’s first do a correlation matrix:\n\ncor(lecturers[, 8:28])\n\n                         accessible  attractive   wide_range\naccessible               1.00000000 -0.61876084  0.190062066\nattractive              -0.61876084  1.00000000 -0.011570169\nwide_range               0.19006207 -0.01157017  1.000000000\nteaching.based_research  0.24021638 -0.21382943  0.462870058\nDress_nicely            -0.53198037  0.81887024 -0.022848836\nfocus_on_complex_issues  0.28252341 -0.04447540  0.567443460\ngives_example           -0.23669223  0.23435837  0.065815857\ngives_out_note          -0.31173854  0.36819405 -0.187661524\nsense_of_humour         -0.29068273  0.33747896  0.081503385\nmark_easily             -0.34976811  0.44361281 -0.158545138\nmoves_around            -0.23371225  0.18475768 -0.087292839\noffers_lecture_notes    -0.25762452  0.28256570 -0.070931605\nclear_objective         -0.21819811  0.14978520 -0.176518448\nsolicit_questions        0.10349600 -0.07667948 -0.022123679\ntext_book                0.03170769  0.02849978 -0.241108100\nprimary_research         0.14147306 -0.06238516 -0.009628388\nown_research            -0.01089281  0.15175696  0.157013029\nteach_in_team            0.09004589 -0.13904104  0.005163136\nunderstand_principles   -0.08521319 -0.01080764 -0.097173242\nuse_a_pointer           -0.51717009  0.25526667 -0.131935123\nwell_respected          -0.12789662  0.04222022  0.037698097\n                        teaching.based_research Dress_nicely\naccessible                           0.24021638 -0.531980374\nattractive                          -0.21382943  0.818870242\nwide_range                           0.46287006 -0.022848836\nteaching.based_research              1.00000000 -0.079662581\nDress_nicely                        -0.07966258  1.000000000\nfocus_on_complex_issues              0.36612054 -0.074522662\ngives_example                       -0.23626944  0.207509283\ngives_out_note                      -0.13154529  0.349298764\nsense_of_humour                     -0.37115541  0.326981372\nmark_easily                         -0.26802282  0.293513890\nmoves_around                         0.35915941  0.175612992\noffers_lecture_notes                -0.26062206  0.304730151\nclear_objective                     -0.10137698  0.162176313\nsolicit_questions                    0.06997014 -0.047353486\ntext_book                           -0.02511112  0.065661794\nprimary_research                     0.08627931 -0.125765210\nown_research                         0.24450347  0.131114596\nteach_in_team                        0.05621459 -0.081891992\nunderstand_principles                0.13360929  0.003830791\nuse_a_pointer                       -0.40503932  0.246189390\nwell_respected                      -0.21574299  0.082434616\n                        focus_on_complex_issues gives_example gives_out_note\naccessible                           0.28252341   -0.23669223    -0.31173854\nattractive                          -0.04447540    0.23435837     0.36819405\nwide_range                           0.56744346    0.06581586    -0.18766152\nteaching.based_research              0.36612054   -0.23626944    -0.13154529\nDress_nicely                        -0.07452266    0.20750928     0.34929876\nfocus_on_complex_issues              1.00000000   -0.15606162    -0.31138949\ngives_example                       -0.15606162    1.00000000     0.41459959\ngives_out_note                      -0.31138949    0.41459959     1.00000000\nsense_of_humour                     -0.07005663    0.22759892     0.25028755\nmark_easily                         -0.18475242    0.18880641     0.39259669\nmoves_around                         0.06116819   -0.15496603     0.26746010\noffers_lecture_notes                 0.04438636    0.67139954     0.32650197\nclear_objective                     -0.05740760    0.22152350     0.49329825\nsolicit_questions                    0.12547982   -0.10034117     0.37306647\ntext_book                           -0.14904151    0.23189356     0.41705241\nprimary_research                     0.33824172   -0.05502391    -0.00258233\nown_research                         0.21007669   -0.03858742     0.11486585\nteach_in_team                       -0.05016868    0.22073735     0.07101795\nunderstand_principles                0.12582565    0.06367067     0.09564742\nuse_a_pointer                       -0.19722690    0.29421612     0.42798936\nwell_respected                      -0.15321258    0.20022401     0.24197769\n                        sense_of_humour  mark_easily moves_around\naccessible                  -0.29068273 -0.349768108 -0.233712249\nattractive                   0.33747896  0.443612809  0.184757685\nwide_range                   0.08150339 -0.158545138 -0.087292839\nteaching.based_research     -0.37115541 -0.268022817  0.359159406\nDress_nicely                 0.32698137  0.293513890  0.175612992\nfocus_on_complex_issues     -0.07005663 -0.184752419  0.061168190\ngives_example                0.22759892  0.188806413 -0.154966026\ngives_out_note               0.25028755  0.392596691  0.267460102\nsense_of_humour              1.00000000  0.279195853  0.147981262\nmark_easily                  0.27919585  1.000000000 -0.102624026\nmoves_around                 0.14798126 -0.102624026  1.000000000\noffers_lecture_notes         0.40350521  0.004540766  0.116586008\nclear_objective              0.30494267  0.241714172  0.328029643\nsolicit_questions            0.22196924  0.221507736  0.357039548\ntext_book                    0.11515826  0.493885950  0.018944976\nprimary_research            -0.11402253  0.013988221  0.201361759\nown_research                 0.19400676 -0.136905883  0.521788320\nteach_in_team               -0.13889755 -0.029214646 -0.002012914\nunderstand_principles        0.08973153  0.029511600  0.281044416\nuse_a_pointer                0.60147995  0.200483903  0.247735287\nwell_respected               0.31777004  0.204840208  0.107275533\n                        offers_lecture_notes clear_objective solicit_questions\naccessible                      -0.257624516     -0.21819811       0.103495996\nattractive                       0.282565698      0.14978520      -0.076679483\nwide_range                      -0.070931605     -0.17651845      -0.022123679\nteaching.based_research         -0.260622062     -0.10137698       0.069970142\nDress_nicely                     0.304730151      0.16217631      -0.047353486\nfocus_on_complex_issues          0.044386363     -0.05740760       0.125479819\ngives_example                    0.671399539      0.22152350      -0.100341166\ngives_out_note                   0.326501972      0.49329825       0.373066471\nsense_of_humour                  0.403505211      0.30494267       0.221969236\nmark_easily                      0.004540766      0.24171417       0.221507736\nmoves_around                     0.116586008      0.32802964       0.357039548\noffers_lecture_notes             1.000000000      0.51135418       0.134906607\nclear_objective                  0.511354179      1.00000000       0.667857876\nsolicit_questions                0.134906607      0.66785788       1.000000000\ntext_book                        0.241763186      0.50985104       0.514856144\nprimary_research                 0.027238866     -0.05933422       0.008953955\nown_research                     0.014724611      0.04037952       0.313649567\nteach_in_team                    0.134589361      0.09431599       0.129502715\nunderstand_principles            0.204933977      0.52773521       0.525180942\nuse_a_pointer                    0.475656687      0.48740938       0.245720968\nwell_respected                   0.173541917      0.20806935       0.217669413\n                           text_book primary_research own_research\naccessible               0.031707686      0.141473062  -0.01089281\nattractive               0.028499784     -0.062385161   0.15175696\nwide_range              -0.241108100     -0.009628388   0.15701303\nteaching.based_research -0.025111119      0.086279306   0.24450347\nDress_nicely             0.065661794     -0.125765210   0.13111460\nfocus_on_complex_issues -0.149041509      0.338241715   0.21007669\ngives_example            0.231893556     -0.055023907  -0.03858742\ngives_out_note           0.417052411     -0.002582330   0.11486585\nsense_of_humour          0.115158256     -0.114022533   0.19400676\nmark_easily              0.493885950      0.013988221  -0.13690588\nmoves_around             0.018944976      0.201361759   0.52178832\noffers_lecture_notes     0.241763186      0.027238866   0.01472461\nclear_objective          0.509851042     -0.059334220   0.04037952\nsolicit_questions        0.514856144      0.008953955   0.31364957\ntext_book                1.000000000     -0.092231864  -0.11384268\nprimary_research        -0.092231864      1.000000000   0.35299282\nown_research            -0.113842676      0.352992824   1.00000000\nteach_in_team            0.176402014     -0.085933891  -0.08668898\nunderstand_principles    0.287773155      0.017218953   0.06370238\nuse_a_pointer            0.001337384     -0.149851010   0.15399945\nwell_respected           0.274222528      0.016210854  -0.10140227\n                        teach_in_team understand_principles use_a_pointer\naccessible                0.090045886          -0.085213191  -0.517170092\nattractive               -0.139041035          -0.010807636   0.255266673\nwide_range                0.005163136          -0.097173242  -0.131935123\nteaching.based_research   0.056214590           0.133609295  -0.405039321\nDress_nicely             -0.081891992           0.003830791   0.246189390\nfocus_on_complex_issues  -0.050168682           0.125825652  -0.197226905\ngives_example             0.220737347           0.063670673   0.294216117\ngives_out_note            0.071017950           0.095647419   0.427989355\nsense_of_humour          -0.138897546           0.089731525   0.601479950\nmark_easily              -0.029214646           0.029511600   0.200483903\nmoves_around             -0.002012914           0.281044416   0.247735287\noffers_lecture_notes      0.134589361           0.204933977   0.475656687\nclear_objective           0.094315990           0.527735214   0.487409381\nsolicit_questions         0.129502715           0.525180942   0.245720968\ntext_book                 0.176402014           0.287773155   0.001337384\nprimary_research         -0.085933891           0.017218953  -0.149851010\nown_research             -0.086688981           0.063702380   0.153999453\nteach_in_team             1.000000000           0.101857142  -0.016777680\nunderstand_principles     0.101857142           1.000000000   0.104631048\nuse_a_pointer            -0.016777680           0.104631048   1.000000000\nwell_respected            0.545915684           0.207782811   0.318541502\n                        well_respected\naccessible                 -0.12789662\nattractive                  0.04222022\nwide_range                  0.03769810\nteaching.based_research    -0.21574299\nDress_nicely                0.08243462\nfocus_on_complex_issues    -0.15321258\ngives_example               0.20022401\ngives_out_note              0.24197769\nsense_of_humour             0.31777004\nmark_easily                 0.20484021\nmoves_around                0.10727553\noffers_lecture_notes        0.17354192\nclear_objective             0.20806935\nsolicit_questions           0.21766941\ntext_book                   0.27422253\nprimary_research            0.01621085\nown_research               -0.10140227\nteach_in_team               0.54591568\nunderstand_principles       0.20778281\nuse_a_pointer               0.31854150\nwell_respected              1.00000000\n\nCorrmatrix &lt;- cor(lecturers[, 8:28])\n\n\n\n\nNow we can do the Bartlett’s Test of Sphericity. This test compares the correlation matrix to an identity matrix. If it is significant, it is worth doing a PCA.\n\nBARTLETT(Corrmatrix, N = 34, cor_method = c(\"pearson\"))\n\n\n✔ The Bartlett's test of sphericity was significant at an alpha level of .05.\n  These data are probably suitable for factor analysis.\n\n  𝜒²(210) = 355.07, p &lt; .001\n\n\n\n\n\nNote: to make this a PCA based on a correlation matrix, we have to scale the variables, hence scale = TRUE. There are two main principal components functions, but they are very similar. Note prcomp calls the loadings “rotations”, not to be confused with rotations below.\n\npca1 &lt;- prcomp(lecturers[, 8:28], scale = TRUE)\npca2 &lt;- princomp(lecturers[, 8:28], cor = TRUE)\n\n\nsummary(pca1)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.2206 1.6616 1.5324 1.29807 1.19561 1.12098 1.08311\nProportion of Variance 0.2348 0.1315 0.1118 0.08024 0.06807 0.05984 0.05586\nCumulative Proportion  0.2348 0.3663 0.4781 0.55833 0.62641 0.68624 0.74211\n                          PC8     PC9    PC10    PC11   PC12    PC13    PC14\nStandard deviation     1.0737 0.95681 0.79545 0.73119 0.7055 0.65580 0.50887\nProportion of Variance 0.0549 0.04359 0.03013 0.02546 0.0237 0.02048 0.01233\nCumulative Proportion  0.7970 0.84060 0.87073 0.89619 0.9199 0.94037 0.95270\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.49817 0.47309 0.42640 0.36970 0.32493 0.24020 0.19872\nProportion of Variance 0.01182 0.01066 0.00866 0.00651 0.00503 0.00275 0.00188\nCumulative Proportion  0.96452 0.97518 0.98384 0.99034 0.99537 0.99812 1.00000\n\nsummary(pca2)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5\nStandard deviation     2.2205654 1.6615887 1.5324030 1.29806969 1.19560985\nProportion of Variance 0.2348053 0.1314703 0.1118219 0.08023738 0.06807062\nCumulative Proportion  0.2348053 0.3662756 0.4780975 0.55833484 0.62640546\n                           Comp.6     Comp.7     Comp.8     Comp.9    Comp.10\nStandard deviation     1.12097809 1.08311410 1.07371597 0.95680927 0.79544930\nProportion of Variance 0.05983771 0.05586363 0.05489838 0.04359448 0.03013046\nCumulative Proportion  0.68624317 0.74210679 0.79700517 0.84059965 0.87073010\n                          Comp.11    Comp.12    Comp.13    Comp.14    Comp.15\nStandard deviation     0.73119377 0.70551609 0.65579857 0.50886890 0.49817027\nProportion of Variance 0.02545925 0.02370252 0.02047961 0.01233084 0.01181779\nCumulative Proportion  0.89618936 0.91989188 0.94037149 0.95270232 0.96452011\n                         Comp.16     Comp.17     Comp.18     Comp.19\nStandard deviation     0.4730918 0.426395520 0.369702631 0.324933440\nProportion of Variance 0.0106579 0.008657769 0.006508573 0.005027702\nCumulative Proportion  0.9751780 0.983835781 0.990344355 0.995372057\n                           Comp.20     Comp.21\nStandard deviation     0.240198751 0.198724365\nProportion of Variance 0.002747402 0.001880542\nCumulative Proportion  0.998119458 1.000000000\n\n\n\n\n\nLet’s look at the loadings. Called “rotations” in prcomp and “loadings” in princomp. They are the Pearson’s correlation between that variable and that Principal Component.\n\npca1\n\nStandard deviations (1, .., p=21):\n [1] 2.2205654 1.6615887 1.5324030 1.2980697 1.1956099 1.1209781 1.0831141\n [8] 1.0737160 0.9568093 0.7954493 0.7311938 0.7055161 0.6557986 0.5088689\n[15] 0.4981703 0.4730918 0.4263955 0.3697026 0.3249334 0.2401988 0.1987244\n\nRotation (n x k) = (21 x 21):\n                                PC1         PC2         PC3          PC4\naccessible              -0.27105784  0.21481141  0.22284903 -0.109380126\nattractive               0.25953819 -0.19001404 -0.35322829  0.047967149\nwide_range              -0.11796047  0.14523503 -0.23536689 -0.465316942\nteaching.based_research -0.17807186  0.33523672 -0.16973235 -0.005540905\nDress_nicely             0.24737662 -0.15550598 -0.32565089  0.019172145\nfocus_on_complex_issues -0.13467331  0.28837561 -0.24412817 -0.297540766\ngives_example            0.22797978 -0.11739164  0.06984178 -0.437201342\ngives_out_note           0.32056620  0.04438472  0.02749136  0.100499687\nsense_of_humour          0.27564472 -0.03120757 -0.13957405 -0.145704311\nmark_easily              0.23686285 -0.11243202  0.08113616  0.196601185\nmoves_around             0.13236700  0.32827473 -0.27087444  0.213497862\noffers_lecture_notes     0.28307512  0.02357872 -0.01627849 -0.372830743\nclear_objective          0.31573398  0.26378859  0.12826236  0.066194937\nsolicit_questions        0.18991164  0.43659484  0.13380240  0.160366349\ntext_book                0.21299001  0.16712512  0.33485831  0.124899445\nprimary_research        -0.05608411  0.19206706 -0.16387602  0.001745603\nown_research             0.04318371  0.28387549 -0.37836892  0.056442158\nteach_in_team            0.04818137  0.11021896  0.29523823 -0.327405642\nunderstand_principles    0.14556568  0.33936721  0.10777339  0.053017840\nuse_a_pointer            0.32019196 -0.03811643 -0.08101559 -0.080227102\nwell_respected           0.19384039  0.06977679  0.22029921 -0.263146206\n                                 PC5         PC6          PC7         PC8\naccessible              -0.006862732  0.20175545  0.075192964  0.13471983\nattractive              -0.304217305 -0.05278985 -0.006014137 -0.01346850\nwide_range              -0.251311171  0.08634713 -0.331523146  0.07096692\nteaching.based_research -0.307853355 -0.22290900 -0.040540304 -0.27328747\nDress_nicely            -0.299180706 -0.12573075 -0.046516486 -0.12380132\nfocus_on_complex_issues -0.170654964  0.29340602 -0.050872762  0.07474147\ngives_example           -0.075647764  0.06855598  0.364935010 -0.17131172\ngives_out_note          -0.125013318 -0.08366732  0.223406707  0.02145305\nsense_of_humour          0.216056466  0.21464296 -0.328598912  0.27397871\nmark_easily             -0.421810061  0.15983061 -0.046406386  0.38018642\nmoves_around             0.166526391 -0.35024271  0.055092321 -0.07707096\noffers_lecture_notes     0.146235397  0.18757528  0.287000528 -0.27400475\nclear_objective          0.060342195  0.17777324 -0.032682046 -0.21837583\nsolicit_questions       -0.005900941  0.14588578 -0.182965113  0.09074671\ntext_book               -0.356204508  0.15361788  0.084096939  0.02338965\nprimary_research         0.039054189  0.06897414  0.592731543  0.45913721\nown_research             0.156844077 -0.09496398  0.160942737  0.20826388\nteach_in_team           -0.096207102 -0.54758870  0.037625674  0.05473642\nunderstand_principles    0.010243773  0.05372028 -0.114475017 -0.27603888\nuse_a_pointer            0.411769112  0.01616452 -0.180288938  0.05917537\nwell_respected           0.013159486 -0.40377696 -0.183055505  0.39655405\n                                PC9        PC10        PC11         PC12\naccessible               0.23711499  0.41979852  0.03945146  0.148881566\nattractive              -0.16438427  0.20365082  0.06904042 -0.107432225\nwide_range               0.22155409 -0.26438749 -0.03259079 -0.018048262\nteaching.based_research  0.19889412 -0.24510749 -0.11135378  0.247899071\nDress_nicely            -0.10406494  0.38639604  0.08377430  0.098387712\nfocus_on_complex_issues -0.28979913  0.00432150  0.36226348 -0.066562002\ngives_example            0.19510676 -0.15342441 -0.35973985 -0.159175284\ngives_out_note           0.41895610 -0.21289394  0.20038838  0.006019332\nsense_of_humour          0.09730489  0.19787809 -0.29410062  0.309619454\nmark_easily             -0.04288340 -0.25885827 -0.05713892 -0.143735223\nmoves_around             0.02114182 -0.04788447  0.02682634  0.353171999\noffers_lecture_notes    -0.07931457  0.16874959  0.11255136  0.207814735\nclear_objective         -0.04989256 -0.08180409  0.29595319  0.009127372\nsolicit_questions        0.10754501  0.13028684  0.14132328 -0.276862219\ntext_book                0.12628767  0.17521009 -0.12180066  0.267854799\nprimary_research        -0.30348754 -0.18921946  0.01954047  0.138689169\nown_research             0.28543090  0.27973695 -0.29907250 -0.427500246\nteach_in_team           -0.07446630  0.20427502  0.21413563 -0.322896702\nunderstand_principles   -0.49690454 -0.08895650 -0.50033599 -0.189676003\nuse_a_pointer            0.09757553 -0.27912944  0.22992547 -0.115886008\nwell_respected          -0.19000184 -0.03498965 -0.08926599  0.276488059\n                               PC13        PC14         PC15         PC16\naccessible               0.29231904  0.29580819 -0.222218786  0.095682695\nattractive               0.10512874  0.20004636  0.069413634 -0.326657310\nwide_range               0.14339377 -0.15578216 -0.042633744 -0.346118318\nteaching.based_research -0.16129758 -0.08569727 -0.291561886  0.304862272\nDress_nicely             0.22024728 -0.29025496 -0.196064822  0.331319175\nfocus_on_complex_issues -0.11651678  0.21121417  0.380088085  0.275036237\ngives_example           -0.02596566  0.13003779  0.073844050 -0.048615414\ngives_out_note           0.52366782  0.22815294  0.150396690  0.187752138\nsense_of_humour         -0.09102492  0.24787583 -0.269328590  0.077361879\nmark_easily             -0.30618081  0.26528469 -0.239118638  0.067806707\nmoves_around            -0.19413727  0.41519824  0.163481854 -0.233951595\noffers_lecture_notes    -0.18626080 -0.01943147 -0.008128194 -0.028997162\nclear_objective          0.02299500 -0.21161788 -0.392005665 -0.377275931\nsolicit_questions        0.07959209 -0.06601352  0.067535709 -0.098815936\ntext_book               -0.31596633 -0.28217199  0.346092878  0.063696048\nprimary_research         0.11269387 -0.18656539 -0.263215477 -0.012502540\nown_research            -0.14671997 -0.23213427  0.082887276  0.001508984\nteach_in_team           -0.23420220  0.19002827 -0.271548974  0.056217287\nunderstand_principles    0.29771815  0.14826763 -0.020146024  0.175848894\nuse_a_pointer           -0.10543967 -0.11603812 -0.008762183  0.433779829\nwell_respected           0.22357927 -0.22236358  0.251201419 -0.060389007\n                                PC17         PC18        PC19        PC20\naccessible              -0.487523780 -0.092943639  0.11623660 -0.08184741\nattractive              -0.106937105 -0.195239242  0.45086924 -0.09762660\nwide_range               0.009616636  0.068156323  0.05963088 -0.34244915\nteaching.based_research  0.006376281  0.018683190  0.22919635  0.23688191\nDress_nicely            -0.124236446  0.174946567 -0.38290347 -0.11111538\nfocus_on_complex_issues  0.057689702 -0.197551507 -0.21480326  0.19658872\ngives_example           -0.206970055  0.030539850 -0.40647428 -0.01600380\ngives_out_note           0.321857496 -0.107588372  0.08121002  0.10250939\nsense_of_humour          0.430141107 -0.113398698 -0.08853507  0.02641527\nmark_easily             -0.269651092  0.167631153  0.01976224  0.19779113\nmoves_around            -0.207902119  0.101506396 -0.24054414 -0.20539739\noffers_lecture_notes     0.024256519  0.371837654  0.41986631  0.20173823\nclear_objective         -0.097333800 -0.419634434 -0.21848656  0.23314508\nsolicit_questions        0.094290088  0.606166863 -0.05758082 -0.04550053\ntext_book                0.072364215 -0.254638772  0.07071928 -0.35855399\nprimary_research         0.130404141  0.041767487  0.01625913 -0.25697656\nown_research            -0.068753059 -0.196939862  0.08026083  0.24801010\nteach_in_team            0.247146896 -0.095095343  0.00645715 -0.19776650\nunderstand_principles   -0.001605969 -0.075193184  0.12987739 -0.13337875\nuse_a_pointer           -0.352414119 -0.113600108  0.19774694 -0.34608373\nwell_respected          -0.231089402  0.008802903  0.03736378  0.36974689\n                                PC21\naccessible              -0.002932735\nattractive               0.405148279\nwide_range              -0.308435374\nteaching.based_research  0.335322467\nDress_nicely            -0.150318850\nfocus_on_complex_issues  0.045604055\ngives_example            0.338040434\ngives_out_note          -0.184254445\nsense_of_humour          0.144536843\nmark_easily             -0.277239849\nmoves_around            -0.151918843\noffers_lecture_notes    -0.289119982\nclear_objective         -0.008051690\nsolicit_questions        0.367111373\ntext_book               -0.026709294\nprimary_research         0.123144467\nown_research            -0.206478451\nteach_in_team           -0.055864220\nunderstand_principles   -0.183080778\nuse_a_pointer            0.121125177\nwell_respected           0.079347988\n\nloadings(pca2)\n\n\nLoadings:\n                        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8\naccessible               0.271  0.215  0.223  0.109         0.202         0.135\nattractive              -0.260 -0.190 -0.353         0.304                     \nwide_range               0.118  0.145 -0.235  0.465  0.251        -0.332       \nteaching.based_research  0.178  0.335 -0.170         0.308 -0.223        -0.273\nDress_nicely            -0.247 -0.156 -0.326         0.299 -0.126        -0.124\nfocus_on_complex_issues  0.135  0.288 -0.244  0.298  0.171  0.293              \ngives_example           -0.228 -0.117         0.437                0.365 -0.171\ngives_out_note          -0.321               -0.100  0.125         0.223       \nsense_of_humour         -0.276        -0.140  0.146 -0.216  0.215 -0.329  0.274\nmark_easily             -0.237 -0.112        -0.197  0.422  0.160         0.380\nmoves_around            -0.132  0.328 -0.271 -0.213 -0.167 -0.350              \noffers_lecture_notes    -0.283                0.373 -0.146  0.188  0.287 -0.274\nclear_objective         -0.316  0.264  0.128                0.178        -0.218\nsolicit_questions       -0.190  0.437  0.134 -0.160         0.146 -0.183       \ntext_book               -0.213  0.167  0.335 -0.125  0.356  0.154              \nprimary_research                0.192 -0.164                       0.593  0.459\nown_research                    0.284 -0.378        -0.157         0.161  0.208\nteach_in_team                   0.110  0.295  0.327        -0.548              \nunderstand_principles   -0.146  0.339  0.108                      -0.114 -0.276\nuse_a_pointer           -0.320                      -0.412        -0.180       \nwell_respected          -0.194         0.220  0.263        -0.404 -0.183  0.397\n                        Comp.9 Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15\naccessible               0.237  0.420           0.149   0.292   0.296   0.222 \nattractive              -0.164  0.204          -0.107   0.105   0.200         \nwide_range               0.222 -0.264                   0.143  -0.156         \nteaching.based_research  0.199 -0.245  -0.111   0.248  -0.161           0.292 \nDress_nicely            -0.104  0.386                   0.220  -0.290   0.196 \nfocus_on_complex_issues -0.290          0.362          -0.117   0.211  -0.380 \ngives_example            0.195 -0.153  -0.360  -0.159           0.130         \ngives_out_note           0.419 -0.213   0.200           0.524   0.228  -0.150 \nsense_of_humour                 0.198  -0.294   0.310           0.248   0.269 \nmark_easily                    -0.259          -0.144  -0.306   0.265   0.239 \nmoves_around                                    0.353  -0.194   0.415  -0.163 \noffers_lecture_notes            0.169   0.113   0.208  -0.186                 \nclear_objective                         0.296                  -0.212   0.392 \nsolicit_questions        0.108  0.130   0.141  -0.277                         \ntext_book                0.126  0.175  -0.122   0.268  -0.316  -0.282  -0.346 \nprimary_research        -0.303 -0.189           0.139   0.113  -0.187   0.263 \nown_research             0.285  0.280  -0.299  -0.428  -0.147  -0.232         \nteach_in_team                   0.204   0.214  -0.323  -0.234   0.190   0.272 \nunderstand_principles   -0.497         -0.500  -0.190   0.298   0.148         \nuse_a_pointer                  -0.279   0.230  -0.116  -0.105  -0.116         \nwell_respected          -0.190                  0.276   0.224  -0.222  -0.251 \n                        Comp.16 Comp.17 Comp.18 Comp.19 Comp.20 Comp.21\naccessible                       0.488           0.116                 \nattractive              -0.327   0.107   0.195   0.451          -0.405 \nwide_range              -0.346                           0.342   0.308 \nteaching.based_research  0.305                   0.229  -0.237  -0.335 \nDress_nicely             0.331   0.124  -0.175  -0.383   0.111   0.150 \nfocus_on_complex_issues  0.275           0.198  -0.215  -0.197         \ngives_example                    0.207          -0.406          -0.338 \ngives_out_note           0.188  -0.322   0.108          -0.103   0.184 \nsense_of_humour                 -0.430   0.113                  -0.145 \nmark_easily                      0.270  -0.168          -0.198   0.277 \nmoves_around            -0.234   0.208  -0.102  -0.241   0.205   0.152 \noffers_lecture_notes                    -0.372   0.420  -0.202   0.289 \nclear_objective         -0.377           0.420  -0.218  -0.233         \nsolicit_questions                       -0.606                  -0.367 \ntext_book                                0.255           0.359         \nprimary_research                -0.130                   0.257  -0.123 \nown_research                             0.197          -0.248   0.206 \nteach_in_team                   -0.247                   0.198         \nunderstand_principles    0.176                   0.130   0.133   0.183 \nuse_a_pointer            0.434   0.352   0.114   0.198   0.346  -0.121 \nwell_respected                   0.231                  -0.370         \n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.048  0.048  0.048  0.048  0.048  0.048  0.048  0.048  0.048\nCumulative Var  0.048  0.095  0.143  0.190  0.238  0.286  0.333  0.381  0.429\n               Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 Comp.17\nSS loadings      1.000   1.000   1.000   1.000   1.000   1.000   1.000   1.000\nProportion Var   0.048   0.048   0.048   0.048   0.048   0.048   0.048   0.048\nCumulative Var   0.476   0.524   0.571   0.619   0.667   0.714   0.762   0.810\n               Comp.18 Comp.19 Comp.20 Comp.21\nSS loadings      1.000   1.000   1.000   1.000\nProportion Var   0.048   0.048   0.048   0.048\nCumulative Var   0.857   0.905   0.952   1.000\n\n\n\n\n\nTo do a screeplot, follow the commands below. Note this is the standard deviations, which are just square root of the variances or eigenvalues.\n\nscreeplot(pca1, type = \"lines\")\n\n\n\n\n\n\n\n\n\n\n\nTo get your principal components scores for plotting and analysis, do the following:\n\npca1$x\n\n               PC1         PC2        PC3        PC4         PC5          PC6\n [1,]  0.834364400 -0.44633674 -0.7389322  0.6505980  0.67162071 -0.059426515\n [2,] -0.057591233  2.43259604  0.3643921  0.1044273 -1.35364156 -1.513379283\n [3,] -1.414356172 -0.85486631  0.2591048 -0.7950895  0.39445964 -0.362282532\n [4,] -3.007651543 -1.54426710  1.8013306 -1.4495208 -0.06900951 -0.378437639\n [5,] -0.913357854  0.15572053  1.9852660  0.2647465  0.05455313 -0.190589811\n [6,]  2.715375811 -0.61458292  1.2581973  0.6291478  1.66664928  0.980101358\n [7,] -3.260274779 -1.46022078 -1.3182136 -1.5445289  2.59801832  1.373084699\n [8,]  1.690020920  4.10059174 -0.8286511 -2.7204201 -0.89839668  0.477869956\n [9,] -2.017591018 -0.19055562  1.2282190 -0.1591693 -1.81724286 -0.504760633\n[10,]  2.684027310 -0.28238921 -2.7053968 -0.6373616  1.24913045  0.114945320\n[11,]  1.602080176  1.73848677 -0.7560946 -0.1669273  1.46214681 -1.241067456\n[12,]  3.864282071  0.91902758 -3.0506154 -0.3116270  0.14774890 -1.707816527\n[13,] -1.537400621  0.79553155  0.2734490  2.2991348 -1.33509038  1.020459263\n[14,] -3.180580364 -1.55370059 -1.1043790  0.3994455 -0.21820422 -0.416321249\n[15,]  0.251424671 -0.87357501  0.8416185  1.3248859  1.62509470 -1.332946815\n[16,]  2.892895033 -0.54107588 -0.4987349  0.5314171 -0.40738282 -0.008629769\n[17,]  2.025139659  0.87994104  2.1794942  1.4021183 -1.10315491 -1.185333328\n[18,] -4.509205462  2.93089784 -2.9570995  3.4152682  0.56608967  1.421754123\n[19,] -0.363910804  0.59536235  1.0533136 -0.1849003 -0.12823425 -0.808435775\n[20,]  0.513833400  1.34820539  1.7227901 -1.3132714 -1.04480943  0.987729132\n[21,]  0.671318696 -1.57314535  0.3233421 -0.3775963  1.07805278  0.122220926\n[22,] -1.199789658 -0.75223096  0.2125650 -1.9753352 -0.38988416  2.577574944\n[23,]  2.699925571 -4.05301101 -2.2580650  0.1089181 -2.08232998  0.035139868\n[24,]  3.671607537 -2.76308860  2.3108314  1.9763565 -0.12222446  1.145347105\n[25,]  1.708338444  1.14870366  0.1941423 -0.8978228  0.53891069  1.218476886\n[26,]  0.697044049 -0.09475043 -0.9235260  0.5556583  1.51777743 -0.726583470\n[27,] -3.637428809 -1.19350763 -0.2178772 -1.3363292 -0.26449368 -3.193959083\n[28,]  0.919217152  1.59671229  1.9213986  0.1968770  0.58065254  0.264448784\n[29,] -0.012679393  2.32518659  0.2155462  0.3501560 -0.47396645  0.658332807\n[30,] -2.075112687 -0.04538414  0.2156446 -1.0632633  0.42197992  0.038464825\n[31,] -2.320188992 -0.57228262 -0.1349819  2.0161446 -0.36632609 -0.152398000\n[32,]  0.005054503 -1.40748778 -2.5517407 -0.5548639 -3.02867573  0.937512584\n[33,]  0.061169986 -0.15050467  1.6836625 -0.7372732  0.53018221  0.408905305\n              PC7         PC8         PC9          PC10        PC11        PC12\n [1,]  0.94163613  1.01611660  0.65086100  0.7049538165 -0.12679861  1.07464589\n [2,]  2.49608290 -1.03057653  0.99789650 -0.6955702786  0.62054036  0.43728658\n [3,] -1.14833452 -0.13690605  1.56172193 -1.0103974039 -0.22663448 -0.62688596\n [4,]  1.55864777 -2.37965647 -1.88594310  0.1842266066  0.56734997 -0.37865892\n [5,] -0.90420171 -1.46124279 -0.41077774  0.7877696717  0.87274235 -0.40364293\n [6,] -0.48508037 -0.39417790 -1.03586330 -0.5943650505 -0.20101933  0.39114013\n [7,]  0.92263413  0.27756052  0.64943390  0.6951735358  0.85851959 -0.18227075\n [8,]  1.01619589  1.42931518 -0.35783368 -0.0705759673  0.38608546 -0.45604663\n [9,] -3.06056251  0.01542140  0.97910235 -0.3689951357 -0.81042854  0.29905809\n[10,] -1.15019858 -2.22756355  1.92108719  1.1858684843  0.03051034 -0.44295070\n[11,] -1.38076963  0.10114808 -1.28404266  0.4344189806  0.24591922 -0.18193776\n[12,]  0.63745807 -0.33245072 -0.46272040 -0.4907654143 -0.49725530  0.06959804\n[13,]  0.50021492  1.64298989 -0.02204408 -0.0008218307 -0.32919047 -1.47757160\n[14,]  0.96910614 -0.33466161  1.43687692 -0.8126002209 -1.05968338 -0.42468108\n[15,]  0.80890447 -0.52662610 -0.88626822  0.4773237243 -0.83842767  0.73180898\n[16,]  0.99711436  1.01401737 -0.40202982  0.1817443327 -0.53794142 -0.60055239\n[17,]  0.20967322 -0.58311856  0.70515256 -0.0396632716  0.45009864  0.68839301\n[18,] -0.81537520 -0.85666393 -0.94273181  0.0414522567  0.07186325  0.63521158\n[19,] -0.39788697  0.90638787 -0.41749861  0.8035047424 -0.32339287 -0.65225431\n[20,] -1.71530837 -0.05810388 -0.44792612  0.6080155185  0.04950397  0.22979058\n[21,] -0.17473258 -0.49430919 -0.74983375 -2.3172182635 -0.86711224 -0.89907281\n[22,]  0.96335825  0.59588805 -0.69173585  0.7875438802 -1.58060139  0.69952356\n[23,] -0.79226369  0.31761740 -1.80001880 -0.5911334313  0.33709088  0.50027519\n[24,]  0.66242150  0.79456398  1.28229789  1.1369072252  0.36494912  0.29993320\n[25,]  0.11491096 -0.67752131  1.55591224 -0.9531517638 -0.43306504  0.96727700\n[26,] -0.97171844  2.00236186  0.03915553  0.0979107090  1.01294064 -0.86467646\n[27,] -0.10713776  1.13254478  0.06800519  1.3104870153 -0.78844151  0.38194366\n[28,] -0.11368363 -1.13303668 -0.44515516 -0.0309968966 -0.05286304 -0.17959227\n[29,]  0.03995439 -0.06103477 -0.17348145  0.3417751499 -1.22149166 -0.09463653\n[30,] -0.61338026  1.72693492 -0.07276041 -1.2660686396  1.38200547  1.95263797\n[31,]  0.79663725  0.35281020  0.08109498 -0.5131215937  0.67712222 -0.13873520\n[32,] -0.05972762 -1.30023954  0.11504776  0.5298008516  1.06058730 -0.29503060\n[33,]  0.25541147  0.66221148  0.44501905 -0.5534313394  0.90651818 -1.05932657\n             PC13        PC14        PC15         PC16         PC17\n [1,]  1.04550104  0.30220825 -0.26493487  0.990718773 -0.161522621\n [2,] -0.51140324  0.23962723  0.04514841 -0.864576510  0.021378514\n [3,] -0.45367844  0.47042709  0.44536737  0.117387548  1.132044168\n [4,]  0.76623701  0.33414241 -0.06674780  0.005890288 -0.317090528\n [5,] -0.22473438  1.02668595 -1.11036905  0.253771120  0.573823233\n [6,] -0.21758119  0.35417045  0.55539524  0.380784887  0.577014348\n [7,] -1.13189958 -0.40654870  0.30620976  0.183130027 -0.149934970\n [8,]  0.48772693 -0.20754494  0.30040817  0.432882057 -0.190155729\n [9,]  0.47865039  0.06044313  0.29758301  0.065987000 -0.412015158\n[10,] -0.03562393  0.19804842  0.33881205 -0.396898429 -0.531313384\n[11,] -0.90180692 -0.34546312 -0.75620771  0.625598888 -0.544476288\n[12,]  0.30509441  0.08813338 -0.58059025 -0.035026694  0.625823661\n[13,] -0.73594105  0.53970030 -0.79585891  0.555871566 -0.252855797\n[14,] -0.21555707 -0.91839176 -1.10063830 -0.346724418 -0.207470713\n[15,]  0.72072562 -0.64024769 -0.07151878 -0.333963902  0.392495606\n[16,]  0.13959352  0.68049473  0.84486635 -0.265106061 -0.372669045\n[17,] -1.02024905 -0.87052303  0.58413566  0.839988817  0.250265142\n[18,]  0.77126745 -0.26031481  0.33705965 -0.046675725  0.049282576\n[19,]  0.70815779  0.48976813  0.01791635 -0.847195425 -0.125274155\n[20,]  0.62144737 -0.75001136 -0.14219657 -0.508865779  0.007346965\n[21,]  0.41210426 -0.10629725  0.27929458  0.308950842 -0.697452881\n[22,] -0.59841689  0.14146264  0.02533706 -0.177525294  0.552971887\n[23,] -0.77530358 -0.21170311 -0.19235313 -0.271578336 -0.009956336\n[24,]  0.33558108 -0.22233178 -0.25908437 -0.300951777 -0.380190914\n[25,]  0.64085526  0.78994926 -0.57225157  0.361706690 -0.160414591\n[26,]  0.13507319  0.08748313  0.02580035 -0.785383761  0.456930349\n[27,] -0.10431199  0.13635919  0.47578097  0.572230799 -0.118992564\n[28,] -1.18521749 -0.05682740  0.35960572 -0.270080750 -0.621317543\n[29,] -0.75999067 -0.33173999  0.05271739 -0.414813487  0.312481307\n[30,] -0.34096948  0.30426194 -0.35124709 -0.539719364 -0.359452130\n[31,]  0.15560337  0.44934089  0.86956307  0.051061891 -0.041440723\n[32,]  0.35869730 -0.18428438  0.18487185  0.420292080  0.248554770\n[33,]  1.13036896 -1.18047718 -0.08187461  0.238832439  0.453583546\n              PC18         PC19          PC20         PC21\n [1,]  0.512534055  0.403637255 -0.2288274019 -0.180597545\n [2,]  0.929440714  0.048122004  0.2051146098 -0.271152883\n [3,]  0.403516656 -0.074179505 -0.2401958070 -0.046316428\n [4,] -0.380413260 -0.016801083  0.0236080284  0.075154120\n [5,] -0.121436088  0.216713273 -0.2532722027  0.163180806\n [6,]  0.016193332 -0.176022291  0.8096079044 -0.052570563\n [7,] -0.041349452  0.491356049  0.1502223084 -0.205615503\n [8,] -0.169826040 -0.182268894 -0.0271317045  0.331935144\n [9,] -0.170416032  0.311027931  0.4390800414 -0.032482679\n[10,] -0.228929235 -0.629795038 -0.1373141246  0.072909759\n[11,]  0.223418908 -0.399313078  0.1457129024 -0.346000339\n[12,] -0.361282411  0.136577651  0.1213875759  0.000801914\n[13,]  0.048394699 -0.297878346  0.0857017322 -0.076965204\n[14,] -0.556266739  0.183329956  0.1575056075  0.005626377\n[15,]  0.003248088 -0.275646771 -0.0488034493 -0.010278083\n[16,] -0.569143575  0.610124017  0.0001699299 -0.165370989\n[17,] -0.876273716  0.145493206 -0.3407035497 -0.133229363\n[18,]  0.139800530  0.266416571 -0.0612808299  0.138282533\n[19,] -0.288699113 -0.005113258  0.1267147465 -0.263464768\n[20,]  0.261595957  0.070258777 -0.3355360658 -0.432480871\n[21,]  0.478432361 -0.146679334 -0.3985086902 -0.071646635\n[22,] -0.090751620 -0.213675088 -0.1815364089 -0.142428632\n[23,]  0.266768773  0.195774478 -0.1770461524  0.191720088\n[24,]  0.354865033 -0.239914062  0.0937794098  0.244839284\n[25,] -0.161337294  0.208473109 -0.0508044281  0.105092550\n[26,] -0.040945263  0.377943641 -0.1672829144  0.091569499\n[27,]  0.419873948 -0.083337959  0.0482601803  0.312179849\n[28,]  0.322418197  0.609269862 -0.0166547989  0.337744117\n[29,]  0.091695107 -0.301130070 -0.0161094691  0.322694249\n[30,] -0.412731002 -0.414631089  0.0294813311  0.084790553\n[31,] -0.298987891 -0.721677547 -0.2130771535 -0.111634161\n[32,]  0.187128445 -0.059177362  0.2716590271 -0.102730154\n[33,]  0.109463929 -0.037277006  0.1860798156  0.166443959\n\n\nSo to combine them with the original variables, do this:\n\nlecturerpcscores &lt;- cbind(lecturers, pca1$x)\n\n\n\n\nWe can produce a biplot in ggplot2, using the factor gender as the colour:\n\nautoplot(pca1, data = lecturerpcscores, colour = 'gender')\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at &lt;https://github.com/sinhrks/ggfortify/issues&gt;.\n\n\n\n\n\n\n\n\n\nWe could add the loadings, but they are a bit messy, best to look at loadings or rotations tables.\nYou can make the plot using different symbols, change background etc through reading the ggplot2 documentation.\n\n\n\nYou can use this file to do the ANOVAs, you will have to do yourself. For example for differences in PC scores for gender:\n\nAOV1 &lt;- aov(lecturerpcscores$PC1 ~ gender, data = lecturerpcscores)\nsummary(AOV1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ngender       1    4.6   4.596    0.93  0.342\nResiduals   31  153.2   4.942               \n\n\nYou can do something similar for your projects and regression, using your PC scores as the predictors.\n\n\n\nTo do a boxplot of variables use ggplot2:\n\nggplot(lecturerpcscores, aes(x = gender, y = PC1)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nTo do a Varimax rotation on your principal components, follow these commands. Do a factor analysis with the rotation being Varimax:\n\nfa1 &lt;- factanal(lecturers[, 8:28], 10, rotation = \"varimax\")\nfa1\n\n\nCall:\nfactanal(x = lecturers[, 8:28], factors = 10, rotation = \"varimax\")\n\nUniquenesses:\n             accessible              attractive              wide_range \n                  0.381                   0.005                   0.005 \nteaching.based_research            Dress_nicely focus_on_complex_issues \n                  0.005                   0.283                   0.332 \n          gives_example          gives_out_note         sense_of_humour \n                  0.005                   0.448                   0.473 \n            mark_easily            moves_around    offers_lecture_notes \n                  0.005                   0.294                   0.141 \n        clear_objective       solicit_questions               text_book \n                  0.244                   0.005                   0.310 \n       primary_research            own_research           teach_in_team \n                  0.310                   0.237                   0.548 \n  understand_principles           use_a_pointer          well_respected \n                  0.584                   0.067                   0.005 \n\nLoadings:\n                        Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7\naccessible                      -0.583  -0.426   0.222  -0.110          -0.130 \nattractive                       0.961   0.106                           0.158 \nwide_range              -0.177                   0.952                         \nteaching.based_research                 -0.356   0.374  -0.114                 \nDress_nicely                     0.829   0.100                                 \nfocus_on_complex_issues  0.116                   0.674          -0.163  -0.141 \ngives_example                    0.122   0.112           0.955   0.163   0.132 \ngives_out_note           0.362   0.290   0.216  -0.196   0.305   0.125   0.261 \nsense_of_humour          0.127   0.229   0.599   0.125                   0.108 \nmark_easily              0.150   0.298   0.143                           0.910 \nmoves_around             0.326   0.244   0.186  -0.102  -0.155          -0.194 \noffers_lecture_notes     0.362   0.240   0.333           0.667          -0.232 \nclear_objective          0.768   0.109   0.343           0.169                 \nsolicit_questions        0.880  -0.139   0.130   0.109  -0.156           0.169 \ntext_book                0.611          -0.141  -0.152   0.166   0.202   0.427 \nprimary_research                                                               \nown_research             0.100                   0.129          -0.112  -0.102 \nteach_in_team            0.154          -0.103           0.155   0.614         \nunderstand_principles    0.608                                   0.155         \nuse_a_pointer            0.164   0.146   0.902  -0.142   0.165                 \nwell_respected           0.133           0.287                   0.931   0.101 \n                        Factor8 Factor9 Factor10\naccessible                                      \nattractive                      -0.121          \nwide_range               0.102   0.137  -0.126  \nteaching.based_research  0.158   0.819          \nDress_nicely                                    \nfocus_on_complex_issues                  0.369  \ngives_example                                   \ngives_out_note           0.244          -0.120  \nsense_of_humour                 -0.226          \nmark_easily             -0.113  -0.106          \nmoves_around             0.429   0.444   0.217  \noffers_lecture_notes    -0.159           0.167  \nclear_objective                                 \nsolicit_questions        0.326                  \ntext_book               -0.146                  \nprimary_research         0.213           0.790  \nown_research             0.784   0.119   0.255  \nteach_in_team                                   \nunderstand_principles            0.110          \nuse_a_pointer            0.109                  \nwell_respected                  -0.100          \n\n               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7 Factor8\nSS loadings      2.654   2.398   1.996   1.705   1.667   1.423   1.325   1.137\nProportion Var   0.126   0.114   0.095   0.081   0.079   0.068   0.063   0.054\nCumulative Var   0.126   0.241   0.336   0.417   0.496   0.564   0.627   0.681\n               Factor9 Factor10\nSS loadings      1.039    0.970\nProportion Var   0.049    0.046\nCumulative Var   0.731    0.777\n\nTest of the hypothesis that 10 factors are sufficient.\nThe chi square statistic is 28.56 on 45 degrees of freedom.\nThe p-value is 0.973"
  },
  {
    "objectID": "tutorials/tutorial10.html#setup",
    "href": "tutorials/tutorial10.html#setup",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "Load these packages to make the plots look better:\n\nlibrary(EFAtools)\nlibrary(ggplot2)\nlibrary(ggfortify)"
  },
  {
    "objectID": "tutorials/tutorial10.html#load-the-data",
    "href": "tutorials/tutorial10.html#load-the-data",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "First open your dataset:\n\nlecturers &lt;- read.csv(\"data/lecturers.csv\", header = TRUE)"
  },
  {
    "objectID": "tutorials/tutorial10.html#correlation-matrix",
    "href": "tutorials/tutorial10.html#correlation-matrix",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "Let’s first do a correlation matrix:\n\ncor(lecturers[, 8:28])\n\n                         accessible  attractive   wide_range\naccessible               1.00000000 -0.61876084  0.190062066\nattractive              -0.61876084  1.00000000 -0.011570169\nwide_range               0.19006207 -0.01157017  1.000000000\nteaching.based_research  0.24021638 -0.21382943  0.462870058\nDress_nicely            -0.53198037  0.81887024 -0.022848836\nfocus_on_complex_issues  0.28252341 -0.04447540  0.567443460\ngives_example           -0.23669223  0.23435837  0.065815857\ngives_out_note          -0.31173854  0.36819405 -0.187661524\nsense_of_humour         -0.29068273  0.33747896  0.081503385\nmark_easily             -0.34976811  0.44361281 -0.158545138\nmoves_around            -0.23371225  0.18475768 -0.087292839\noffers_lecture_notes    -0.25762452  0.28256570 -0.070931605\nclear_objective         -0.21819811  0.14978520 -0.176518448\nsolicit_questions        0.10349600 -0.07667948 -0.022123679\ntext_book                0.03170769  0.02849978 -0.241108100\nprimary_research         0.14147306 -0.06238516 -0.009628388\nown_research            -0.01089281  0.15175696  0.157013029\nteach_in_team            0.09004589 -0.13904104  0.005163136\nunderstand_principles   -0.08521319 -0.01080764 -0.097173242\nuse_a_pointer           -0.51717009  0.25526667 -0.131935123\nwell_respected          -0.12789662  0.04222022  0.037698097\n                        teaching.based_research Dress_nicely\naccessible                           0.24021638 -0.531980374\nattractive                          -0.21382943  0.818870242\nwide_range                           0.46287006 -0.022848836\nteaching.based_research              1.00000000 -0.079662581\nDress_nicely                        -0.07966258  1.000000000\nfocus_on_complex_issues              0.36612054 -0.074522662\ngives_example                       -0.23626944  0.207509283\ngives_out_note                      -0.13154529  0.349298764\nsense_of_humour                     -0.37115541  0.326981372\nmark_easily                         -0.26802282  0.293513890\nmoves_around                         0.35915941  0.175612992\noffers_lecture_notes                -0.26062206  0.304730151\nclear_objective                     -0.10137698  0.162176313\nsolicit_questions                    0.06997014 -0.047353486\ntext_book                           -0.02511112  0.065661794\nprimary_research                     0.08627931 -0.125765210\nown_research                         0.24450347  0.131114596\nteach_in_team                        0.05621459 -0.081891992\nunderstand_principles                0.13360929  0.003830791\nuse_a_pointer                       -0.40503932  0.246189390\nwell_respected                      -0.21574299  0.082434616\n                        focus_on_complex_issues gives_example gives_out_note\naccessible                           0.28252341   -0.23669223    -0.31173854\nattractive                          -0.04447540    0.23435837     0.36819405\nwide_range                           0.56744346    0.06581586    -0.18766152\nteaching.based_research              0.36612054   -0.23626944    -0.13154529\nDress_nicely                        -0.07452266    0.20750928     0.34929876\nfocus_on_complex_issues              1.00000000   -0.15606162    -0.31138949\ngives_example                       -0.15606162    1.00000000     0.41459959\ngives_out_note                      -0.31138949    0.41459959     1.00000000\nsense_of_humour                     -0.07005663    0.22759892     0.25028755\nmark_easily                         -0.18475242    0.18880641     0.39259669\nmoves_around                         0.06116819   -0.15496603     0.26746010\noffers_lecture_notes                 0.04438636    0.67139954     0.32650197\nclear_objective                     -0.05740760    0.22152350     0.49329825\nsolicit_questions                    0.12547982   -0.10034117     0.37306647\ntext_book                           -0.14904151    0.23189356     0.41705241\nprimary_research                     0.33824172   -0.05502391    -0.00258233\nown_research                         0.21007669   -0.03858742     0.11486585\nteach_in_team                       -0.05016868    0.22073735     0.07101795\nunderstand_principles                0.12582565    0.06367067     0.09564742\nuse_a_pointer                       -0.19722690    0.29421612     0.42798936\nwell_respected                      -0.15321258    0.20022401     0.24197769\n                        sense_of_humour  mark_easily moves_around\naccessible                  -0.29068273 -0.349768108 -0.233712249\nattractive                   0.33747896  0.443612809  0.184757685\nwide_range                   0.08150339 -0.158545138 -0.087292839\nteaching.based_research     -0.37115541 -0.268022817  0.359159406\nDress_nicely                 0.32698137  0.293513890  0.175612992\nfocus_on_complex_issues     -0.07005663 -0.184752419  0.061168190\ngives_example                0.22759892  0.188806413 -0.154966026\ngives_out_note               0.25028755  0.392596691  0.267460102\nsense_of_humour              1.00000000  0.279195853  0.147981262\nmark_easily                  0.27919585  1.000000000 -0.102624026\nmoves_around                 0.14798126 -0.102624026  1.000000000\noffers_lecture_notes         0.40350521  0.004540766  0.116586008\nclear_objective              0.30494267  0.241714172  0.328029643\nsolicit_questions            0.22196924  0.221507736  0.357039548\ntext_book                    0.11515826  0.493885950  0.018944976\nprimary_research            -0.11402253  0.013988221  0.201361759\nown_research                 0.19400676 -0.136905883  0.521788320\nteach_in_team               -0.13889755 -0.029214646 -0.002012914\nunderstand_principles        0.08973153  0.029511600  0.281044416\nuse_a_pointer                0.60147995  0.200483903  0.247735287\nwell_respected               0.31777004  0.204840208  0.107275533\n                        offers_lecture_notes clear_objective solicit_questions\naccessible                      -0.257624516     -0.21819811       0.103495996\nattractive                       0.282565698      0.14978520      -0.076679483\nwide_range                      -0.070931605     -0.17651845      -0.022123679\nteaching.based_research         -0.260622062     -0.10137698       0.069970142\nDress_nicely                     0.304730151      0.16217631      -0.047353486\nfocus_on_complex_issues          0.044386363     -0.05740760       0.125479819\ngives_example                    0.671399539      0.22152350      -0.100341166\ngives_out_note                   0.326501972      0.49329825       0.373066471\nsense_of_humour                  0.403505211      0.30494267       0.221969236\nmark_easily                      0.004540766      0.24171417       0.221507736\nmoves_around                     0.116586008      0.32802964       0.357039548\noffers_lecture_notes             1.000000000      0.51135418       0.134906607\nclear_objective                  0.511354179      1.00000000       0.667857876\nsolicit_questions                0.134906607      0.66785788       1.000000000\ntext_book                        0.241763186      0.50985104       0.514856144\nprimary_research                 0.027238866     -0.05933422       0.008953955\nown_research                     0.014724611      0.04037952       0.313649567\nteach_in_team                    0.134589361      0.09431599       0.129502715\nunderstand_principles            0.204933977      0.52773521       0.525180942\nuse_a_pointer                    0.475656687      0.48740938       0.245720968\nwell_respected                   0.173541917      0.20806935       0.217669413\n                           text_book primary_research own_research\naccessible               0.031707686      0.141473062  -0.01089281\nattractive               0.028499784     -0.062385161   0.15175696\nwide_range              -0.241108100     -0.009628388   0.15701303\nteaching.based_research -0.025111119      0.086279306   0.24450347\nDress_nicely             0.065661794     -0.125765210   0.13111460\nfocus_on_complex_issues -0.149041509      0.338241715   0.21007669\ngives_example            0.231893556     -0.055023907  -0.03858742\ngives_out_note           0.417052411     -0.002582330   0.11486585\nsense_of_humour          0.115158256     -0.114022533   0.19400676\nmark_easily              0.493885950      0.013988221  -0.13690588\nmoves_around             0.018944976      0.201361759   0.52178832\noffers_lecture_notes     0.241763186      0.027238866   0.01472461\nclear_objective          0.509851042     -0.059334220   0.04037952\nsolicit_questions        0.514856144      0.008953955   0.31364957\ntext_book                1.000000000     -0.092231864  -0.11384268\nprimary_research        -0.092231864      1.000000000   0.35299282\nown_research            -0.113842676      0.352992824   1.00000000\nteach_in_team            0.176402014     -0.085933891  -0.08668898\nunderstand_principles    0.287773155      0.017218953   0.06370238\nuse_a_pointer            0.001337384     -0.149851010   0.15399945\nwell_respected           0.274222528      0.016210854  -0.10140227\n                        teach_in_team understand_principles use_a_pointer\naccessible                0.090045886          -0.085213191  -0.517170092\nattractive               -0.139041035          -0.010807636   0.255266673\nwide_range                0.005163136          -0.097173242  -0.131935123\nteaching.based_research   0.056214590           0.133609295  -0.405039321\nDress_nicely             -0.081891992           0.003830791   0.246189390\nfocus_on_complex_issues  -0.050168682           0.125825652  -0.197226905\ngives_example             0.220737347           0.063670673   0.294216117\ngives_out_note            0.071017950           0.095647419   0.427989355\nsense_of_humour          -0.138897546           0.089731525   0.601479950\nmark_easily              -0.029214646           0.029511600   0.200483903\nmoves_around             -0.002012914           0.281044416   0.247735287\noffers_lecture_notes      0.134589361           0.204933977   0.475656687\nclear_objective           0.094315990           0.527735214   0.487409381\nsolicit_questions         0.129502715           0.525180942   0.245720968\ntext_book                 0.176402014           0.287773155   0.001337384\nprimary_research         -0.085933891           0.017218953  -0.149851010\nown_research             -0.086688981           0.063702380   0.153999453\nteach_in_team             1.000000000           0.101857142  -0.016777680\nunderstand_principles     0.101857142           1.000000000   0.104631048\nuse_a_pointer            -0.016777680           0.104631048   1.000000000\nwell_respected            0.545915684           0.207782811   0.318541502\n                        well_respected\naccessible                 -0.12789662\nattractive                  0.04222022\nwide_range                  0.03769810\nteaching.based_research    -0.21574299\nDress_nicely                0.08243462\nfocus_on_complex_issues    -0.15321258\ngives_example               0.20022401\ngives_out_note              0.24197769\nsense_of_humour             0.31777004\nmark_easily                 0.20484021\nmoves_around                0.10727553\noffers_lecture_notes        0.17354192\nclear_objective             0.20806935\nsolicit_questions           0.21766941\ntext_book                   0.27422253\nprimary_research            0.01621085\nown_research               -0.10140227\nteach_in_team               0.54591568\nunderstand_principles       0.20778281\nuse_a_pointer               0.31854150\nwell_respected              1.00000000\n\nCorrmatrix &lt;- cor(lecturers[, 8:28])"
  },
  {
    "objectID": "tutorials/tutorial10.html#bartletts-test-of-sphericity",
    "href": "tutorials/tutorial10.html#bartletts-test-of-sphericity",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "Now we can do the Bartlett’s Test of Sphericity. This test compares the correlation matrix to an identity matrix. If it is significant, it is worth doing a PCA.\n\nBARTLETT(Corrmatrix, N = 34, cor_method = c(\"pearson\"))\n\n\n✔ The Bartlett's test of sphericity was significant at an alpha level of .05.\n  These data are probably suitable for factor analysis.\n\n  𝜒²(210) = 355.07, p &lt; .001"
  },
  {
    "objectID": "tutorials/tutorial10.html#principal-components-analysis",
    "href": "tutorials/tutorial10.html#principal-components-analysis",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "Note: to make this a PCA based on a correlation matrix, we have to scale the variables, hence scale = TRUE. There are two main principal components functions, but they are very similar. Note prcomp calls the loadings “rotations”, not to be confused with rotations below.\n\npca1 &lt;- prcomp(lecturers[, 8:28], scale = TRUE)\npca2 &lt;- princomp(lecturers[, 8:28], cor = TRUE)\n\n\nsummary(pca1)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.2206 1.6616 1.5324 1.29807 1.19561 1.12098 1.08311\nProportion of Variance 0.2348 0.1315 0.1118 0.08024 0.06807 0.05984 0.05586\nCumulative Proportion  0.2348 0.3663 0.4781 0.55833 0.62641 0.68624 0.74211\n                          PC8     PC9    PC10    PC11   PC12    PC13    PC14\nStandard deviation     1.0737 0.95681 0.79545 0.73119 0.7055 0.65580 0.50887\nProportion of Variance 0.0549 0.04359 0.03013 0.02546 0.0237 0.02048 0.01233\nCumulative Proportion  0.7970 0.84060 0.87073 0.89619 0.9199 0.94037 0.95270\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.49817 0.47309 0.42640 0.36970 0.32493 0.24020 0.19872\nProportion of Variance 0.01182 0.01066 0.00866 0.00651 0.00503 0.00275 0.00188\nCumulative Proportion  0.96452 0.97518 0.98384 0.99034 0.99537 0.99812 1.00000\n\nsummary(pca2)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5\nStandard deviation     2.2205654 1.6615887 1.5324030 1.29806969 1.19560985\nProportion of Variance 0.2348053 0.1314703 0.1118219 0.08023738 0.06807062\nCumulative Proportion  0.2348053 0.3662756 0.4780975 0.55833484 0.62640546\n                           Comp.6     Comp.7     Comp.8     Comp.9    Comp.10\nStandard deviation     1.12097809 1.08311410 1.07371597 0.95680927 0.79544930\nProportion of Variance 0.05983771 0.05586363 0.05489838 0.04359448 0.03013046\nCumulative Proportion  0.68624317 0.74210679 0.79700517 0.84059965 0.87073010\n                          Comp.11    Comp.12    Comp.13    Comp.14    Comp.15\nStandard deviation     0.73119377 0.70551609 0.65579857 0.50886890 0.49817027\nProportion of Variance 0.02545925 0.02370252 0.02047961 0.01233084 0.01181779\nCumulative Proportion  0.89618936 0.91989188 0.94037149 0.95270232 0.96452011\n                         Comp.16     Comp.17     Comp.18     Comp.19\nStandard deviation     0.4730918 0.426395520 0.369702631 0.324933440\nProportion of Variance 0.0106579 0.008657769 0.006508573 0.005027702\nCumulative Proportion  0.9751780 0.983835781 0.990344355 0.995372057\n                           Comp.20     Comp.21\nStandard deviation     0.240198751 0.198724365\nProportion of Variance 0.002747402 0.001880542\nCumulative Proportion  0.998119458 1.000000000"
  },
  {
    "objectID": "tutorials/tutorial10.html#loadings",
    "href": "tutorials/tutorial10.html#loadings",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "Let’s look at the loadings. Called “rotations” in prcomp and “loadings” in princomp. They are the Pearson’s correlation between that variable and that Principal Component.\n\npca1\n\nStandard deviations (1, .., p=21):\n [1] 2.2205654 1.6615887 1.5324030 1.2980697 1.1956099 1.1209781 1.0831141\n [8] 1.0737160 0.9568093 0.7954493 0.7311938 0.7055161 0.6557986 0.5088689\n[15] 0.4981703 0.4730918 0.4263955 0.3697026 0.3249334 0.2401988 0.1987244\n\nRotation (n x k) = (21 x 21):\n                                PC1         PC2         PC3          PC4\naccessible              -0.27105784  0.21481141  0.22284903 -0.109380126\nattractive               0.25953819 -0.19001404 -0.35322829  0.047967149\nwide_range              -0.11796047  0.14523503 -0.23536689 -0.465316942\nteaching.based_research -0.17807186  0.33523672 -0.16973235 -0.005540905\nDress_nicely             0.24737662 -0.15550598 -0.32565089  0.019172145\nfocus_on_complex_issues -0.13467331  0.28837561 -0.24412817 -0.297540766\ngives_example            0.22797978 -0.11739164  0.06984178 -0.437201342\ngives_out_note           0.32056620  0.04438472  0.02749136  0.100499687\nsense_of_humour          0.27564472 -0.03120757 -0.13957405 -0.145704311\nmark_easily              0.23686285 -0.11243202  0.08113616  0.196601185\nmoves_around             0.13236700  0.32827473 -0.27087444  0.213497862\noffers_lecture_notes     0.28307512  0.02357872 -0.01627849 -0.372830743\nclear_objective          0.31573398  0.26378859  0.12826236  0.066194937\nsolicit_questions        0.18991164  0.43659484  0.13380240  0.160366349\ntext_book                0.21299001  0.16712512  0.33485831  0.124899445\nprimary_research        -0.05608411  0.19206706 -0.16387602  0.001745603\nown_research             0.04318371  0.28387549 -0.37836892  0.056442158\nteach_in_team            0.04818137  0.11021896  0.29523823 -0.327405642\nunderstand_principles    0.14556568  0.33936721  0.10777339  0.053017840\nuse_a_pointer            0.32019196 -0.03811643 -0.08101559 -0.080227102\nwell_respected           0.19384039  0.06977679  0.22029921 -0.263146206\n                                 PC5         PC6          PC7         PC8\naccessible              -0.006862732  0.20175545  0.075192964  0.13471983\nattractive              -0.304217305 -0.05278985 -0.006014137 -0.01346850\nwide_range              -0.251311171  0.08634713 -0.331523146  0.07096692\nteaching.based_research -0.307853355 -0.22290900 -0.040540304 -0.27328747\nDress_nicely            -0.299180706 -0.12573075 -0.046516486 -0.12380132\nfocus_on_complex_issues -0.170654964  0.29340602 -0.050872762  0.07474147\ngives_example           -0.075647764  0.06855598  0.364935010 -0.17131172\ngives_out_note          -0.125013318 -0.08366732  0.223406707  0.02145305\nsense_of_humour          0.216056466  0.21464296 -0.328598912  0.27397871\nmark_easily             -0.421810061  0.15983061 -0.046406386  0.38018642\nmoves_around             0.166526391 -0.35024271  0.055092321 -0.07707096\noffers_lecture_notes     0.146235397  0.18757528  0.287000528 -0.27400475\nclear_objective          0.060342195  0.17777324 -0.032682046 -0.21837583\nsolicit_questions       -0.005900941  0.14588578 -0.182965113  0.09074671\ntext_book               -0.356204508  0.15361788  0.084096939  0.02338965\nprimary_research         0.039054189  0.06897414  0.592731543  0.45913721\nown_research             0.156844077 -0.09496398  0.160942737  0.20826388\nteach_in_team           -0.096207102 -0.54758870  0.037625674  0.05473642\nunderstand_principles    0.010243773  0.05372028 -0.114475017 -0.27603888\nuse_a_pointer            0.411769112  0.01616452 -0.180288938  0.05917537\nwell_respected           0.013159486 -0.40377696 -0.183055505  0.39655405\n                                PC9        PC10        PC11         PC12\naccessible               0.23711499  0.41979852  0.03945146  0.148881566\nattractive              -0.16438427  0.20365082  0.06904042 -0.107432225\nwide_range               0.22155409 -0.26438749 -0.03259079 -0.018048262\nteaching.based_research  0.19889412 -0.24510749 -0.11135378  0.247899071\nDress_nicely            -0.10406494  0.38639604  0.08377430  0.098387712\nfocus_on_complex_issues -0.28979913  0.00432150  0.36226348 -0.066562002\ngives_example            0.19510676 -0.15342441 -0.35973985 -0.159175284\ngives_out_note           0.41895610 -0.21289394  0.20038838  0.006019332\nsense_of_humour          0.09730489  0.19787809 -0.29410062  0.309619454\nmark_easily             -0.04288340 -0.25885827 -0.05713892 -0.143735223\nmoves_around             0.02114182 -0.04788447  0.02682634  0.353171999\noffers_lecture_notes    -0.07931457  0.16874959  0.11255136  0.207814735\nclear_objective         -0.04989256 -0.08180409  0.29595319  0.009127372\nsolicit_questions        0.10754501  0.13028684  0.14132328 -0.276862219\ntext_book                0.12628767  0.17521009 -0.12180066  0.267854799\nprimary_research        -0.30348754 -0.18921946  0.01954047  0.138689169\nown_research             0.28543090  0.27973695 -0.29907250 -0.427500246\nteach_in_team           -0.07446630  0.20427502  0.21413563 -0.322896702\nunderstand_principles   -0.49690454 -0.08895650 -0.50033599 -0.189676003\nuse_a_pointer            0.09757553 -0.27912944  0.22992547 -0.115886008\nwell_respected          -0.19000184 -0.03498965 -0.08926599  0.276488059\n                               PC13        PC14         PC15         PC16\naccessible               0.29231904  0.29580819 -0.222218786  0.095682695\nattractive               0.10512874  0.20004636  0.069413634 -0.326657310\nwide_range               0.14339377 -0.15578216 -0.042633744 -0.346118318\nteaching.based_research -0.16129758 -0.08569727 -0.291561886  0.304862272\nDress_nicely             0.22024728 -0.29025496 -0.196064822  0.331319175\nfocus_on_complex_issues -0.11651678  0.21121417  0.380088085  0.275036237\ngives_example           -0.02596566  0.13003779  0.073844050 -0.048615414\ngives_out_note           0.52366782  0.22815294  0.150396690  0.187752138\nsense_of_humour         -0.09102492  0.24787583 -0.269328590  0.077361879\nmark_easily             -0.30618081  0.26528469 -0.239118638  0.067806707\nmoves_around            -0.19413727  0.41519824  0.163481854 -0.233951595\noffers_lecture_notes    -0.18626080 -0.01943147 -0.008128194 -0.028997162\nclear_objective          0.02299500 -0.21161788 -0.392005665 -0.377275931\nsolicit_questions        0.07959209 -0.06601352  0.067535709 -0.098815936\ntext_book               -0.31596633 -0.28217199  0.346092878  0.063696048\nprimary_research         0.11269387 -0.18656539 -0.263215477 -0.012502540\nown_research            -0.14671997 -0.23213427  0.082887276  0.001508984\nteach_in_team           -0.23420220  0.19002827 -0.271548974  0.056217287\nunderstand_principles    0.29771815  0.14826763 -0.020146024  0.175848894\nuse_a_pointer           -0.10543967 -0.11603812 -0.008762183  0.433779829\nwell_respected           0.22357927 -0.22236358  0.251201419 -0.060389007\n                                PC17         PC18        PC19        PC20\naccessible              -0.487523780 -0.092943639  0.11623660 -0.08184741\nattractive              -0.106937105 -0.195239242  0.45086924 -0.09762660\nwide_range               0.009616636  0.068156323  0.05963088 -0.34244915\nteaching.based_research  0.006376281  0.018683190  0.22919635  0.23688191\nDress_nicely            -0.124236446  0.174946567 -0.38290347 -0.11111538\nfocus_on_complex_issues  0.057689702 -0.197551507 -0.21480326  0.19658872\ngives_example           -0.206970055  0.030539850 -0.40647428 -0.01600380\ngives_out_note           0.321857496 -0.107588372  0.08121002  0.10250939\nsense_of_humour          0.430141107 -0.113398698 -0.08853507  0.02641527\nmark_easily             -0.269651092  0.167631153  0.01976224  0.19779113\nmoves_around            -0.207902119  0.101506396 -0.24054414 -0.20539739\noffers_lecture_notes     0.024256519  0.371837654  0.41986631  0.20173823\nclear_objective         -0.097333800 -0.419634434 -0.21848656  0.23314508\nsolicit_questions        0.094290088  0.606166863 -0.05758082 -0.04550053\ntext_book                0.072364215 -0.254638772  0.07071928 -0.35855399\nprimary_research         0.130404141  0.041767487  0.01625913 -0.25697656\nown_research            -0.068753059 -0.196939862  0.08026083  0.24801010\nteach_in_team            0.247146896 -0.095095343  0.00645715 -0.19776650\nunderstand_principles   -0.001605969 -0.075193184  0.12987739 -0.13337875\nuse_a_pointer           -0.352414119 -0.113600108  0.19774694 -0.34608373\nwell_respected          -0.231089402  0.008802903  0.03736378  0.36974689\n                                PC21\naccessible              -0.002932735\nattractive               0.405148279\nwide_range              -0.308435374\nteaching.based_research  0.335322467\nDress_nicely            -0.150318850\nfocus_on_complex_issues  0.045604055\ngives_example            0.338040434\ngives_out_note          -0.184254445\nsense_of_humour          0.144536843\nmark_easily             -0.277239849\nmoves_around            -0.151918843\noffers_lecture_notes    -0.289119982\nclear_objective         -0.008051690\nsolicit_questions        0.367111373\ntext_book               -0.026709294\nprimary_research         0.123144467\nown_research            -0.206478451\nteach_in_team           -0.055864220\nunderstand_principles   -0.183080778\nuse_a_pointer            0.121125177\nwell_respected           0.079347988\n\nloadings(pca2)\n\n\nLoadings:\n                        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8\naccessible               0.271  0.215  0.223  0.109         0.202         0.135\nattractive              -0.260 -0.190 -0.353         0.304                     \nwide_range               0.118  0.145 -0.235  0.465  0.251        -0.332       \nteaching.based_research  0.178  0.335 -0.170         0.308 -0.223        -0.273\nDress_nicely            -0.247 -0.156 -0.326         0.299 -0.126        -0.124\nfocus_on_complex_issues  0.135  0.288 -0.244  0.298  0.171  0.293              \ngives_example           -0.228 -0.117         0.437                0.365 -0.171\ngives_out_note          -0.321               -0.100  0.125         0.223       \nsense_of_humour         -0.276        -0.140  0.146 -0.216  0.215 -0.329  0.274\nmark_easily             -0.237 -0.112        -0.197  0.422  0.160         0.380\nmoves_around            -0.132  0.328 -0.271 -0.213 -0.167 -0.350              \noffers_lecture_notes    -0.283                0.373 -0.146  0.188  0.287 -0.274\nclear_objective         -0.316  0.264  0.128                0.178        -0.218\nsolicit_questions       -0.190  0.437  0.134 -0.160         0.146 -0.183       \ntext_book               -0.213  0.167  0.335 -0.125  0.356  0.154              \nprimary_research                0.192 -0.164                       0.593  0.459\nown_research                    0.284 -0.378        -0.157         0.161  0.208\nteach_in_team                   0.110  0.295  0.327        -0.548              \nunderstand_principles   -0.146  0.339  0.108                      -0.114 -0.276\nuse_a_pointer           -0.320                      -0.412        -0.180       \nwell_respected          -0.194         0.220  0.263        -0.404 -0.183  0.397\n                        Comp.9 Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15\naccessible               0.237  0.420           0.149   0.292   0.296   0.222 \nattractive              -0.164  0.204          -0.107   0.105   0.200         \nwide_range               0.222 -0.264                   0.143  -0.156         \nteaching.based_research  0.199 -0.245  -0.111   0.248  -0.161           0.292 \nDress_nicely            -0.104  0.386                   0.220  -0.290   0.196 \nfocus_on_complex_issues -0.290          0.362          -0.117   0.211  -0.380 \ngives_example            0.195 -0.153  -0.360  -0.159           0.130         \ngives_out_note           0.419 -0.213   0.200           0.524   0.228  -0.150 \nsense_of_humour                 0.198  -0.294   0.310           0.248   0.269 \nmark_easily                    -0.259          -0.144  -0.306   0.265   0.239 \nmoves_around                                    0.353  -0.194   0.415  -0.163 \noffers_lecture_notes            0.169   0.113   0.208  -0.186                 \nclear_objective                         0.296                  -0.212   0.392 \nsolicit_questions        0.108  0.130   0.141  -0.277                         \ntext_book                0.126  0.175  -0.122   0.268  -0.316  -0.282  -0.346 \nprimary_research        -0.303 -0.189           0.139   0.113  -0.187   0.263 \nown_research             0.285  0.280  -0.299  -0.428  -0.147  -0.232         \nteach_in_team                   0.204   0.214  -0.323  -0.234   0.190   0.272 \nunderstand_principles   -0.497         -0.500  -0.190   0.298   0.148         \nuse_a_pointer                  -0.279   0.230  -0.116  -0.105  -0.116         \nwell_respected          -0.190                  0.276   0.224  -0.222  -0.251 \n                        Comp.16 Comp.17 Comp.18 Comp.19 Comp.20 Comp.21\naccessible                       0.488           0.116                 \nattractive              -0.327   0.107   0.195   0.451          -0.405 \nwide_range              -0.346                           0.342   0.308 \nteaching.based_research  0.305                   0.229  -0.237  -0.335 \nDress_nicely             0.331   0.124  -0.175  -0.383   0.111   0.150 \nfocus_on_complex_issues  0.275           0.198  -0.215  -0.197         \ngives_example                    0.207          -0.406          -0.338 \ngives_out_note           0.188  -0.322   0.108          -0.103   0.184 \nsense_of_humour                 -0.430   0.113                  -0.145 \nmark_easily                      0.270  -0.168          -0.198   0.277 \nmoves_around            -0.234   0.208  -0.102  -0.241   0.205   0.152 \noffers_lecture_notes                    -0.372   0.420  -0.202   0.289 \nclear_objective         -0.377           0.420  -0.218  -0.233         \nsolicit_questions                       -0.606                  -0.367 \ntext_book                                0.255           0.359         \nprimary_research                -0.130                   0.257  -0.123 \nown_research                             0.197          -0.248   0.206 \nteach_in_team                   -0.247                   0.198         \nunderstand_principles    0.176                   0.130   0.133   0.183 \nuse_a_pointer            0.434   0.352   0.114   0.198   0.346  -0.121 \nwell_respected                   0.231                  -0.370         \n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.048  0.048  0.048  0.048  0.048  0.048  0.048  0.048  0.048\nCumulative Var  0.048  0.095  0.143  0.190  0.238  0.286  0.333  0.381  0.429\n               Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 Comp.17\nSS loadings      1.000   1.000   1.000   1.000   1.000   1.000   1.000   1.000\nProportion Var   0.048   0.048   0.048   0.048   0.048   0.048   0.048   0.048\nCumulative Var   0.476   0.524   0.571   0.619   0.667   0.714   0.762   0.810\n               Comp.18 Comp.19 Comp.20 Comp.21\nSS loadings      1.000   1.000   1.000   1.000\nProportion Var   0.048   0.048   0.048   0.048\nCumulative Var   0.857   0.905   0.952   1.000"
  },
  {
    "objectID": "tutorials/tutorial10.html#screeplot",
    "href": "tutorials/tutorial10.html#screeplot",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "To do a screeplot, follow the commands below. Note this is the standard deviations, which are just square root of the variances or eigenvalues.\n\nscreeplot(pca1, type = \"lines\")"
  },
  {
    "objectID": "tutorials/tutorial10.html#principal-component-scores",
    "href": "tutorials/tutorial10.html#principal-component-scores",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "To get your principal components scores for plotting and analysis, do the following:\n\npca1$x\n\n               PC1         PC2        PC3        PC4         PC5          PC6\n [1,]  0.834364400 -0.44633674 -0.7389322  0.6505980  0.67162071 -0.059426515\n [2,] -0.057591233  2.43259604  0.3643921  0.1044273 -1.35364156 -1.513379283\n [3,] -1.414356172 -0.85486631  0.2591048 -0.7950895  0.39445964 -0.362282532\n [4,] -3.007651543 -1.54426710  1.8013306 -1.4495208 -0.06900951 -0.378437639\n [5,] -0.913357854  0.15572053  1.9852660  0.2647465  0.05455313 -0.190589811\n [6,]  2.715375811 -0.61458292  1.2581973  0.6291478  1.66664928  0.980101358\n [7,] -3.260274779 -1.46022078 -1.3182136 -1.5445289  2.59801832  1.373084699\n [8,]  1.690020920  4.10059174 -0.8286511 -2.7204201 -0.89839668  0.477869956\n [9,] -2.017591018 -0.19055562  1.2282190 -0.1591693 -1.81724286 -0.504760633\n[10,]  2.684027310 -0.28238921 -2.7053968 -0.6373616  1.24913045  0.114945320\n[11,]  1.602080176  1.73848677 -0.7560946 -0.1669273  1.46214681 -1.241067456\n[12,]  3.864282071  0.91902758 -3.0506154 -0.3116270  0.14774890 -1.707816527\n[13,] -1.537400621  0.79553155  0.2734490  2.2991348 -1.33509038  1.020459263\n[14,] -3.180580364 -1.55370059 -1.1043790  0.3994455 -0.21820422 -0.416321249\n[15,]  0.251424671 -0.87357501  0.8416185  1.3248859  1.62509470 -1.332946815\n[16,]  2.892895033 -0.54107588 -0.4987349  0.5314171 -0.40738282 -0.008629769\n[17,]  2.025139659  0.87994104  2.1794942  1.4021183 -1.10315491 -1.185333328\n[18,] -4.509205462  2.93089784 -2.9570995  3.4152682  0.56608967  1.421754123\n[19,] -0.363910804  0.59536235  1.0533136 -0.1849003 -0.12823425 -0.808435775\n[20,]  0.513833400  1.34820539  1.7227901 -1.3132714 -1.04480943  0.987729132\n[21,]  0.671318696 -1.57314535  0.3233421 -0.3775963  1.07805278  0.122220926\n[22,] -1.199789658 -0.75223096  0.2125650 -1.9753352 -0.38988416  2.577574944\n[23,]  2.699925571 -4.05301101 -2.2580650  0.1089181 -2.08232998  0.035139868\n[24,]  3.671607537 -2.76308860  2.3108314  1.9763565 -0.12222446  1.145347105\n[25,]  1.708338444  1.14870366  0.1941423 -0.8978228  0.53891069  1.218476886\n[26,]  0.697044049 -0.09475043 -0.9235260  0.5556583  1.51777743 -0.726583470\n[27,] -3.637428809 -1.19350763 -0.2178772 -1.3363292 -0.26449368 -3.193959083\n[28,]  0.919217152  1.59671229  1.9213986  0.1968770  0.58065254  0.264448784\n[29,] -0.012679393  2.32518659  0.2155462  0.3501560 -0.47396645  0.658332807\n[30,] -2.075112687 -0.04538414  0.2156446 -1.0632633  0.42197992  0.038464825\n[31,] -2.320188992 -0.57228262 -0.1349819  2.0161446 -0.36632609 -0.152398000\n[32,]  0.005054503 -1.40748778 -2.5517407 -0.5548639 -3.02867573  0.937512584\n[33,]  0.061169986 -0.15050467  1.6836625 -0.7372732  0.53018221  0.408905305\n              PC7         PC8         PC9          PC10        PC11        PC12\n [1,]  0.94163613  1.01611660  0.65086100  0.7049538165 -0.12679861  1.07464589\n [2,]  2.49608290 -1.03057653  0.99789650 -0.6955702786  0.62054036  0.43728658\n [3,] -1.14833452 -0.13690605  1.56172193 -1.0103974039 -0.22663448 -0.62688596\n [4,]  1.55864777 -2.37965647 -1.88594310  0.1842266066  0.56734997 -0.37865892\n [5,] -0.90420171 -1.46124279 -0.41077774  0.7877696717  0.87274235 -0.40364293\n [6,] -0.48508037 -0.39417790 -1.03586330 -0.5943650505 -0.20101933  0.39114013\n [7,]  0.92263413  0.27756052  0.64943390  0.6951735358  0.85851959 -0.18227075\n [8,]  1.01619589  1.42931518 -0.35783368 -0.0705759673  0.38608546 -0.45604663\n [9,] -3.06056251  0.01542140  0.97910235 -0.3689951357 -0.81042854  0.29905809\n[10,] -1.15019858 -2.22756355  1.92108719  1.1858684843  0.03051034 -0.44295070\n[11,] -1.38076963  0.10114808 -1.28404266  0.4344189806  0.24591922 -0.18193776\n[12,]  0.63745807 -0.33245072 -0.46272040 -0.4907654143 -0.49725530  0.06959804\n[13,]  0.50021492  1.64298989 -0.02204408 -0.0008218307 -0.32919047 -1.47757160\n[14,]  0.96910614 -0.33466161  1.43687692 -0.8126002209 -1.05968338 -0.42468108\n[15,]  0.80890447 -0.52662610 -0.88626822  0.4773237243 -0.83842767  0.73180898\n[16,]  0.99711436  1.01401737 -0.40202982  0.1817443327 -0.53794142 -0.60055239\n[17,]  0.20967322 -0.58311856  0.70515256 -0.0396632716  0.45009864  0.68839301\n[18,] -0.81537520 -0.85666393 -0.94273181  0.0414522567  0.07186325  0.63521158\n[19,] -0.39788697  0.90638787 -0.41749861  0.8035047424 -0.32339287 -0.65225431\n[20,] -1.71530837 -0.05810388 -0.44792612  0.6080155185  0.04950397  0.22979058\n[21,] -0.17473258 -0.49430919 -0.74983375 -2.3172182635 -0.86711224 -0.89907281\n[22,]  0.96335825  0.59588805 -0.69173585  0.7875438802 -1.58060139  0.69952356\n[23,] -0.79226369  0.31761740 -1.80001880 -0.5911334313  0.33709088  0.50027519\n[24,]  0.66242150  0.79456398  1.28229789  1.1369072252  0.36494912  0.29993320\n[25,]  0.11491096 -0.67752131  1.55591224 -0.9531517638 -0.43306504  0.96727700\n[26,] -0.97171844  2.00236186  0.03915553  0.0979107090  1.01294064 -0.86467646\n[27,] -0.10713776  1.13254478  0.06800519  1.3104870153 -0.78844151  0.38194366\n[28,] -0.11368363 -1.13303668 -0.44515516 -0.0309968966 -0.05286304 -0.17959227\n[29,]  0.03995439 -0.06103477 -0.17348145  0.3417751499 -1.22149166 -0.09463653\n[30,] -0.61338026  1.72693492 -0.07276041 -1.2660686396  1.38200547  1.95263797\n[31,]  0.79663725  0.35281020  0.08109498 -0.5131215937  0.67712222 -0.13873520\n[32,] -0.05972762 -1.30023954  0.11504776  0.5298008516  1.06058730 -0.29503060\n[33,]  0.25541147  0.66221148  0.44501905 -0.5534313394  0.90651818 -1.05932657\n             PC13        PC14        PC15         PC16         PC17\n [1,]  1.04550104  0.30220825 -0.26493487  0.990718773 -0.161522621\n [2,] -0.51140324  0.23962723  0.04514841 -0.864576510  0.021378514\n [3,] -0.45367844  0.47042709  0.44536737  0.117387548  1.132044168\n [4,]  0.76623701  0.33414241 -0.06674780  0.005890288 -0.317090528\n [5,] -0.22473438  1.02668595 -1.11036905  0.253771120  0.573823233\n [6,] -0.21758119  0.35417045  0.55539524  0.380784887  0.577014348\n [7,] -1.13189958 -0.40654870  0.30620976  0.183130027 -0.149934970\n [8,]  0.48772693 -0.20754494  0.30040817  0.432882057 -0.190155729\n [9,]  0.47865039  0.06044313  0.29758301  0.065987000 -0.412015158\n[10,] -0.03562393  0.19804842  0.33881205 -0.396898429 -0.531313384\n[11,] -0.90180692 -0.34546312 -0.75620771  0.625598888 -0.544476288\n[12,]  0.30509441  0.08813338 -0.58059025 -0.035026694  0.625823661\n[13,] -0.73594105  0.53970030 -0.79585891  0.555871566 -0.252855797\n[14,] -0.21555707 -0.91839176 -1.10063830 -0.346724418 -0.207470713\n[15,]  0.72072562 -0.64024769 -0.07151878 -0.333963902  0.392495606\n[16,]  0.13959352  0.68049473  0.84486635 -0.265106061 -0.372669045\n[17,] -1.02024905 -0.87052303  0.58413566  0.839988817  0.250265142\n[18,]  0.77126745 -0.26031481  0.33705965 -0.046675725  0.049282576\n[19,]  0.70815779  0.48976813  0.01791635 -0.847195425 -0.125274155\n[20,]  0.62144737 -0.75001136 -0.14219657 -0.508865779  0.007346965\n[21,]  0.41210426 -0.10629725  0.27929458  0.308950842 -0.697452881\n[22,] -0.59841689  0.14146264  0.02533706 -0.177525294  0.552971887\n[23,] -0.77530358 -0.21170311 -0.19235313 -0.271578336 -0.009956336\n[24,]  0.33558108 -0.22233178 -0.25908437 -0.300951777 -0.380190914\n[25,]  0.64085526  0.78994926 -0.57225157  0.361706690 -0.160414591\n[26,]  0.13507319  0.08748313  0.02580035 -0.785383761  0.456930349\n[27,] -0.10431199  0.13635919  0.47578097  0.572230799 -0.118992564\n[28,] -1.18521749 -0.05682740  0.35960572 -0.270080750 -0.621317543\n[29,] -0.75999067 -0.33173999  0.05271739 -0.414813487  0.312481307\n[30,] -0.34096948  0.30426194 -0.35124709 -0.539719364 -0.359452130\n[31,]  0.15560337  0.44934089  0.86956307  0.051061891 -0.041440723\n[32,]  0.35869730 -0.18428438  0.18487185  0.420292080  0.248554770\n[33,]  1.13036896 -1.18047718 -0.08187461  0.238832439  0.453583546\n              PC18         PC19          PC20         PC21\n [1,]  0.512534055  0.403637255 -0.2288274019 -0.180597545\n [2,]  0.929440714  0.048122004  0.2051146098 -0.271152883\n [3,]  0.403516656 -0.074179505 -0.2401958070 -0.046316428\n [4,] -0.380413260 -0.016801083  0.0236080284  0.075154120\n [5,] -0.121436088  0.216713273 -0.2532722027  0.163180806\n [6,]  0.016193332 -0.176022291  0.8096079044 -0.052570563\n [7,] -0.041349452  0.491356049  0.1502223084 -0.205615503\n [8,] -0.169826040 -0.182268894 -0.0271317045  0.331935144\n [9,] -0.170416032  0.311027931  0.4390800414 -0.032482679\n[10,] -0.228929235 -0.629795038 -0.1373141246  0.072909759\n[11,]  0.223418908 -0.399313078  0.1457129024 -0.346000339\n[12,] -0.361282411  0.136577651  0.1213875759  0.000801914\n[13,]  0.048394699 -0.297878346  0.0857017322 -0.076965204\n[14,] -0.556266739  0.183329956  0.1575056075  0.005626377\n[15,]  0.003248088 -0.275646771 -0.0488034493 -0.010278083\n[16,] -0.569143575  0.610124017  0.0001699299 -0.165370989\n[17,] -0.876273716  0.145493206 -0.3407035497 -0.133229363\n[18,]  0.139800530  0.266416571 -0.0612808299  0.138282533\n[19,] -0.288699113 -0.005113258  0.1267147465 -0.263464768\n[20,]  0.261595957  0.070258777 -0.3355360658 -0.432480871\n[21,]  0.478432361 -0.146679334 -0.3985086902 -0.071646635\n[22,] -0.090751620 -0.213675088 -0.1815364089 -0.142428632\n[23,]  0.266768773  0.195774478 -0.1770461524  0.191720088\n[24,]  0.354865033 -0.239914062  0.0937794098  0.244839284\n[25,] -0.161337294  0.208473109 -0.0508044281  0.105092550\n[26,] -0.040945263  0.377943641 -0.1672829144  0.091569499\n[27,]  0.419873948 -0.083337959  0.0482601803  0.312179849\n[28,]  0.322418197  0.609269862 -0.0166547989  0.337744117\n[29,]  0.091695107 -0.301130070 -0.0161094691  0.322694249\n[30,] -0.412731002 -0.414631089  0.0294813311  0.084790553\n[31,] -0.298987891 -0.721677547 -0.2130771535 -0.111634161\n[32,]  0.187128445 -0.059177362  0.2716590271 -0.102730154\n[33,]  0.109463929 -0.037277006  0.1860798156  0.166443959\n\n\nSo to combine them with the original variables, do this:\n\nlecturerpcscores &lt;- cbind(lecturers, pca1$x)"
  },
  {
    "objectID": "tutorials/tutorial10.html#biplot",
    "href": "tutorials/tutorial10.html#biplot",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "We can produce a biplot in ggplot2, using the factor gender as the colour:\n\nautoplot(pca1, data = lecturerpcscores, colour = 'gender')\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at &lt;https://github.com/sinhrks/ggfortify/issues&gt;.\n\n\n\n\n\n\n\n\n\nWe could add the loadings, but they are a bit messy, best to look at loadings or rotations tables.\nYou can make the plot using different symbols, change background etc through reading the ggplot2 documentation."
  },
  {
    "objectID": "tutorials/tutorial10.html#anova-on-pc-scores",
    "href": "tutorials/tutorial10.html#anova-on-pc-scores",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "You can use this file to do the ANOVAs, you will have to do yourself. For example for differences in PC scores for gender:\n\nAOV1 &lt;- aov(lecturerpcscores$PC1 ~ gender, data = lecturerpcscores)\nsummary(AOV1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ngender       1    4.6   4.596    0.93  0.342\nResiduals   31  153.2   4.942               \n\n\nYou can do something similar for your projects and regression, using your PC scores as the predictors."
  },
  {
    "objectID": "tutorials/tutorial10.html#boxplot",
    "href": "tutorials/tutorial10.html#boxplot",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "To do a boxplot of variables use ggplot2:\n\nggplot(lecturerpcscores, aes(x = gender, y = PC1)) +\n  geom_boxplot()"
  },
  {
    "objectID": "tutorials/tutorial10.html#factor-analysis-with-varimax-rotation",
    "href": "tutorials/tutorial10.html#factor-analysis-with-varimax-rotation",
    "title": "Tutorial 10: PCA and Factor Analysis",
    "section": "",
    "text": "To do a Varimax rotation on your principal components, follow these commands. Do a factor analysis with the rotation being Varimax:\n\nfa1 &lt;- factanal(lecturers[, 8:28], 10, rotation = \"varimax\")\nfa1\n\n\nCall:\nfactanal(x = lecturers[, 8:28], factors = 10, rotation = \"varimax\")\n\nUniquenesses:\n             accessible              attractive              wide_range \n                  0.381                   0.005                   0.005 \nteaching.based_research            Dress_nicely focus_on_complex_issues \n                  0.005                   0.283                   0.332 \n          gives_example          gives_out_note         sense_of_humour \n                  0.005                   0.448                   0.473 \n            mark_easily            moves_around    offers_lecture_notes \n                  0.005                   0.294                   0.141 \n        clear_objective       solicit_questions               text_book \n                  0.244                   0.005                   0.310 \n       primary_research            own_research           teach_in_team \n                  0.310                   0.237                   0.548 \n  understand_principles           use_a_pointer          well_respected \n                  0.584                   0.067                   0.005 \n\nLoadings:\n                        Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7\naccessible                      -0.583  -0.426   0.222  -0.110          -0.130 \nattractive                       0.961   0.106                           0.158 \nwide_range              -0.177                   0.952                         \nteaching.based_research                 -0.356   0.374  -0.114                 \nDress_nicely                     0.829   0.100                                 \nfocus_on_complex_issues  0.116                   0.674          -0.163  -0.141 \ngives_example                    0.122   0.112           0.955   0.163   0.132 \ngives_out_note           0.362   0.290   0.216  -0.196   0.305   0.125   0.261 \nsense_of_humour          0.127   0.229   0.599   0.125                   0.108 \nmark_easily              0.150   0.298   0.143                           0.910 \nmoves_around             0.326   0.244   0.186  -0.102  -0.155          -0.194 \noffers_lecture_notes     0.362   0.240   0.333           0.667          -0.232 \nclear_objective          0.768   0.109   0.343           0.169                 \nsolicit_questions        0.880  -0.139   0.130   0.109  -0.156           0.169 \ntext_book                0.611          -0.141  -0.152   0.166   0.202   0.427 \nprimary_research                                                               \nown_research             0.100                   0.129          -0.112  -0.102 \nteach_in_team            0.154          -0.103           0.155   0.614         \nunderstand_principles    0.608                                   0.155         \nuse_a_pointer            0.164   0.146   0.902  -0.142   0.165                 \nwell_respected           0.133           0.287                   0.931   0.101 \n                        Factor8 Factor9 Factor10\naccessible                                      \nattractive                      -0.121          \nwide_range               0.102   0.137  -0.126  \nteaching.based_research  0.158   0.819          \nDress_nicely                                    \nfocus_on_complex_issues                  0.369  \ngives_example                                   \ngives_out_note           0.244          -0.120  \nsense_of_humour                 -0.226          \nmark_easily             -0.113  -0.106          \nmoves_around             0.429   0.444   0.217  \noffers_lecture_notes    -0.159           0.167  \nclear_objective                                 \nsolicit_questions        0.326                  \ntext_book               -0.146                  \nprimary_research         0.213           0.790  \nown_research             0.784   0.119   0.255  \nteach_in_team                                   \nunderstand_principles            0.110          \nuse_a_pointer            0.109                  \nwell_respected                  -0.100          \n\n               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7 Factor8\nSS loadings      2.654   2.398   1.996   1.705   1.667   1.423   1.325   1.137\nProportion Var   0.126   0.114   0.095   0.081   0.079   0.068   0.063   0.054\nCumulative Var   0.126   0.241   0.336   0.417   0.496   0.564   0.627   0.681\n               Factor9 Factor10\nSS loadings      1.039    0.970\nProportion Var   0.049    0.046\nCumulative Var   0.731    0.777\n\nTest of the hypothesis that 10 factors are sufficient.\nThe chi square statistic is 28.56 on 45 degrees of freedom.\nThe p-value is 0.973"
  },
  {
    "objectID": "tutorials/tutorial07.html",
    "href": "tutorials/tutorial07.html",
    "title": "Tutorial 07",
    "section": "",
    "text": "This is the same dataset used in the lecture.\nFragmentation of forest habitat has an impact of wildlife abundance. This study looked at the relationship between bird abundance (bird ha-1) and the characteristics of forest patches at 56 locations in SE Victoria.\nThe predictor variables are:\n\nALT Altitude (m)\nYR.ISOL Year when the patch was isolated (years)\nGRAZE Grazing (coded 1-5 which is light to heavy)\nAREA Patch area (ha)\nDIST Distance to nearest patch (km)\nLDIST Distance to largest patch (km)\n\nImport the data from the “Loyn” tab in the MS Excel file.\n\nlibrary(readxl)\nloyn &lt;- read_xlsx(\"data/mlr.xlsx\", \"Loyn\")\n\nOften, the first step in model development is to examine the data. This is a good way to get a feel for the data and to identify any issues that may need to be addressed. In this case, we will examine the data using histograms and a correlation matrix.\n\n\nThere are a breadth of ways to create histograms in R. In each tab below you will find some different ways to create the same plot outputs.\n\nhist()hist.data.frame() from Hmiscggplot()ggplot() with dplyr\n\n\nThis is a straightforward way to create multiple histograms with hist(). The par() function is used to arrange the plots on the page. The mfrow argument specifies the number of rows and columns of plots.\n\n# par(mfrow=c(3,3))\nhist(loyn$ABUND)\n\n\n\n\n\n\n\nhist(loyn$ALT)\n\n\n\n\n\n\n\nhist(loyn$YR.ISOL)\n\n\n\n\n\n\n\nhist(loyn$GRAZE)\n\n\n\n\n\n\n\nhist(loyn$AREA)\n\n\n\n\n\n\n\nhist(loyn$DIST)\n\n\n\n\n\n\n\nhist(loyn$LDIST)\n\n\n\n\n\n\n\n# par(mfrow=c(1,1))\n\n\n\nThe Hmisc package provides a function hist.data.frame() that can be used to create multiple histograms, which can be called by simply using hist(). You may need to tweak the nclass argument to get the desired number of bins, as the default may not look appropriate.\n\n# install.packages(\"Hmisc\")\nlibrary(Hmisc)\nhist(loyn, nclass = 50)\n\n\n\nA more modern approach is to use ggplot() with facet_wrap() to arrange multiple plots on a single page. To do this, the pivot_longer() function from the tidyr package is used to reshape the data into a tidy format.\n\n# tidy the data\nloyn_tidy &lt;- pivot_longer(loyn, cols = everything())\n\n# plot\nggplot(loyn_tidy, aes(x = value)) +\n    geom_histogram() +\n    facet_wrap(~name, scales = \"free\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\nHere we use the pipe operator %&gt;% from dplyr to chain together a series of commands. The pipe operator takes the output of the command on the left and passes it to the command on the right (or below) the pipe. This means that we can create a series of commands that are executed in order.\n\nloyn %&gt;%\n    pivot_longer(cols = everything()) %&gt;%\n    ggplot(aes(x = value)) +\n    geom_histogram() +\n    facet_wrap(~name, scales = \"free\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComment on the histograms in terms of leverage. Hint: what is the relationship between leverage and skewness?\n\n\n\nCalculate the correlation matrix using cor(Loyn).\n\ncor(loyn)\n\n              ABUND         AREA      YR.ISOL       DIST       LDIST\nABUND    1.00000000  0.255970206  0.503357741  0.2361125  0.08715258\nAREA     0.25597021  1.000000000 -0.001494192  0.1083429  0.03458035\nYR.ISOL  0.50335774 -0.001494192  1.000000000  0.1132175 -0.08331686\nDIST     0.23611248  0.108342870  0.113217524  1.0000000  0.31717234\nLDIST    0.08715258  0.034580346 -0.083316857  0.3171723  1.00000000\nGRAZE   -0.68251138 -0.310402417 -0.635567104 -0.2558418 -0.02800944\nALT      0.38583617  0.387753885  0.232715406 -0.1101125 -0.30602220\n              GRAZE        ALT\nABUND   -0.68251138  0.3858362\nAREA    -0.31040242  0.3877539\nYR.ISOL -0.63556710  0.2327154\nDIST    -0.25584182 -0.1101125\nLDIST   -0.02800944 -0.3060222\nGRAZE    1.00000000 -0.4071671\nALT     -0.40716705  1.0000000\n\n\n\n\n\nWhich independent variables are useful for predicting the dependent variable abundance? Is there evidence for multi-collinearity?\n\n\n\nExamine correlations visually using pairs() or corrplot() from the corrplot package.\n\nScatterplot matrixCorrelation matrix\n\n\n\npairs(loyn)\n\n\n\n\n\n\n\n\n\n\n\nlibrary(corrplot)\ncorrplot(cor(loyn))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAre there any trends visible from the plots?\n\n\n\n\n\n\n\nTip\n\n\n\nWe can also bring in variance inflation factors (VIF) to help us identify multi-collinearity, but that is done only after we have selected a model.\n\n\n\n\nThe AREA predictor has a small number of observations with very large values. Apply a log10 transformation and label the new variable Loyn$L10AREA.\n\nloyn$L10AREA &lt;- log10(loyn$AREA)\n\n\n\n\nWhy are we transforming AREA?\n\n\n\nRe-run pairs(Loyn) and create a histogram using the transformed value of AREA, how do the plots look?\n\n\nhist(loyn$L10AREA)\npairs(loyn)\n\n\n\nIn preparation for modelling, transform the remaining skewed variables, DIST and LDIST the same way you did for AREA and examine the histogram and pairs plots using these new variables.\n\nMake sure you end up with two new variables labelled loyn$L10DIST and loyn$L10LDIST."
  },
  {
    "objectID": "tutorials/tutorial07.html#bird-abundance",
    "href": "tutorials/tutorial07.html#bird-abundance",
    "title": "Tutorial 07",
    "section": "",
    "text": "This is the same dataset used in the lecture.\nFragmentation of forest habitat has an impact of wildlife abundance. This study looked at the relationship between bird abundance (bird ha-1) and the characteristics of forest patches at 56 locations in SE Victoria.\nThe predictor variables are:\n\nALT Altitude (m)\nYR.ISOL Year when the patch was isolated (years)\nGRAZE Grazing (coded 1-5 which is light to heavy)\nAREA Patch area (ha)\nDIST Distance to nearest patch (km)\nLDIST Distance to largest patch (km)\n\nImport the data from the “Loyn” tab in the MS Excel file.\n\nlibrary(readxl)\nloyn &lt;- read_xlsx(\"data/mlr.xlsx\", \"Loyn\")\n\nOften, the first step in model development is to examine the data. This is a good way to get a feel for the data and to identify any issues that may need to be addressed. In this case, we will examine the data using histograms and a correlation matrix.\n\n\nThere are a breadth of ways to create histograms in R. In each tab below you will find some different ways to create the same plot outputs.\n\nhist()hist.data.frame() from Hmiscggplot()ggplot() with dplyr\n\n\nThis is a straightforward way to create multiple histograms with hist(). The par() function is used to arrange the plots on the page. The mfrow argument specifies the number of rows and columns of plots.\n\n# par(mfrow=c(3,3))\nhist(loyn$ABUND)\n\n\n\n\n\n\n\nhist(loyn$ALT)\n\n\n\n\n\n\n\nhist(loyn$YR.ISOL)\n\n\n\n\n\n\n\nhist(loyn$GRAZE)\n\n\n\n\n\n\n\nhist(loyn$AREA)\n\n\n\n\n\n\n\nhist(loyn$DIST)\n\n\n\n\n\n\n\nhist(loyn$LDIST)\n\n\n\n\n\n\n\n# par(mfrow=c(1,1))\n\n\n\nThe Hmisc package provides a function hist.data.frame() that can be used to create multiple histograms, which can be called by simply using hist(). You may need to tweak the nclass argument to get the desired number of bins, as the default may not look appropriate.\n\n# install.packages(\"Hmisc\")\nlibrary(Hmisc)\nhist(loyn, nclass = 50)\n\n\n\nA more modern approach is to use ggplot() with facet_wrap() to arrange multiple plots on a single page. To do this, the pivot_longer() function from the tidyr package is used to reshape the data into a tidy format.\n\n# tidy the data\nloyn_tidy &lt;- pivot_longer(loyn, cols = everything())\n\n# plot\nggplot(loyn_tidy, aes(x = value)) +\n    geom_histogram() +\n    facet_wrap(~name, scales = \"free\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\nHere we use the pipe operator %&gt;% from dplyr to chain together a series of commands. The pipe operator takes the output of the command on the left and passes it to the command on the right (or below) the pipe. This means that we can create a series of commands that are executed in order.\n\nloyn %&gt;%\n    pivot_longer(cols = everything()) %&gt;%\n    ggplot(aes(x = value)) +\n    geom_histogram() +\n    facet_wrap(~name, scales = \"free\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComment on the histograms in terms of leverage. Hint: what is the relationship between leverage and skewness?\n\n\n\nCalculate the correlation matrix using cor(Loyn).\n\ncor(loyn)\n\n              ABUND         AREA      YR.ISOL       DIST       LDIST\nABUND    1.00000000  0.255970206  0.503357741  0.2361125  0.08715258\nAREA     0.25597021  1.000000000 -0.001494192  0.1083429  0.03458035\nYR.ISOL  0.50335774 -0.001494192  1.000000000  0.1132175 -0.08331686\nDIST     0.23611248  0.108342870  0.113217524  1.0000000  0.31717234\nLDIST    0.08715258  0.034580346 -0.083316857  0.3171723  1.00000000\nGRAZE   -0.68251138 -0.310402417 -0.635567104 -0.2558418 -0.02800944\nALT      0.38583617  0.387753885  0.232715406 -0.1101125 -0.30602220\n              GRAZE        ALT\nABUND   -0.68251138  0.3858362\nAREA    -0.31040242  0.3877539\nYR.ISOL -0.63556710  0.2327154\nDIST    -0.25584182 -0.1101125\nLDIST   -0.02800944 -0.3060222\nGRAZE    1.00000000 -0.4071671\nALT     -0.40716705  1.0000000\n\n\n\n\n\nWhich independent variables are useful for predicting the dependent variable abundance? Is there evidence for multi-collinearity?\n\n\n\nExamine correlations visually using pairs() or corrplot() from the corrplot package.\n\nScatterplot matrixCorrelation matrix\n\n\n\npairs(loyn)\n\n\n\n\n\n\n\n\n\n\n\nlibrary(corrplot)\ncorrplot(cor(loyn))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAre there any trends visible from the plots?\n\n\n\n\n\n\n\nTip\n\n\n\nWe can also bring in variance inflation factors (VIF) to help us identify multi-collinearity, but that is done only after we have selected a model.\n\n\n\n\nThe AREA predictor has a small number of observations with very large values. Apply a log10 transformation and label the new variable Loyn$L10AREA.\n\nloyn$L10AREA &lt;- log10(loyn$AREA)\n\n\n\n\nWhy are we transforming AREA?\n\n\n\nRe-run pairs(Loyn) and create a histogram using the transformed value of AREA, how do the plots look?\n\n\nhist(loyn$L10AREA)\npairs(loyn)\n\n\n\nIn preparation for modelling, transform the remaining skewed variables, DIST and LDIST the same way you did for AREA and examine the histogram and pairs plots using these new variables.\n\nMake sure you end up with two new variables labelled loyn$L10DIST and loyn$L10LDIST."
  },
  {
    "objectID": "tutorials/tutorial05.html",
    "href": "tutorials/tutorial05.html",
    "title": "Tutorial 05: Experimental Design and ANOVA",
    "section": "",
    "text": "In this week’s lectures we covered complete randomised designs (CRD) and randomized block designs (RBD) for experiments with one factor or treatment. We also covered analysis of variance (ANOVA) as a method to analyse data from these experimental designs."
  },
  {
    "objectID": "tutorials/tutorial05.html#exercise-1-testing-randomisation",
    "href": "tutorials/tutorial05.html#exercise-1-testing-randomisation",
    "title": "Tutorial 05: Experimental Design and ANOVA",
    "section": "1.1 Exercise 1: Testing randomisation",
    "text": "1.1 Exercise 1: Testing randomisation\nLet’s work together as a class to test the effect of randomisation on an experiment. We will simulate an experiment with and without randomisation and compare the results.\nAssign each student a number from 1 to the total number of students in the class. Each student will be asked to answer the question:\n\n“Do you know the student next to you? Yes or No”\n\n\n1.1) Non-random sample\n\n“Do you know the student sitting next to you? Yes or No”\n\nPut your hand up if you answer “Yes”, and keep it down if you answer “No”. The students with their hands up are counted as “Yes” and the students with their hands down are counted as “No”. This is our non-random sample.\nClass answers — Replace these with your class numbers:\n\ntotal_nonrandom &lt;- 60 # total students in non-random sample/class size\ncount_yes_nonrandom &lt;- 38 # yes I know the person next to me\ncount_no_nonrandom  &lt;- 22 # no I don't know the person next to me\n\nClass size (number of students present):\n\nclass_size &lt;- total_nonrandom\n\n\n\n1.2) Percentages from the non-random sample\n\nprop_yes_nonrandom &lt;- (count_yes_nonrandom / total_nonrandom) * 100\nprop_no_nonrandom  &lt;- (count_no_nonrandom  / total_nonrandom) * 100\n\n\ncat(\"Non-random sample % Yes:\", round(prop_yes_nonrandom, 1), \"%\\n\")\n\nNon-random sample % Yes: 63.3 %\n\ncat(\"Non-random sample % No :\", round(prop_no_nonrandom, 1),  \"%\\n\\n\")\n\nNon-random sample % No : 36.7 %\n\n\n\n\n1.3) Random sample\nWe simulate student IDs and assign a random number to each ID.\n\nset.seed(123)  # For reproducibility\nstudent_ids &lt;- seq_len(class_size)\nrand_num &lt;- runif(class_size)\nclass_df &lt;- data.frame(id = student_ids, rand_num = rand_num)\n\nRandom sample size (how many students you will sample at random):\n\nrandom_sample_size &lt;- 30 # choose a sample size for the random sample\n\nRandomly select students:\n\nset.seed(202)\nsampled_ids &lt;- sample(class_df$id, size = random_sample_size, replace = FALSE)\n\nsampled_ids # students with these numbers were randomly selected to be asked the question again.\n\n [1] 16 46 40  8  6 60 22 36 45 57 33 51 55 11 27 43 41 25 49 54 47 24 29 39 59\n[26] 17 13  1 50  4\n\n\n\n“Do you know the student next to you? Yes or No”\n\nEnter counts for the random sample:\nAfter you randomly select those students and ask the question, enter the counts here.\n\ncount_yes_random &lt;- 14\ncount_no_random  &lt;- 16\n\nPercentages for the random sample:\n\nprop_yes_random &lt;- (count_yes_random / random_sample_size) * 100\nprop_no_random  &lt;- (count_no_random  / random_sample_size) * 100\n\n\ncat(\"Random sample % Yes:\", round(prop_yes_random, 1), \"%\\n\")\n\nRandom sample % Yes: 46.7 %\n\ncat(\"Random sample % No :\", round(prop_no_random, 1),  \"%\\n\\n\")\n\nRandom sample % No : 53.3 %\n\n\nEstimate the bias:\nBias here is the difference between the non-random sample proportion and the random sample proportion.\n\nbias_yes &lt;- prop_yes_nonrandom - prop_yes_random\nbias_no  &lt;- prop_no_nonrandom  - prop_no_random\n\n\ncat(\"Estimated bias in % Yes (non-random - random):\", round(bias_yes, 1), \"%\\n\")\n\nEstimated bias in % Yes (non-random - random): 16.7 %\n\ncat(\"Estimated bias in % No  (non-random - random):\", round(bias_no, 1),  \"%\\n\\n\")\n\nEstimated bias in % No  (non-random - random): -16.7 %\n\n\nVisualisation:\nBar plot of % Yes/No for non-random vs random samples using ggplot2.\n\npercent_df &lt;- data.frame(\n  sample_type = rep(c(\"Non-random sample\", \"Random sample\"), each = 2),\n  response     = rep(c(\"Yes\", \"No\"), times = 2),\n  percent      = c(prop_yes_nonrandom, prop_no_nonrandom, prop_yes_random, prop_no_random)\n)\n\nlibrary(ggplot2)\n\np &lt;- ggplot(percent_df, aes(x = response, y = percent, fill = sample_type)) +\n  geom_col(position = position_dodge(width = 1)) +\n  labs(title = \"Non-random vs Random Sample: % Knowing the Neighbour\",\n       x = \"Response\", y = \"Percent\", fill = \"Sample type\") +\n  scale_fill_manual(values = c(\"Non-random sample\" = \"#4C78A8\", \"Random sample\" = \"#F58518\")) +\n  theme_minimal(base_size = 12)\n\nprint(p)\n\n\n\n\n\n\n\n\nNon-random sampling (asking about the person next to you) is typically biased because seating patterns cluster friends or cohorts.\n\nRandom sampling gives each student an equal chance, reducing systematic bias.\nComparing the two percentages provides a simple measure of bias in this context."
  },
  {
    "objectID": "tutorials/tutorial05.html#exercise-2-identifying-units",
    "href": "tutorials/tutorial05.html#exercise-2-identifying-units",
    "title": "Tutorial 05: Experimental Design and ANOVA",
    "section": "2.1 Exercise 2: Identifying units",
    "text": "2.1 Exercise 2: Identifying units\nConsider an experiment to test the effect of different fertilizers on plant growth. The experiment uses 12 pots, with 4 pots assigned to each of 3 fertilizer treatments (A, B, and C). Each pot contains 5 plants, and the height of each plant is measured after 4 weeks.\n\nWhat is the experimental unit and what is the sampling unit in this experiment?\n\n\n# Experimental unit: pot\n# Sampling unit: individual plant\n\n\nThe experimental unit is the pot, as the fertilizer treatment is applied to the entire pot.\nThe sampling unit is the individual plant, as the height of each plant is measured.\n\n\nWorking in pairs, identify other examples of experimental units vs sampling units and check answers with staff.\n\n\n# Examples here"
  },
  {
    "objectID": "tutorials/tutorial05.html#exercise-3-completely-randomised-design-crd",
    "href": "tutorials/tutorial05.html#exercise-3-completely-randomised-design-crd",
    "title": "Tutorial 05: Experimental Design and ANOVA",
    "section": "4.1 Exercise 3: Completely Randomised Design (CRD)",
    "text": "4.1 Exercise 3: Completely Randomised Design (CRD)\nConsider an experiment to test the effect of different fertilizers on plant growth using a CRD. The experiment uses 12 pots, with 4 pots assigned to each of 3 fertilizer treatments (A, B, and C). Each pot contains 5 plants, and the height of each plant is measured after 4 weeks.\n\n# Randomly assign treatments to pots\nset.seed(123)\ntreatments &lt;- rep(c(\"A\", \"B\", \"C\"), each = 4)\npots &lt;- data.frame(\n  pot_id = 1:12,\n  treatment = sample(treatments)\n)\npots\n\n   pot_id treatment\n1       1         A\n2       2         C\n3       3         C\n4       4         A\n5       5         B\n6       6         C\n7       7         B\n8       8         A\n9       9         C\n10     10         B\n11     11         A\n12     12         B\n\n\n\nThe treatments are randomly assigned to the pots.\nThe height of each plant is measured after 4 weeks.\n\n\n# Simulate plant height data\nset.seed(456)\nplant_heights &lt;- data.frame(\n  pot_id = rep(1:12, each = 5),\n  plant_id = 1:60,\n  height = rnorm(60, mean = rep(c(10, 15, 20), each = 20), sd = 2)\n)\nplant_heights &lt;- merge(plant_heights, pots, by = \"pot_id\")\nplant_heights\n\n   pot_id plant_id    height treatment\n1       1        1  7.312957         A\n2       1        2 11.243551         A\n3       1        3 11.601749         A\n4       1        4  7.222215         A\n5       1        5  8.571286         A\n6       2        6  9.351878         C\n7       2        7 11.381286         C\n8       2        8 10.501096         C\n9       2        9 12.014705         C\n10      2       10 11.146469         C\n11      3       11  8.168379         C\n12      3       12 12.622195         C\n13      3       13 11.977453         C\n14      3       14 13.307857         C\n15      3       15  7.118390         C\n16      4       16 13.894713         A\n17      4       17 13.473872         A\n18      4       18 10.774967         A\n19      4       19 14.560068         A\n20      4       20 13.075767         A\n21      5       21 14.050792         B\n22      5       22 11.565382         B\n23      5       23 12.146339         B\n24      5       24 15.416472         B\n25      5       25 14.928328         B\n26      6       26 17.268569         C\n27      6       27 14.074290         C\n28      6       28 14.343232         C\n29      6       29 17.969079         C\n30      6       30 12.821244         C\n31      7       31 13.942412         B\n32      7       32 13.812414         B\n33      7       33 11.002169         B\n34      7       34 15.592306         B\n35      7       35 15.341251         B\n36      8       36 18.631305         A\n37      8       37 13.678794         A\n38      8       38 14.719496         A\n39      8       39 14.152042         A\n40      8       40 14.922528         A\n41      9       41 19.942116         C\n42      9       42 20.786075         C\n43      9       43 19.500772         C\n44      9       44 20.166900         C\n45      9       45 24.157749         C\n46     10       46 20.241704         B\n47     10       47 20.236299         B\n48     10       48 21.540108         B\n49     10       49 17.649195         B\n50     10       50 20.818077         B\n51     11       51 18.670098         A\n52     11       52 19.486950         A\n53     11       53 21.357564         A\n54     11       54 21.793689         A\n55     11       55 21.236713         A\n56     12       56 21.462907         B\n57     12       57 19.173651         B\n58     12       58 23.115626         B\n59     12       59 21.083398         B\n60     12       60 21.154301         B\n\n\n\nThe data can be analysed using ANOVA to test for differences in plant height between the fertilizer treatments.\n\n\nIs there a significant difference in plant height between the fertilizer treatments?\n\n\n# answer here\n\n\n# ANOVA analysis\nanova_result &lt;- aov(height ~ treatment, data = plant_heights)\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntreatment    2  100.1   50.04   2.641   0.08 .\nResiduals   57 1080.2   18.95                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe ANOVA results will indicate whether there are significant differences in plant height between the fertilizer treatments."
  },
  {
    "objectID": "tutorials/tutorial05.html#exercise-4-randomised-complete-block-design-rcbd",
    "href": "tutorials/tutorial05.html#exercise-4-randomised-complete-block-design-rcbd",
    "title": "Tutorial 05: Experimental Design and ANOVA",
    "section": "5.1 Exercise 4: Randomised Complete Block Design (RCBD)",
    "text": "5.1 Exercise 4: Randomised Complete Block Design (RCBD)\nConsider an experiment to test the effect of different fertilizers on plant growth using an RCBD. The experiment uses 12 pots, with 4 pots assigned to each of 3 fertilizer treatments (A, B, and C). The pots are grouped into 4 blocks based on their location in the greenhouse (North, South, East, West). Each pot contains 5 plants, and the height of each plant is measured after 4 weeks.\n\n# Create blocks and randomly assign treatments within blocks\nset.seed(123)\nblocks &lt;- rep(c(\"North\", \"South\", \"East\", \"West\"), each = 3)\ntreatments &lt;- rep(c(\"A\", \"B\", \"C\"), times = 4)\npots &lt;- data.frame(\n  pot_id = 1:12,\n  block = blocks,\n  treatment = sample(treatments)\n)\npots\n\n   pot_id block treatment\n1       1 North         C\n2       2 North         C\n3       3 North         A\n4       4 South         B\n5       5 South         C\n6       6 South         B\n7       7  East         B\n8       8  East         A\n9       9  East         C\n10     10  West         B\n11     11  West         A\n12     12  West         A\n\n\n\nThe treatments are randomly assigned to the pots within each block.\nThe height of each plant is measured after 4 weeks.\n\n\n# Simulate plant height data\nset.seed(456)\nplant_heights &lt;- data.frame(\n  pot_id = rep(1:12, each = 5),\n  plant_id = 1:60,\n  height = rnorm(60, mean = rep(c(10, 15, 20), each = 20) + rep(c(2, -2, 1, -1), each = 15), sd = 2)\n)\nplant_heights &lt;- merge(plant_heights, pots, by = \"pot_id\")\nplant_heights\n\n   pot_id plant_id    height block treatment\n1       1        1  9.312957 North         C\n2       1        2 13.243551 North         C\n3       1        3 13.601749 North         C\n4       1        4  9.222215 North         C\n5       1        5 10.571286 North         C\n6       2        6 11.351878 North         C\n7       2        7 13.381286 North         C\n8       2        8 12.501096 North         C\n9       2        9 14.014705 North         C\n10      2       10 13.146469 North         C\n11      3       11 10.168379 North         A\n12      3       12 14.622195 North         A\n13      3       13 13.977453 North         A\n14      3       14 15.307857 North         A\n15      3       15  9.118390 North         A\n16      4       16 11.894713 South         B\n17      4       17 11.473872 South         B\n18      4       18  8.774967 South         B\n19      4       19 12.560068 South         B\n20      4       20 11.075767 South         B\n21      5       21 12.050792 South         C\n22      5       22  9.565382 South         C\n23      5       23 10.146339 South         C\n24      5       24 13.416472 South         C\n25      5       25 12.928328 South         C\n26      6       26 15.268569 South         B\n27      6       27 12.074290 South         B\n28      6       28 12.343232 South         B\n29      6       29 15.969079 South         B\n30      6       30 10.821244 South         B\n31      7       31 14.942412  East         B\n32      7       32 14.812414  East         B\n33      7       33 12.002169  East         B\n34      7       34 16.592306  East         B\n35      7       35 16.341251  East         B\n36      8       36 19.631305  East         A\n37      8       37 14.678794  East         A\n38      8       38 15.719496  East         A\n39      8       39 15.152042  East         A\n40      8       40 15.922528  East         A\n41      9       41 20.942116  East         C\n42      9       42 21.786075  East         C\n43      9       43 20.500772  East         C\n44      9       44 21.166900  East         C\n45      9       45 25.157749  East         C\n46     10       46 19.241704  West         B\n47     10       47 19.236299  West         B\n48     10       48 20.540108  West         B\n49     10       49 16.649195  West         B\n50     10       50 19.818077  West         B\n51     11       51 17.670098  West         A\n52     11       52 18.486950  West         A\n53     11       53 20.357564  West         A\n54     11       54 20.793689  West         A\n55     11       55 20.236713  West         A\n56     12       56 20.462907  West         A\n57     12       57 18.173651  West         A\n58     12       58 22.115626  West         A\n59     12       59 20.083398  West         A\n60     12       60 20.154301  West         A\n\n\nThe data can be analysed using ANOVA to test for differences in plant height between the fertilizer treatments while accounting for block effects.\n\n# ANOVA analysis with blocking\nanova_result &lt;- aov(height ~ block + treatment, data = plant_heights)\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nblock        3  664.5  221.50  41.589 4.59e-14 ***\ntreatment    2   33.5   16.77   3.148   0.0509 .  \nResiduals   54  287.6    5.33                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA results will indicate whether there are significant differences in plant height between the fertilizer treatments while controlling for block effects.\nThe percentage of variation explained by treatments and blocking terms can be calculated from the ANOVA table.\n\n\n\n\n\n\nTipCalculating percentage of variation explained\n\n\n\n\nSum of Squares (SS) for each source of variation can be found in the ANOVA table.\nPercentage of variation explained by a source = (SS for that source / Total SS) × 100\n\nFor example, to calculate the percentage of variation explained by treatments and blocks:\n\nSS_total = Sum of Squares Total\nSS_treatment = Sum of Squares for Treatment\nSS_block = Sum of Squares for Block\n\nPercentage of variation explained by treatments = (SS_treatment / SS_total) × 100\nPercentage of variation explained by blocks = (SS_block / SS_total) × 100\n\n\nWe are using R to subset the ANOVA results to calculate these percentages, but you can do this by hand from the ANOVA table using a calculator. Try this and compare your answers to the results from the code below.\n\n\n\n\n\n\nImportantNote\n\n\n\nYou will need to know how to read the ANOVA table and calculate the percentage variation explained by hand for your assessments.\n\n\n\nanova_table &lt;- summary(anova_result)[[1]]\nss_total &lt;- sum(anova_table$`Sum Sq`)\nss_treatment &lt;- anova_table$`Sum Sq`[2]\nss_block &lt;- anova_table$`Sum Sq`[1]\npercent_treatment &lt;- (ss_treatment / ss_total) * 100\npercent_block &lt;- (ss_block / ss_total) * 100\ncat(\"Percentage of variation explained by treatments:\", round(percent_treatment, 1), \"%\\n\")\n\nPercentage of variation explained by treatments: 3.4 %\n\ncat(\"Percentage of variation explained by blocks:\", round(percent_block, 1), \"%\\n\")\n\nPercentage of variation explained by blocks: 67.4 %\n\n\nThis information helps to understand the contribution of treatments and blocks to the overall variation in plant height."
  },
  {
    "objectID": "tutorials/tutorial05.html#example-exam-questions",
    "href": "tutorials/tutorial05.html#example-exam-questions",
    "title": "Tutorial 05: Experimental Design and ANOVA",
    "section": "6.1 Example exam questions",
    "text": "6.1 Example exam questions\n\nDefine the terms “experimental unit” and “sampling unit”. Provide an example of each from a hypothetical agricultural experiment.\nExplain the importance of randomisation in experimental design. How does randomisation help to reduce bias in an experiment?\nDescribe the key features of a Completely Randomised Design (CRD). When is it appropriate to use a CRD?\nDescribe the key features of a Randomised Complete Block Design (RCBD). When is it appropriate to use an RCBD?\nGiven an ANOVA table from an RCBD experiment, calculate the percentage of variation explained by treatments and blocks."
  },
  {
    "objectID": "tutorials/tutorial03.html",
    "href": "tutorials/tutorial03.html",
    "title": "Tutorial 03",
    "section": "",
    "text": "Welcome to Tutorial 03. In this tutorial we will show you how to:\n\nImport data in various ways into R, focusing on MS Excel files\nManually calculate a one-way Analysis of Variance (ANOVA)"
  },
  {
    "objectID": "tutorials/tutorial03.html#welcome",
    "href": "tutorials/tutorial03.html#welcome",
    "title": "Tutorial 03",
    "section": "",
    "text": "Welcome to Tutorial 03. In this tutorial we will show you how to:\n\nImport data in various ways into R, focusing on MS Excel files\nManually calculate a one-way Analysis of Variance (ANOVA)"
  },
  {
    "objectID": "tutorials/tutorial03.html#getting-started",
    "href": "tutorials/tutorial03.html#getting-started",
    "title": "Tutorial 03",
    "section": "0.2 Getting started",
    "text": "0.2 Getting started\n\n\nIn some instances you will see notes like this one in the margin, which often contain useful tips or reminders.\n\nThese tutorials are designed to be comprehensive yet accessible. I recommend following along and grasping the statistical concepts as much as you can.\nYou will need to create your own Quarto (recommended) or R Markdown document to complete the exercises. In RStudio, go to\n\nFile -&gt; New File -&gt; Quarto Document... or\nFile -&gt; New File -&gt; R Markdown... and follow the intuitive prompts.\n\nFor those of you who are interested in furthering your data science skills and are keen to learn more, I have included optional sections. These techniques are not necessary for you to get through ENVX2001 — we do not cover code in the exams. Nevertheless these skills come handy in report writing and data analysis and can be a great addition to your projects.\nIf you have any suggestions for improvement, please let me know! Send me an email (you can find my details on Canvas)."
  },
  {
    "objectID": "tutorials/tutorial03.html#introduction",
    "href": "tutorials/tutorial03.html#introduction",
    "title": "Tutorial 03",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\n\n\n\n\n\nNote\n\n\n\nStarting this week you will be regularly reading data from both .csv and .xlsx files for your labs and assignments. We have found that many students struggle with this even though they have been taught how to do it in ENVX1002, so we have created this section as a refresher, focusing on the more complex .xlsx file format. If you already know how to import files like a pro, feel free to skip ahead.\n\n\nData import in R can be tricky because it involves learning how to work with file paths and working directories. In this section we will import the Diatoms worksheet from the diatoms.xlsx file. This data has been mixed with other information such as exploratory work and calculations, so we will need to specify the range of cells that we are interested in to import the data appropriately.\n\n\n\n\n\n\nFigure 1: We need to separate the raw data (which we are interested in) from other information such as exploratory work and calculations (which we may not be interested in). Screenshot taken directly from the diatoms.xlsx file."
  },
  {
    "objectID": "tutorials/tutorial03.html#find-the-working-directory",
    "href": "tutorials/tutorial03.html#find-the-working-directory",
    "title": "Tutorial 03",
    "section": "1.2 Find the working directory",
    "text": "1.2 Find the working directory\nUse getwd() to check the current working directory. This is the location on your computer where R will look for files.\ngetwd()\n\nIf you are working with a document file (like a Quarto or R Markdown file), the working directory is the location of the document file.\nIf you are working with a script file (like an R script), the working directory might be different! You can use setwd() to change the working directory, but this is not recommended1. Instead, use RStudio projects to set the working directory — see Tutorial 1 for more information or check out the documentation from Posit.\n\n1 The setwd() function is not recommended because it is not reproducible: it can cause issues when sharing your code with others, or when working on different computers. Instead, use RStudio projects to set the working directory."
  },
  {
    "objectID": "tutorials/tutorial03.html#move-the-file",
    "href": "tutorials/tutorial03.html#move-the-file",
    "title": "Tutorial 03",
    "section": "1.3 Move the file",
    "text": "1.3 Move the file\nMove diatoms.xlsx to the working directory. This will simplify the file path for data import.\n\n\n\n\n\n\nFigure 2: Place data in the working directory to simplify the file path for data import. In this screenshot, diatoms.xlsx has been placed in the folder that contains an .Rproj file.\n\n\n\nThen, use read_excel() from the readxl package to read the file, noting the file path, the worksheet name, and the range of cells that contain the data. Below we have assigned the output to an object called diatoms.\n\nlibrary(readxl)\ndiatoms &lt;- read_excel(\"data/diatoms.xlsx\",\n    sheet = \"Diatoms\",\n    range = \"A1:D35\"\n)"
  },
  {
    "objectID": "tutorials/tutorial03.html#moving-the-file-and-updating-the-path",
    "href": "tutorials/tutorial03.html#moving-the-file-and-updating-the-path",
    "title": "Tutorial 03",
    "section": "1.4 Moving the file and updating the path",
    "text": "1.4 Moving the file and updating the path\nSuppose you want to organise your project files and put all data files in a single folder. You can do this by creating a new folder in the same directory as your Quarto file, and moving the data file(s) into it.\nHere, we create a folder called data and move the diatoms.xlsx file into it.\n\n\n\n\n\n\nFigure 3: Create a folder called data (but it can be any other name, really), and move your data file(s) to this folder. You will have to update the path to the file in the read_excel() function to read the data from the new location.\n\n\n\nWe then update the path to the file in the read_excel() function by adding the folder name data/ to the file path.\n\ndiatoms &lt;- read_excel(\n    path = \"data/diatoms.xlsx\",\n    sheet = \"Diatoms\",\n    range = \"A1:D35\"\n)"
  },
  {
    "objectID": "tutorials/tutorial03.html#exercises",
    "href": "tutorials/tutorial03.html#exercises",
    "title": "Tutorial 03",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nHere are some quick exercises to see if you can navigate your way around file paths in R.\n\nCreate a new folder called output inside the data folder and move the diatoms.xlsx file into it. How will you update the path in the read_excel() function to reflect this change?\nRename the output folder to raw.\nRename the data folder to diatoms and update the code."
  },
  {
    "objectID": "tutorials/tutorial03.html#introduction-1",
    "href": "tutorials/tutorial03.html#introduction-1",
    "title": "Tutorial 03",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\n\n\n\n\n\nNote\n\n\n\nIn this section you will use the summary statistics from an experiment to calculate the missing values in the ANOVA table manually.\nImportantly, the point of this exercise is to appreciate the ANOVA table and how it is constructed, not so much to do the calculations. For example, you should be able to answer the following questions:\n\nWhat does the F-statistic tell us?\nWhat is the difference between the treatment and residual sum of squares?\nWhat is total mean squares (MS_{tot}) and what relationship does it have with the variance of the data?\nCan the degrees of freedom tell us whether an experiment is balanced or unbalanced?\n\nIn practice, R can perform an ANOVA and generate the table in a split second. Many people tend to ignore much of the table and focus on the p-value, which diminishes the value of all the other information in the table. We hope that by doing this exercise you will appreciate what is arguably one of the most popular statistical tests in the world. It can also help you assess the quality of any publication that uses ANOVA!\nANOVA table calculations and interpretation are both examinable."
  },
  {
    "objectID": "tutorials/tutorial03.html#a-zombie-apocalypse",
    "href": "tutorials/tutorial03.html#a-zombie-apocalypse",
    "title": "Tutorial 03",
    "section": "2.2 A zombie apocalypse",
    "text": "2.2 A zombie apocalypse\nYou and a small community have survived the zombie apocalypse. It is a world with no computers but you have many books and some goats. You have managed to perform an experiment to explore how best to feed your goats. You wish to analyse the data and have to do the ANOVA “by hand” and reconstruct an ANOVA table.\nIn your maize fodder crop experiment, you tested three planting densities (20, 30, 40 plants/unit area) across 15 similar plots resulting in a total of 5 plots per planting density. The sample means and standard deviations (kg of dry matter/plot) for each planting density are shown below:\n\n\n\n\n\nTreatment\nMean\nStd.Dev\nVariance\n\n\n\n\n20 plants per unit area\n17.58\n2.70\n7.29\n\n\n30 plants per unit area\n27.18\n1.89\n3.577\n\n\n40 plants per unit area\n27.14\n2.02\n4.093\n\n\nOverall\n23.97\n5.11\n-\n\n\n\n\n\nYou wish to perform an analysis of variance (ANOVA) to determine if the density of planting influenced the total dry weight of maize for the plot. The ANOVA table comes to mind and is mostly empty; below is what you have so far:\n\n\nMissing values in the ANOVA table have been labelled a to i for the purpose of this exercise.\n\n\n\n\n\nSource\ndf\nsum sq\nmean sq\nF value\n\n\n\n\nPlanting_Density\na\nd\n152.96\ni\n\n\nResidual\nb\ne\ng\n-\n\n\nTotal\nc\nf\nh\n-\n\n\n\n\n\nYou want to calculate the missing values in the ANOVA table above given the following:\n\nYou have access to the summary statistics that make up the table.\nEquations from the lectures, which you can use to calculate the missing values. Those equations are also provided in the table below.\nThe treatment MS value for Planting_density is known at 152.96.\nThe overall standard deviation is 5.11 for all the data. That is, if we were to pool all the data together, the standard deviation would be 5.11.\nHint: the total mean sum of squares MS_{tot} is approximately the overall variance of the data.\n\n\n\n\n\n\n\nTip\n\n\n\nThe formulas for the ANOVA table are as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF value\n\n\n\n\nTreatment\nt - 1\nSS_{trt}\nMS_{trt} = SS_{trt} / df_{trt}\nMS_{trt}/MS_{res}\n\n\nResidual\nN - t\nSS_{res}\nMS_{res} = SS_{res} / df_{res}\n\n\n\nTotal\nN - 1\nSS_{tot}\nMS_{tot} = SS_{tot} / df_{tot}"
  },
  {
    "objectID": "tutorials/tutorial03.html#lets-begin",
    "href": "tutorials/tutorial03.html#lets-begin",
    "title": "Tutorial 03",
    "section": "2.3 Let’s begin!",
    "text": "2.3 Let’s begin!\n\n\n\n\n\n\nWarningSTEP 1: Degrees of freedom\n\n\n\n\n\nIt’s easy to fill in the degrees of freedom values for Treatment, Residual and Total, so let’s start with that. If: t = 3 N = 15 where t is the number of treatments and N is the total number of samples/replicates, then:\n\nTreatment df = t - 1 = 3 - 1 = 2\nResidual df = N - t = 15 - 3 = 12\nTotal df = N - 1 = 15 - 1 = 14\n\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF value\n\n\n\n\nTreatment\n\\mathbf{2}\nd\n152.96\ni\n\n\nResidual\n\\mathbf{12}\ne\nh\n\n\n\nTotal\n\\mathbf{14}\nf\ni\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTest your understanding 1\n\n\n\n\n\nWhat does the degrees of freedom tell us about the balance of the experiment?\nIf the sum of the degrees of freedom for the treatment and residual is equal to the total degrees of freedom, then the experiment is balanced. If not, then the experiment is unbalanced i.e. the number of replicates is not the same for each treatment.\n\n\n\n\n\n\n\n\n\nWarningSTEP 2: Total mean sum of squares MS_{tot}\n\n\n\n\n\nTo calculate total mean sum of squares MS_{tot}, we can use the overall standard deviation s = 5.11 to estimate the total variance, which is equivalent to the total mean square. We can calculate MS_{tot} by squaring the overall standard deviation s^2: MS_{tot} = s^2_{tot} = 5.11^2 = 26.1121\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF value\n\n\n\n\nTreatment\n2\nd\n152.96\ni\n\n\nResidual\n12\ne\nh\n\n\n\nTotal\n14\nf\n\\mathbf{26.11}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTest your understanding 2\n\n\n\n\n\nWhen you look at the total mean square value, what does it tell you about the “spread” of the data?\nThe total mean square is the variance of the data as a whole (i.e. not treatment-specific). It tells us how much the data varies from the overall mean. The larger the value, the more spread out the data is.\nA large value for MS_{tot} can be due to a large treatment effect or a large residual effect. The F-statistic will help us determine which one it is.\n\n\n\n\n\n\n\n\n\nWarningSTEP 3: Total SS_{tot} and Treatment SS_{trt} sum of squares\n\n\n\n\n\nSince we know that a mean sum of squares is calculated by dividing the sum of squares by the degrees of freedom i.e. MS = \\frac{SS}{df} then, rearranging the equation we have: SS = MS \\times df\nWe can calculate the total sum of squares SS_{tot} by multiplying the total mean square MS_{tot} by the total degrees of freedom df_{tot}: SS_{tot} = MS_{tot} \\times df_{tot} = 26.1121 \\times 14 = 365.5694\nThe same can be done for the treatment sum of squares SS_{trt}: SS_{trt} = MS_{trt} \\times df_{trt} = 152.96 \\times 2 = 305.92\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF value\n\n\n\n\nTreatment\n2\n\\mathbf{305.92}\n152.96\ni\n\n\nResidual\n12\ne\nh\n\n\n\nTotal\n14\n\\mathbf{365.60}\n26.11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTest your understanding 3\n\n\n\n\n\nWhat is the difference between the treatment and residual sum of squares?\nThe treatment sum of squares SS_{trt} is the variation between the treatment means and the overall mean. It tells us how much the treatment means differ from the overall mean.\nThe residual sum of squares SS_{res} is the variation between the individual observations and their treatment means. It tells us how much the individual observations differ from their treatment means.\nBy simply looking at the sum of squares, we can tell how much of the variation is due to the treatment and how much is due to the residual. The F-statistic will help us determine which one is more important.\n\n\n\n\n\n\n\n\n\nWarningSTEP 4: Residual SS_{res} and MS_{res}\n\n\n\n\n\nSince total sum of squares SS_{tot} is the sum of the treatment sum of squares SS_{trt} and the residual sum of squares SS_{res}, we can calculate the residual sum of squares SS_{res} by subtracting the treatment sum of squares SS_{trt} from the total sum of squares SS_{tot}: SS_{res} = SS_{tot} - SS_{trt} = 365.5694 - 305.92 = 59.6494\nWe can then calculate the residual mean square MS_{res} by dividing the residual sum of squares SS_{res} by the residual degrees of freedom df_{res}: MS_{res} = \\frac{SS_{res}}{df_{res}} = \\frac{59.6494}{12} = 4.970783\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF value\n\n\n\n\nTreatment\n2\n305.92\n152.96\ni\n\n\nResidual\n12\n\\mathbf{59.65}\n\\mathbf{4.97}\n\n\n\nTotal\n14\n365.60\n26.11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTest your understanding 4\n\n\n\n\n\nWhy do we need to calculate mean sums of squares MS if we already have the sum of squares SS?\nThe mean sum of squares MS is a measure of the variation in the data that is independent of the degrees of freedom i.e. standardised. This makes it easier to compare the variation between different treatments and the residual variation. The resulting F-statistic is therefore also independent of the degrees of freedom.\n\n\n\n\n\n\n\n\n\nWarningSTEP 5: F-Statistic\n\n\n\n\n\nFinally, we can calculate the F-statistic by dividing the treatment mean square MS_{trt} by the residual mean square MS_{res}: F = \\frac{MS_{trt}}{MS_{res}} = \\frac{152.96}{4.970783} = 30.7781\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF value\n\n\n\n\nTreatment\n2\n305.92\n152.96\n\\mathbf{30.78}\n\n\nResidual\n12\n59.65\n4.97\n\n\n\nTotal\n14\n365.60\n26.11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTest your understanding 5\n\n\n\n\n\nWhat does the F-statistic tell us?\nThe F-statistic is a measure of the variation between the treatment means and the residual variation. It tells us how much the treatment means differ from the overall mean relative to how much the individual observations differ from their treatment means. If the F-statistic is large (i.e. greater than 1), then the treatment means differ from the overall mean more than the individual observations differ from their treatment means. This means that the treatment means are significantly different from each other. The F-statistic is used to test the null hypothesis that the treatment means are equal."
  },
  {
    "objectID": "tutorials/tutorial03.html#final-anova-table",
    "href": "tutorials/tutorial03.html#final-anova-table",
    "title": "Tutorial 03",
    "section": "2.4 Final ANOVA table",
    "text": "2.4 Final ANOVA table\n\n\n\n\n\nSource\nDf\nSum Sq\nMean Sq\nF value\n\n\n\n\nTreatment\n2\n305.92\n152.96\n30.78\n\n\nResidual\n12\n59.65\n4.97\n\n\n\nTotal\n14\n365.60\n26.11"
  },
  {
    "objectID": "tutorials/tutorial03.html#bonus-doing-it-in-r",
    "href": "tutorials/tutorial03.html#bonus-doing-it-in-r",
    "title": "Tutorial 03",
    "section": "2.5 Bonus: doing it in R",
    "text": "2.5 Bonus: doing it in R\nIn code:\n\n# define known values\nN &lt;- 15 # total no. of samples/replicates\nt &lt;- 3 # no. of treatments\nms_trt &lt;- 152.96 # treatment MS\nsd_overall &lt;- 5.11\n \ndf_trt &lt;- t - 1    # a: treatment df\ndf_residual &lt;- N - t  # b: residual df\ndf_total &lt;- N - 1   # c: total df\n\nms_total &lt;- sd_overall^2    # h: total Mean Sq\nss_total &lt;- ms_total * df_total # f: total Sum Sq\n\nss_trt &lt;- ms_trt * df_trt    # d: treatment Sum Sq\n\nss_residual &lt;- ss_total - ss_trt    # e: residual Sum Sq\nms_residual &lt;- ss_residual/df_residual # g: residual Mean Sq \n\nf_statistic &lt;- ms_trt/ms_residual # i: F value\n\nIf we fill the table:\n\nknitr::kable(data.frame(\n Source = c(\"Treatment\", \"Residual\", \"Total\"),\n Df = c(df_trt, df_residual, df_total),\n `Sum Sq` = round(c(ss_trt, ss_residual, ss_total), 1),\n `Mean Sq` = round(c(ms_trt, ms_residual, ms_total), 1),\n `F value` = c(round(f_statistic, 1), \"\", \"\")\n))\n\n\n\n\nSource\nDf\nSum.Sq\nMean.Sq\nF.value\n\n\n\n\nTreatment\n2\n305.9\n153.0\n30.8\n\n\nResidual\n12\n59.6\n5.0\n\n\n\nTotal\n14\n365.6\n26.1"
  },
  {
    "objectID": "tutorials/tutorial03.html#do-you-reject-the-null-hypothesis",
    "href": "tutorials/tutorial03.html#do-you-reject-the-null-hypothesis",
    "title": "Tutorial 03",
    "section": "2.6 Do you reject the null hypothesis?",
    "text": "2.6 Do you reject the null hypothesis?\nTo determine whether we reject the null hypothesis, we need to compare the F-statistic to the Fcritical value. The Fcritical value is the value of F that we would expect to see if the null hypothesis were true. If the F-statistic is greater than the Fcritical value, then we reject the null hypothesis.\nTo obtain the Fcritical value, we need the degrees of freedom for the treatment and residual and the alpha level (usually 0.05). We then look up the Fcritical value in an F-distribution table or use the qf() function in R.\n\n\nUse this resource to help you learn more about the qf() function in R.\n\nqf(p = .05, # critical or alpha p value\n  df1 = 2,  # treatment degrees of freedom\n  df2 = 12,  # residual degrees of freedom\n  lower.tail = FALSE) # use FALSE for probability that F-stat &gt; F_crit\n\n[1] 3.885294\n\n\nSince our F-statistic is 30.78, which is higher than the Fcritical value of 3.89, we can say that for our data P &lt; 0.05 and we reject the null hypothesis that the treatment means are equal."
  },
  {
    "objectID": "tutorials/tutorial03.html#which-treatment-means-are-different",
    "href": "tutorials/tutorial03.html#which-treatment-means-are-different",
    "title": "Tutorial 03",
    "section": "2.7 Which treatment means are different?",
    "text": "2.7 Which treatment means are different?\nTo determine which treatment means are different, we can perform a post-hoc test such as Tukey’s HSD test. This test will tell us which treatment means are significantly different from each other.\nSince we do not have access to R (in our zombie apocalypse scenario!), we will not be able to perform the Tukey’s HSD test here. But can the original table give us a hint?\n\nknitr::kable(maize)\n\n\n\n\nTreatment\nMean\nStd.Dev\nVariance\n\n\n\n\n20 plants per unit area\n17.58\n2.70\n7.29\n\n\n30 plants per unit area\n27.18\n1.89\n3.577\n\n\n40 plants per unit area\n27.14\n2.02\n4.093\n\n\nOverall\n23.97\n5.11\n-\n\n\n\n\n\nFrom the table above we can see that the mean kg of dry matter per plot for the 30 and 40 plants per unit area are very similar. This suggests that the treatment means for 30 and 40 plants per unit area are not significantly different from each other. The treatment mean for 20 plants per unit area is probably significantly different from the other two treatments.\nBut beware! This is only a hint and not a definitive answer. We need to perform a post-hoc test to be sure. This will be covered in the labs!"
  },
  {
    "objectID": "tutorials/tutorial01.html",
    "href": "tutorials/tutorial01.html",
    "title": "Tutorial 01",
    "section": "",
    "text": "Welcome to Tutorial 1. In this tutorial we will show you how to:\n\nCreate an RStudio Project.\nCreate a Quarto document and customise it.\nFormat text and explore some advanced Quarto options."
  },
  {
    "objectID": "tutorials/tutorial01.html#welcome",
    "href": "tutorials/tutorial01.html#welcome",
    "title": "Tutorial 01",
    "section": "",
    "text": "Welcome to Tutorial 1. In this tutorial we will show you how to:\n\nCreate an RStudio Project.\nCreate a Quarto document and customise it.\nFormat text and explore some advanced Quarto options."
  },
  {
    "objectID": "tutorials/tutorial01.html#getting-started",
    "href": "tutorials/tutorial01.html#getting-started",
    "title": "Tutorial 01",
    "section": "0.1 Getting started",
    "text": "0.1 Getting started\n\n\nIn some instances you will see notes like this one in the margin, which often contain useful tips or reminders.\n\nThese tutorials are designed to be comprehensive yet accessible. I recommend following along and completing the exercises as you go.\nYou will need to create your own Quarto document to complete the exercises. In RStudio, go to\n\nFile -&gt; New File -&gt; Quarto Document... or\nFile -&gt; New File -&gt; R Markdown... and follow the intuitive prompts.\n\nIf you have any suggestions for improvement, please let me know! Send me an email (you can find my details on Canvas)."
  },
  {
    "objectID": "tutorials/tutorial01.html#why-use-projects",
    "href": "tutorials/tutorial01.html#why-use-projects",
    "title": "Tutorial 01",
    "section": "1.1 Why Use Projects?",
    "text": "1.1 Why Use Projects?\nRStudio Projects help with organisation by keeping all project files together in one directory. They also ensure reproducibility by making your code work consistently across different computers. Think of them as a way to “tell” RStudio that all your work for a particular project is in one place.\nAt its heart, an RStudio Project is simply a text file with the .Rproj extension. This file lives in your folder that you’re working in, and tells RStudio to treat that folder as a project. Once created, RStudio manages the project for you and you never have to edit the .Rproj file directly."
  },
  {
    "objectID": "tutorials/tutorial01.html#setting-up-a-project",
    "href": "tutorials/tutorial01.html#setting-up-a-project",
    "title": "Tutorial 01",
    "section": "1.2 Setting up a Project",
    "text": "1.2 Setting up a Project\nLet’s set up a new project for ENVX2001 in RStudio.\n\nOpen RStudio\nClick File -&gt; New Project…\nSelect either New Directory or Existing Directory\n\nNew Directory creates a new folder for your project\nExisting Directory lets you choose a folder that already exists\n\nSelect New Project unless you have a specific project you want to work on (e.g. a Quarto Blog)\nName your project “ENVX2001” or similar, choose where to save it, and click Create Project. You will see other options, but you can ignore them for now.\n\n\n\nGive your project a meaningful name, ideally with “ENVX2001” in it so you can identify it later. For example “ENVX2001 Labs”."
  },
  {
    "objectID": "tutorials/tutorial01.html#recommended-folder-structure",
    "href": "tutorials/tutorial01.html#recommended-folder-structure",
    "title": "Tutorial 01",
    "section": "1.3 Recommended folder structure",
    "text": "1.3 Recommended folder structure\nModern technology means that some of you may never have to touch the file system. However, it’s still good to know how to organise your files. Search the internet for using the Finder (macOS) or the File Explorer (Windows) if you need a refresher.\nHere’s a recommended folder structure for your project:\nENVX2001/\n├── labs/\n│   ├── data/\n│   ├── Lab01.qmd\n│   ├── Lab02.qmd\n│   ├── ...\n│   └── ENVX2001-labs.Rproj\n├── project_1/\n│   ├── data/\n│   ├── project1.qmd\n│   └── ENVX2001-labs.Rproj\n├── project_2/\n│   ├── data/\n│   └── ...\n└── ..."
  },
  {
    "objectID": "tutorials/tutorial01.html#opening-projects",
    "href": "tutorials/tutorial01.html#opening-projects",
    "title": "Tutorial 01",
    "section": "1.4 Opening Projects",
    "text": "1.4 Opening Projects\n\nFrom File Explorer (PC) or Finder (Mac)\n\nNavigate to your project folder\nDouble-click the .Rproj file\n\nOn Windows: Look for the RStudio cube icon\nOn Mac: Look for the .Rproj extension\n\n\n\n\nFrom Within RStudio\n\nManually:\n\nClick File -&gt; Open Project...\nNavigate to your project folder\nSelect the .Rproj file\nClick “Open”\n\n\n\nRecent projects:\nIf you’ve opened the project before, your project may already be open! Check the top-right corner of RStudio – if you see your project name, you’re good to go. Click on the dropdown to switch to recently opened projects."
  },
  {
    "objectID": "tutorials/tutorial01.html#benefits",
    "href": "tutorials/tutorial01.html#benefits",
    "title": "Tutorial 01",
    "section": "1.5 Benefits",
    "text": "1.5 Benefits\nWhen in a project, RStudio will remember your working directory, history, and environment. This means you can close RStudio and come back to your project later, and everything will be as you left it. This is especially useful when working on a project over multiple days. Additionally, adding data files to your project folder will make them easier to access in your code as you can use relative paths. For example, instead of using:\nread.csv(\"C:/Users/username/Documents/ENVX2001/data.csv\")\nyou can use:\nread.csv(\"data/data.csv\")\nThe latter is more portable and will work on any computer, as long as the data file is in the data folder, while the former will only work on your computer as it would have your user name on it!"
  },
  {
    "objectID": "tutorials/tutorial01.html#whats-next",
    "href": "tutorials/tutorial01.html#whats-next",
    "title": "Tutorial 01",
    "section": "1.6 What’s next?",
    "text": "1.6 What’s next?\nSet up an RStudio project separately for your labs and projects. This will make it easier to keep your work organised and reproducible. Otherwise, there isn’t much else to think about as RStudio will manage the project for you. Just remember to open your project when you start working on it!"
  },
  {
    "objectID": "tutorials/tutorial01.html#getting-started-1",
    "href": "tutorials/tutorial01.html#getting-started-1",
    "title": "Tutorial 01",
    "section": "2.1 Getting Started",
    "text": "2.1 Getting Started\nLet’s create and modify a Quarto document about The Beatles. Follow these steps to learn about Quarto’s features:\n\nIn RStudio, go to File &gt; New File &gt; Quarto Document…\nIn the dialogue box that appears:\n\nEnter “The Beatles: A Musical Journey” as the title\nAdd your name as the author\nSelect “HTML” as the output format\n\nClick “Create”"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-1-setting-up-your-document",
    "href": "tutorials/tutorial01.html#exercise-1-setting-up-your-document",
    "title": "Tutorial 01",
    "section": "2.2 Exercise 1: Setting Up Your Document",
    "text": "2.2 Exercise 1: Setting Up Your Document\n\nDelete all the example content in your new document (but keep the YAML header)\nReplace your YAML header with this enhanced version:\n\n---\ntitle: \"The Beatles: A Musical Journey\"\nsubtitle: \"From Liverpool to Global Stardom\"\nauthor: \"Your Name\"\ndate: \"today\"\nformat: \n  html:\n    toc: true\n    number-sections: true\n    theme: cosmo\n---\n\nClick “Render” to see what an empty document looks like"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-2-adding-content",
    "href": "tutorials/tutorial01.html#exercise-2-adding-content",
    "title": "Tutorial 01",
    "section": "2.3 Exercise 2: Adding Content",
    "text": "2.3 Exercise 2: Adding Content\nCopy and paste this text after your YAML header:\n## The Early Years\n\nThe Beatles' story begins in Liverpool, England, where John Lennon formed a band called the Quarrymen in 1957. Paul McCartney joined soon after, followed by George Harrison in 1958. The band went through several name changes before settling on \"The Beatles\" in 1960.\n\n## Hamburg and The Cavern Club\n\n### The Hamburg Experience\n\nIn 1960, the Beatles began playing in Hamburg, Germany. These extended performances helped them develop their stage presence and musical style. During this period, they recruited Ringo Starr as their drummer, completing the lineup that would become legendary.\n\n### The Cavern Club Era\n\nBack in Liverpool, the Beatles became regular performers at The Cavern Club. Between 1961 and 1963, they performed there 292 times. The club became synonymous with their early success and the birth of the \"Merseybeat\" sound.\n\n## Global Phenomenon\n\nBy 1964, \"Beatlemania\" had taken hold worldwide. Their appearance on The Ed Sullivan Show in February 1964 was watched by an estimated 73 million viewers. This marked the beginning of the \"British Invasion\" and revolutionised popular music.\n\n## Musical Evolution\n\nThe Beatles continuously evolved their musical style throughout their career:\n\n1. Early rock and roll (1962-1964)\n2. Folk rock period (1965-1966)\n3. Psychedelic era (1966-1968)\n4. Return to roots (1969-1970)\n\nTheir willingness to experiment with different musical styles and recording techniques set new standards for popular music.\n\nClick “Render” again to see the formatted document"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-3-experimenting-with-yaml-options",
    "href": "tutorials/tutorial01.html#exercise-3-experimenting-with-yaml-options",
    "title": "Tutorial 01",
    "section": "2.4 Exercise 3: Experimenting with YAML Options",
    "text": "2.4 Exercise 3: Experimenting with YAML Options\nTry these modifications to your YAML header one at a time, rendering after each change:\n\nChange the theme:\n\ntheme: darkly  # Try: flatly, journal, darkly\n\nAdd a table of contents:\n\ntoc: true\n\nAdd section numbers:\n\nnumber-sections: true\n\nTry them all together:\n\nformat:\n  html:\n    toc: true\n    theme: cosmo\n    number-sections: true"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-4-try-different-output-formats",
    "href": "tutorials/tutorial01.html#exercise-4-try-different-output-formats",
    "title": "Tutorial 01",
    "section": "2.5 Exercise 4: Try Different Output Formats",
    "text": "2.5 Exercise 4: Try Different Output Formats\n\nMicrosoft Word\nTry creating a Word document by changing your YAML to:\nformat:\n  docx:\n    toc: true\n    number-sections: true\nYou can learn more about Word output options in the Quarto Word documentation.\n\n\nTypst (Advanced Option)\nTypst is a modern alternative to LaTeX/PDF, but it requires some learning to customise effectively. If you’re interested in exploring it:\nformat:\n  typst:\n    toc: true\nLearn more about Typst in: - Quarto’s Typst documentation - Official Typst documentation\n\n\nMultiple Output Formats\nYou can create multiple output formats simultaneously by specifying them in your YAML header:\nformat:\n  html:\n    toc: true\n    number-sections: true\n    theme: cosmo\n  docx:\n    toc: true\n    number-sections: true\nEach format can have its own specific options. For example, you could have different themes for HTML while maintaining consistent structure across all formats."
  },
  {
    "objectID": "tutorials/tutorial01.html#quick-tips",
    "href": "tutorials/tutorial01.html#quick-tips",
    "title": "Tutorial 01",
    "section": "2.6 Quick Tips",
    "text": "2.6 Quick Tips\n\nUse keyboard shortcut to render:\n\nWindows/Linux: Ctrl+Shift+K\nMac: Cmd+Shift+K\n\nPreview different themes from the complete list here\nKeep experimenting with different YAML options to see what works best for your documents"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-1-markdown",
    "href": "tutorials/tutorial01.html#exercise-1-markdown",
    "title": "Tutorial 01",
    "section": "3.1 Exercise 1: Markdown",
    "text": "3.1 Exercise 1: Markdown\nFirst, let’s practice some basic markdown formatting.\n\n\n\n\n\n\nNoteReplicate this\n\n\n\nHere’s a brief overview of Christopher Nolan’s filmmaking:\n\nHis film Inception (2010) took ten years to write\nHe wrote Memento based on two key concepts:\n\nAnterograde amnesia\nNon-linear storytelling\n\nHis highest-grossing film is The Dark Knigh Rises (2012), which earned over $1 billion worldwide.\n\nHis signature style: complex narratives with time manipulation as seen in Interstellar and Tenet.\nVisit IMDB’s Nolan Page for his complete filmography.\n\n\nTry to replicate the text and formatting above, including the link to IMDB, in your own document, using markdown syntax. For reference, check out the Quarto Markdown Basics."
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-2-working-with-code-chunks",
    "href": "tutorials/tutorial01.html#exercise-2-working-with-code-chunks",
    "title": "Tutorial 01",
    "section": "3.2 Exercise 2: Working with code chunks",
    "text": "3.2 Exercise 2: Working with code chunks\nNow let’s focus on code. We will work with data from the palmerpenguins package to visualise and summarise some data. Try running the following code chunks in your document:\n```{r}\n#| label: setup\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n```\n\n\nNever run package installation code (install.packages()) in your Quarto document! Instead, install required packages in the R console:\ninstall.packages(c(\"tidyverse\", \"palmerpenguins\"))\nThis ensures your document remains reproducible and prevents installation errors during rendering.\nLet’s break down the chunk options:\n\nlabel: gives your chunk a unique name\nmessage: false suppresses package loading messages\nwarning: false suppresses warnings\nThe #| syntax makes options more readable"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-3-creating-visualizations",
    "href": "tutorials/tutorial01.html#exercise-3-creating-visualizations",
    "title": "Tutorial 01",
    "section": "3.3 Exercise 3: Creating Visualizations",
    "text": "3.3 Exercise 3: Creating Visualizations\nNow let’s create some plots with carefully controlled output:\n```{r}\n#| label: penguin-plot\n#| fig-cap: \"Penguin body mass by species\"\n#| fig-width: 8\n#| fig-height: 6\n#| fig-align: center\n#| echo: true\n\nggplot(penguins, aes(species, body_mass_g, fill = species)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(y = \"Body Mass (g)\",\n       x = \"Species\") +\n  theme(legend.position = \"none\")\n```\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nPenguin body mass by species\n\n\n\n\nNew options to notice:\n\nfig-width and fig-height control plot dimensions\nfig-align centers the plot\necho: true shows the code (try changing to false)"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-4-tables-and-data",
    "href": "tutorials/tutorial01.html#exercise-4-tables-and-data",
    "title": "Tutorial 01",
    "section": "3.4 Exercise 4: Tables and data",
    "text": "3.4 Exercise 4: Tables and data\nLet’s explore different ways to display data:\n```{r}\n#| label: penguin-summary\n#| tbl-cap: \"Summary Statistics by Species\"\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    mean_mass = mean(body_mass_g, na.rm = TRUE),\n    sd_mass = sd(body_mass_g, na.rm = TRUE),\n    n = n()\n  ) %&gt;%\n  knitr::kable()\n```\n\n\n\nSummary Statistics by Species\n\n\nspecies\nmean_mass\nsd_mass\nn\n\n\n\n\nAdelie\n3700.662\n458.5661\n152\n\n\nChinstrap\n3733.088\n384.3351\n68\n\n\nGentoo\n5076.016\n504.1162\n124"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-5-interactive-features",
    "href": "tutorials/tutorial01.html#exercise-5-interactive-features",
    "title": "Tutorial 01",
    "section": "3.5 Exercise 5: Interactive features",
    "text": "3.5 Exercise 5: Interactive features\nQuarto excels at creating interactive content. Try this:\n```{r}\n#| label: interactive-plot\n#| warning: false\n\nlibrary(plotly)\np &lt;- ggplot(penguins, \n            aes(flipper_length_mm, \n                body_mass_g, \n                color = species)) +\n  geom_point() +\n  theme_minimal()\n\nggplotly(p)\n```\n\n\n\n\n\n\nAnd for interactive tables:\n```{r}\n#| label: interactive-table\n\nlibrary(DT)\ndatatable(penguins)\n```"
  },
  {
    "objectID": "tutorials/tutorial01.html#exercise-6-useful-chunk-options",
    "href": "tutorials/tutorial01.html#exercise-6-useful-chunk-options",
    "title": "Tutorial 01",
    "section": "3.6 Exercise 6: Useful chunk options",
    "text": "3.6 Exercise 6: Useful chunk options\nHere are some useful chunk options, try them:\n\nCache computations (great for slow calculations):\n\n```{r}\n#| label: slow-calculation\n#| cache: true\n\n# Your time-consuming code here\nSys.sleep(2)  # Simulating slow computation\n```\n\nRun code but hide everything:\n\n```{r}\n#| label: hidden-setup\n#| include: false\n\n# This code runs but shows nothing\ntheme_set(theme_minimal())\n```\n\nShow code but don’t run it:\n\n```{r}\n#| label: example-code\n#| eval: false\n\n# This code is displayed but not executed\nggplot(data) +\n  geom_point()\n```\nTry changing the values for options like echo, include, and others from true to false (or vice versa). See what happens."
  },
  {
    "objectID": "tutorials/tutorial01.html#quick-tips-1",
    "href": "tutorials/tutorial01.html#quick-tips-1",
    "title": "Tutorial 01",
    "section": "3.7 Quick tips",
    "text": "3.7 Quick tips\n\nUse meaningful chunk labels - they help with debugging\nKeep chunks focused on single tasks\nCache slow computations with cache: true\nUse include: false for setup code you don’t want to show"
  },
  {
    "objectID": "tutorials/tutorial01.html#next-steps",
    "href": "tutorials/tutorial01.html#next-steps",
    "title": "Tutorial 01",
    "section": "3.8 Next Steps",
    "text": "3.8 Next Steps\nThat’s it! Well done. The Quarto documentation is a great resource for more advanced features, but beware – it’s a rabbit hole…"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#workflow",
    "href": "lectures/L09/Lecture-09.html#workflow",
    "title": "Regression: predictive modelling",
    "section": "Workflow",
    "text": "Workflow\n\nModel development\n\nExplore: visualise, summarise\nTransform predictors: linearise, reduce skewness/leverage\nModel: fit, check assumptions, interpret, transform. Repeat.\n\nVariable selection\n\nVIF: remove predictors with high variance inflation factor\nModel selection: stepwise selection, AIC, principle of parsimony, assumption checks\n\nPredictive modelling\n\nPredict: Use the model to predict new data\nValidate: Evaluate the model’s performance"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#previously-on-envx2001",
    "href": "lectures/L09/Lecture-09.html#previously-on-envx2001",
    "title": "Regression: predictive modelling",
    "section": "Previously on ENVX2001…",
    "text": "Previously on ENVX2001…\nWe fitted a multiple linear regression model to the data.\n\nfit &lt;- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind\\]"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#predictions-by-hand",
    "href": "lectures/L09/Lecture-09.html#predictions-by-hand",
    "title": "Regression: predictive modelling",
    "section": "Predictions by hand",
    "text": "Predictions by hand\n\\[ \\widehat{log(Ozone)}=-0.262 + \\color{darkorchid}{0.0492} \\cdot Temp + \\color{darkorange}{0.00252} \\cdot Solar.R - \\color{seagreen}{0.0616} \\cdot Wind \\]\nOn a certain day, we measured (in imperial units):\n\ntemperature Temp to be 80 degrees Fahrenheit\nsolar radiation Solar.R to be 145 units (Langleys)\nwind speed Wind to be 10.9 miles per hour\n\nWhat is the predicted ozone level?\n\\[\\widehat{log(Ozone)}= -0.262 + \\color{darkorchid}{0.0492 \\cdot 80} + \\color{darkorange}{0.00252 \\cdot 145} - \\color{seagreen}{0.0616 \\cdot 10.9}\\]\nEasy! The two things we need to think about are…\n\nWhat is the uncertainty in this prediction?\nCan this model be used to predict ozone if we collect new data in the future?"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#uncertainty",
    "href": "lectures/L09/Lecture-09.html#uncertainty",
    "title": "Regression: predictive modelling",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nConfidence interval: uncertainty in the mean response at a given predictor value.\nPrediction interval: uncertainty in a single response at a given predictor value.\n\nWhat it means\n\n95% confidence interval: Given the parameters of the model, we are 95% confident that the mean response at a given predictor value is between \\(y_1\\) and \\(y_2\\).\n\n\n95% prediction interval: Given the parameters of the model, we are 95% confident that a single response at a given predictor value is between \\(y_1\\) and \\(y_2\\)."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#equations",
    "href": "lectures/L09/Lecture-09.html#equations",
    "title": "Regression: predictive modelling",
    "section": "Equations",
    "text": "Equations\nThe equation to calculate any prediction interval is:\n\\[ \\widehat{y} \\pm t_{\\alpha/2} \\cdot se(\\widehat{y}) \\]\nwhere:\n\n\\(\\widehat{y}\\) is the predicted value\n\\(t_{\\alpha/2}\\) is the critical value of the t-distribution for a given confidence level\n\\(se(\\widehat{y})\\) is the standard error of the prediction\n\nThe difference in calculating a confidence interval and a prediction interval is in the standard error of the prediction."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#equations-1",
    "href": "lectures/L09/Lecture-09.html#equations-1",
    "title": "Regression: predictive modelling",
    "section": "Equations",
    "text": "Equations\nCI: standard error of the fit\n\\[ se(\\widehat{y}) = \\sqrt{MSE \\cdot \\left( \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)} \\]\nPI: standard error of the prediction\n\\[ se(\\widehat{y}) = \\sqrt{MSE \\cdot \\left( 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)}\\]\n\n\\(x_0\\) is value of the predictor for which we want a response\n\\(MSE\\) is the mean squared error of the fit (\\(SS_{xx}\\))\n\\(\\sum_{i=1}^n (x_i - \\bar{x})^2\\) is the sum of squares of the predictor values\n\\(n\\) is the number of observations\n\\(\\bar{x}\\) is the mean of the predictor values\n\nThe prediction interval formula has an additional term (\\(1 \\cdot MSE\\)). There is uncertainty that the mean prediction will be similar to the observed, and additional uncertainty/variability for a single response (equivalent to the MSE). Thus the confidence interval is always narrower than the prediction interval."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#predictions-in-r",
    "href": "lectures/L09/Lecture-09.html#predictions-in-r",
    "title": "Regression: predictive modelling",
    "section": "Predictions in R",
    "text": "Predictions in R\n\nFirst, we need to create a new data frame with the predictor values we want to predict at – it must include all variables in the fitted model.\n\n\npredict_df &lt;- data.frame(Temp = 80, Solar.R = 145, Wind = 10.9)\n\n\nWe use the predict() function to obtain the predicted value.\nSpecifying interval = \"confidence\" or interval = \"prediction\" also calculates the confidence or prediction interval.\n\n\npredict(fit, newdata = predict_df) # the predicted value\n\n       1 \n3.365227 \n\npredict(fit, newdata = predict_df, interval = \"confidence\") # predicted value and CI\n\n       fit      lwr      upr\n1 3.365227 3.246265 3.484189\n\npredict(fit, newdata = predict_df, interval = \"prediction\") # predicted value and PI\n\n       fit      lwr      upr\n1 3.365227 2.350051 4.380404"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#visualising-ci-and-pi",
    "href": "lectures/L09/Lecture-09.html#visualising-ci-and-pi",
    "title": "Regression: predictive modelling",
    "section": "Visualising CI and PI",
    "text": "Visualising CI and PI\n\nairquality$pred &lt;- predict(fit, newdata = airquality) # predict for existing data\n\npreds_ci &lt;- predict(fit, newdata = airquality, interval = \"confidence\") # confidence interval\npreds_pi &lt;- predict(fit, newdata = airquality, interval = \"prediction\") # prediction interval\n\n\nWe can now plot the CI and PI as shaded areas around the predicted line\n\n\n\nCode\np &lt;-\n  ggplot(airquality, aes(pred, log(Ozone))) +\n  geom_point() + \n  geom_line(data = preds_ci, aes(fit, lwr), color = \"blue\") +\n  geom_line(data = preds_ci, aes(fit, upr), color = \"blue\") +\n  geom_line(data = preds_pi, aes(fit, lwr), color = \"red\") +\n  geom_line(data = preds_pi, aes(fit, upr), color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\np"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#visualising-with-geom_smooth",
    "href": "lectures/L09/Lecture-09.html#visualising-with-geom_smooth",
    "title": "Regression: predictive modelling",
    "section": "Visualising with geom_smooth()",
    "text": "Visualising with geom_smooth()\n\n\n\ngeom_smooth() fits a linear model to obtain a smooth line\nFor visualisation we can use geom_smooth() instead of geom_line() to fit smoothed lines\n\n\n\nCode\np &lt;-\n  ggplot(airquality, aes(pred, log(Ozone))) +\n  geom_point() + \n  geom_smooth(data = preds_ci, aes(fit, lwr), color = \"blue\", se = F) +\n  geom_smooth(data = preds_ci, aes(fit, upr), color = \"blue\", se = F) +\n  geom_smooth(data = preds_pi, aes(fit, lwr), color = \"red\", se = F) +\n  geom_smooth(data = preds_pi, aes(fit, upr), color = \"red\", se = F) +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\np\n\n\n\n\n\n\n\n\n\n\n\nThe hyperparameter se = TRUE fits a smoothed CI around the predicted line\nSmoothed with loess functions\nCannot fit the prediction interval\n\n\n\nCode\np + geom_smooth(method = \"lm\", se = TRUE)"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#why-separate-calibration-and-validation-data",
    "href": "lectures/L09/Lecture-09.html#why-separate-calibration-and-validation-data",
    "title": "Regression: predictive modelling",
    "section": "Why separate calibration and validation data?",
    "text": "Why separate calibration and validation data?\nIn model development we try many models and choose the one that fits that specific dataset very well.\nSo our model may be too complex and overfits the data. If we predict onto new data (in the real world) the model does not give plausible predictions.\nWhy might this be a problem?\n\nPredict the wrong ozone levels (and people with respiratory issues are not warned)\nPredict the wrong disease numbers (and local health services are not prepared)\nPredict the wrong crop yield (and farmers under/overapply fertiliser)\n\nPredictions can directly be used for decision-making, which has consequences."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#general-idea",
    "href": "lectures/L09/Lecture-09.html#general-idea",
    "title": "Regression: predictive modelling",
    "section": "General Idea",
    "text": "General Idea\nWhat if we want to know how well the model predicts new data, i.e. data that we did not use to fit the model?\n\nWe build the model with one dataset (calibration).\nWe validate the model’s predictions with an independent dataset.\n\nIf the model is good, we expect the predictions to be close to the actual values.\nIf the model is bad, we expect the predictions to be far from the actual values.\n\nThe dataset can be obtained by:\n\nCollecting new data.\nSplitting the existing data into two parts before model building.\n\nData splitting\nCross-validation"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#definitions",
    "href": "lectures/L09/Lecture-09.html#definitions",
    "title": "Regression: predictive modelling",
    "section": "Definitions",
    "text": "Definitions\nSometimes the terms for calibration and validation can get muddled.\nBest practice:\n\nCalibration/Training Dataset: the data used to train the model\nValidation: the data used to fine tune the model (e.g. variable selection, hyperparameters in machine learning)\nTest: remaining data that has not been used in any kind of model training\n\nTo keep things simple (and if datasets are small), also common:\n\nCalibration/Training: the data used to train the model\nValidation/Test: the data used to assess the model’s prediction performance"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#our-data",
    "href": "lectures/L09/Lecture-09.html#our-data",
    "title": "Regression: predictive modelling",
    "section": "Our data",
    "text": "Our data\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#collecting-new-data",
    "href": "lectures/L09/Lecture-09.html#collecting-new-data",
    "title": "Regression: predictive modelling",
    "section": "Collecting new data",
    "text": "Collecting new data\n\n\n\n\nDataset (train)\n\\(+\\)\n\n\n\n\nNew dataset (test)\n\n\nThe best way to assess how well a model predicts new data is to collect new data.\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#collecting-new-data-1",
    "href": "lectures/L09/Lecture-09.html#collecting-new-data-1",
    "title": "Regression: predictive modelling",
    "section": "Collecting new data",
    "text": "Collecting new data\nPros\n\nThe new data is completely independent of the data used to fit the model.\nMore data to fit and validate compared to data splitting.\n\nCons\n\nIt can be expensive and time-consuming to collect new data.\nSome data may be impossible to collect (e.g. historical data)."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#data-splitting",
    "href": "lectures/L09/Lecture-09.html#data-splitting",
    "title": "Regression: predictive modelling",
    "section": "Data splitting",
    "text": "Data splitting\n\n\n\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#data-splitting-1",
    "href": "lectures/L09/Lecture-09.html#data-splitting-1",
    "title": "Regression: predictive modelling",
    "section": "Data splitting",
    "text": "Data splitting\n\n\n\n\n\n\n\n\nDataset\n(Training)\nSubset (Test)"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#data-splitting-2",
    "href": "lectures/L09/Lecture-09.html#data-splitting-2",
    "title": "Regression: predictive modelling",
    "section": "Data splitting",
    "text": "Data splitting\n\n\n\n\n\n\n\n\nDataset\n(Training)\nSubset (Test)\n\n\nSplit the existing dataset into training and test datasets (80:20, 70:30, 60:40, etc.)\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#data-splitting-3",
    "href": "lectures/L09/Lecture-09.html#data-splitting-3",
    "title": "Regression: predictive modelling",
    "section": "Data splitting",
    "text": "Data splitting\n\n\n\n\n\n\n\n\nDataset\n(Training)\nSubset (Test)\n\n\nOnly possible for larger datasets (hundreds of observations)\nSplit the existing dataset into calibration, validation and test datasets (70:15:15, etc.)\n\nCalibration set: used to fit the model.\nValidation set: used to test model development (prevent overfitting).\nTest set: used to assess how well the model predicts new data."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#data-splitting-4",
    "href": "lectures/L09/Lecture-09.html#data-splitting-4",
    "title": "Regression: predictive modelling",
    "section": "Data splitting",
    "text": "Data splitting\nPros\n\nCompared to collecting new data, it is cheaper and faster to split existing data.\n\nCons\n\nWe have less data to fit the model and less data to validate the model.\nHow do we split the data? Randomly? By time? By location?"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-1",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-1",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-2",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-2",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-3",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-3",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1\nIteration 2\nIteration 3\nAnd so on…"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-4",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-4",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 1 Iteration 2 Iteration 3 3-fold cross-validation Fold 1 Fold 2 Fold 3\n\n\nLike data splitting, where existing data is split into two parts:\n\nTraining set: used to fit the model.\nTest set: used to assess how well the model predicts new data.\n\nThe difference is that the splitting is done multiple times, and the model is fit and validated multiple times.\nEach iteration or fold is used for testing once."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#k-fold-cross-validation-5",
    "href": "lectures/L09/Lecture-09.html#k-fold-cross-validation-5",
    "title": "Regression: predictive modelling",
    "section": "k-fold cross-validation",
    "text": "k-fold cross-validation\nPros\n\nSame as data splitting, but also:\n\nThe model is fit and validated multiple times, so we can get a better estimate of how well the model predicts new data.\nGreatly reduces overfitting as the model’s performance is not just a result of the particular way the data was split.\n\n\nCons\n\nBias in small datasets: each fold may contain too little data to provide a representative sample.\nEach fold fits a new model so it is not used for interpretation, only for prediction quality.\n\nComputationally more expensive."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#cross-validation",
    "href": "lectures/L09/Lecture-09.html#cross-validation",
    "title": "Regression: predictive modelling",
    "section": "Cross-validation",
    "text": "Cross-validation\nk-fold cross-validation splits data in each fold randomly.\nIf there is some underlying structure to the data, consider:\n\nSpatial cross-validation (e.g. fields on a farm, each fold is one field)\nTemporal cross-validation (e.g. time series data, each fold is a time period)\nStratified cross-validation (e.g. each fold has the same proportion of each category)"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#visually",
    "href": "lectures/L09/Lecture-09.html#visually",
    "title": "Regression: predictive modelling",
    "section": "Visually",
    "text": "Visually\n\nPlot: observed (\\(y_i\\)) vs predicted (\\(\\hat y\\)) values\n\n\nggplot(data = airquality, aes(x = log(Ozone), y = pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed log(Ozone)\", y = \"Predicted log(Ozone)\") +\n  theme_classic()\n\n\nWe can also use metrics to quantify how well the model predicts new data:\n\nHow close points are to the 1:1 line\nHow linear the relationship is between observed and predicted values"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#error",
    "href": "lectures/L09/Lecture-09.html#error",
    "title": "Regression: predictive modelling",
    "section": "Error",
    "text": "Error\nThe smaller the error, the better the model.\nMean error: the average difference between observed and predicted values.\n\nCan be positive or negative to indicate over- or under-estimation (a measure of bias)\n\n\\[ME = \\frac{1}{n} \\sum_{i=1}^{n} y_i - \\hat{y}_i\\] (in \\(y\\) units)\nMean absolute error: the average (absolute) difference between observed and predicted values (residual).\n\\[MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\] (in \\(y\\) units)\nMean squared error: the average of the squared residuals\n\nSquared so positive and negative errors do not cancel each other out\nPenalises poor predictions more\n\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nRoot mean squared error: the standard deviation of the residuals\n\nSquaring the error penalises poor predictions more\n\n\\[RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\] (in \\(y\\) units)"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#linearity",
    "href": "lectures/L09/Lecture-09.html#linearity",
    "title": "Regression: predictive modelling",
    "section": "Linearity",
    "text": "Linearity\nThe more linear the relationship between observed and predicted values, the better the model.\nPearson’s correlation coefficient (r):\n\nA measure of the strength and direction of a linear relationship between two variables.\nRanges from -1 to 1, with 0 indicating no relationship and 1 indicating a perfect positive linear relationship.\n\n\\[r = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(\\hat{y}_i - \\bar{\\hat{y}})}{\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{\\hat{y}})^2}}\\]\nR2:\n\nThe proportion of variance explained by the variables in the model.\nWhen two variables are compared (e.g. observed vs predicted) it is the same as correlation squared.\nA value of 1 indicates a perfect linear relationship.\n\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] Lin’s concordance correlation coefficient (\\(\\rho_c\\)):\n\nA measure of agreement between two variables (based on covariance, variances, and difference in means).\nHow well the points fit the 1:1 line.\nA value of 0 indicates no agreement and 1 indicating perfect agreement.\n\n\\[LCCC = \\frac{2\\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) + (\\mu_X - \\mu_Y)^2}\\]"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#lccc-vs-r-and-r2",
    "href": "lectures/L09/Lecture-09.html#lccc-vs-r-and-r2",
    "title": "Regression: predictive modelling",
    "section": "LCCC vs \\(r\\) and R2",
    "text": "LCCC vs \\(r\\) and R2\nLCCC combines error and linearity so it better measures the fit to the 1:1 line.\n\n\nCode\ndf &lt;- tibble(y = seq(0, 100, 5),\n  \"45 degree line | CCC = 1\" = seq(0, 100, 5)) %&gt;%\n  mutate(\"Location shift | CCC = 0.89\" = `45 degree line | CCC = 1` - 15) %&gt;%\n  mutate(\"Scale shift | CCC = 0.52\" = y / 2) %&gt;%\n  mutate(\"Location and scale shift | CCC = 0.67\" = y * 2 - 20)\n\n# pivot\ndf_long &lt;- df %&gt;%\n  pivot_longer(-1, values_to = \"x\") %&gt;%\n  mutate(name = factor(name, \n    levels = c(\"45 degree line | CCC = 1\",\n      \"Location shift | CCC = 0.89\",\n      \"Scale shift | CCC = 0.52\",\n      \"Location and scale shift | CCC = 0.67\")))\n\nggplot(df_long, aes(x, y)) +\n  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = \"grey\") +\n  facet_wrap(~name) +\n  geom_point() +\n  xlim(0, 100) +\n  labs(x = \"\", y = \"\") +\n  theme_classic() +\n  geom_blank()"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#limitations",
    "href": "lectures/L09/Lecture-09.html#limitations",
    "title": "Regression: predictive modelling",
    "section": "Limitations",
    "text": "Limitations\nNo one metric is perfect.\nEach prediction below has an LCCC of 0.6.\nWadoux and Minasny 2024 \n\n\n\n\n\n\nTip\n\n\nUse multiple metrics to test prediction quality, and always plot the predicted vs observed."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#about",
    "href": "lectures/L09/Lecture-09.html#about",
    "title": "Regression: predictive modelling",
    "section": "About",
    "text": "About\nData on the relationship between bird abundance (bird ha-1) and the characteristics of forest patches at 56 locations in SE Victoria.\nThe predictor variables are:\n\nALT Altitude (m)\nYR.ISOL Year when the patch was isolated (years)\nGRAZE Grazing (coded 1-5 which is light to heavy)\nAREA Patch area (ha)\nDIST Distance to nearest patch (km)\nLDIST Distance to largest patch (km)\n\n\nloyn &lt;- read_csv(\"data/loyn.csv\")"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#dataset-splitting",
    "href": "lectures/L09/Lecture-09.html#dataset-splitting",
    "title": "Regression: predictive modelling",
    "section": "Dataset splitting",
    "text": "Dataset splitting\nWe will split the data into training and test sets.\nAs the dataset is quite small, we will use a 80:20 split.\n\nset.seed(100)\nindexes &lt;- sample(1:nrow(loyn), size = 0.2 * nrow(loyn)) # randomly sample 20% of rows in the dataset\nloyn_train &lt;- loyn[-indexes, ] # remove the 20% - training dataset\nloyn_test &lt;- loyn[indexes, ] # select the 20% - test dataset"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#checking-the-split",
    "href": "lectures/L09/Lecture-09.html#checking-the-split",
    "title": "Regression: predictive modelling",
    "section": "Checking the split",
    "text": "Checking the split\nCheck out the str() of the data to see if the split worked (number of observations).\n\nstr(loyn_train)\n\ntibble [45 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ AREA   : num [1:45] 0.1 0.5 0.5 1 1 1 1 1 2 2 ...\n $ YR.ISOL: num [1:45] 1968 1920 1900 1966 1918 ...\n $ DIST   : num [1:45] 39 234 104 66 246 467 284 156 66 93 ...\n $ LDIST  : num [1:45] 39 234 311 66 246 ...\n $ GRAZE  : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT    : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n\nstr(loyn_test)\n\ntibble [11 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND  : num [1:11] 3 29.5 26 39.6 34.4 19.5 14.6 28.3 15.8 5 ...\n $ AREA   : num [1:11] 1 973 18 49 96 6 2 34 5 4 ...\n $ YR.ISOL: num [1:11] 1900 1970 1966 1972 1976 ...\n $ DIST   : num [1:11] 311 337 40 1427 39 ...\n $ LDIST  : num [1:11] 571 1323 3188 1557 519 ...\n $ GRAZE  : num [1:11] 5 1 2 1 2 3 1 1 3 5 ...\n $ ALT    : num [1:11] 130 190 190 180 175 170 210 110 130 120 ..."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#exploratory-data-analysis",
    "href": "lectures/L09/Lecture-09.html#exploratory-data-analysis",
    "title": "Regression: predictive modelling",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nThe next step is to visualise the data.\nExpore relationships between the predictors and the response via histograms, scatterplots, boxplots, correlations etc.\n\nIn this lecture we will just look at histograms."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#histograms",
    "href": "lectures/L09/Lecture-09.html#histograms",
    "title": "Regression: predictive modelling",
    "section": "Histograms",
    "text": "Histograms\n\n\nCode\nloyn_train %&gt;%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")\n\n\n\n\nLooks like AREA LDIST and DIST are skewed – we will transform them so that they are more normally distributed."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#transforming-predictors",
    "href": "lectures/L09/Lecture-09.html#transforming-predictors",
    "title": "Regression: predictive modelling",
    "section": "Transforming predictors",
    "text": "Transforming predictors\nWe will use log10() to transform the predictors. The mutate() function from the dplyr package is useful for this as it can create new columns in the data frame with the transformed values.\n\nloyn_train &lt;- loyn_train %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    )\n\nThen, remove the untransformed variables from the dataset. Here we can use the select() function from the dplyr package to “delselect” columns by using the - sign.\n\nloyn_train &lt;- loyn_train %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nstr(loyn_train)\n\ntibble [45 × 7] (S3: tbl_df/tbl/data.frame)\n $ ABUND    : num [1:45] 5.3 2 1.5 17.1 13.8 3.8 2.2 3.3 27.6 1.8 ...\n $ YR.ISOL  : num [1:45] 1968 1920 1900 1966 1918 ...\n $ GRAZE    : num [1:45] 2 5 5 3 5 5 5 4 3 5 ...\n $ ALT      : num [1:45] 160 60 140 160 140 90 60 130 210 160 ...\n $ AREA_L10 : num [1:45] -1 -0.301 -0.301 0 0 ...\n $ LDIST_L10: num [1:45] 1.59 2.37 2.49 1.82 2.39 ...\n $ DIST_L10 : num [1:45] 1.59 2.37 2.02 1.82 2.39 ..."
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#final-inspection",
    "href": "lectures/L09/Lecture-09.html#final-inspection",
    "title": "Regression: predictive modelling",
    "section": "Final inspection",
    "text": "Final inspection\nView the histograms again to check that the transformation worked.\n\n\nCode\nloyn_train %&gt;%\n    pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free\")"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#full-model",
    "href": "lectures/L09/Lecture-09.html#full-model",
    "title": "Regression: predictive modelling",
    "section": "Full model",
    "text": "Full model\nWe start with a full model that includes all the predictors.\n\nfull_fit &lt;- lm(ABUND ~ ., data = loyn_train)\nsummary(full_fit)\n\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#assumptions---round-1",
    "href": "lectures/L09/Lecture-09.html#assumptions---round-1",
    "title": "Regression: predictive modelling",
    "section": "Assumptions - Round 1",
    "text": "Assumptions - Round 1\nAs usual, we should check the assumptions of the model (CLINE + outliers). We will use the check_model() function from the perfomance package (because it looks nice and has interpretation instructions).\n\nperformance::check_model(full_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\"))"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#assumptions---round-1-1",
    "href": "lectures/L09/Lecture-09.html#assumptions---round-1-1",
    "title": "Regression: predictive modelling",
    "section": "Assumptions - Round 1",
    "text": "Assumptions - Round 1\nWe check multicollinearity with variable inflation factors (VIF) - VIFs are all &lt; 10, so there is no multicollinearity. All assumptions are thus met.\n\ncheck_model(full_fit, check = c(\"vif\"))"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#backwards-stepwise-selection",
    "href": "lectures/L09/Lecture-09.html#backwards-stepwise-selection",
    "title": "Regression: predictive modelling",
    "section": "Backwards stepwise selection",
    "text": "Backwards stepwise selection\nUse the step() function perform backwards stepwise selection. This function uses AIC to select the best model.\nDepending on the dataset splitting, the best model may be different each time we randomly sample the data. In this case we should all have the same results as we set the seed.\nIf we compare to the full model, the adjusted r-squared is slightly higher, and the AIC is lower.\n\nstep_fit &lt;- step(full_fit, direction = \"backward\")\n\nStart:  AIC=174.81\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- ALT        1     14.67 1618.5 173.22\n- LDIST_L10  1     16.92 1620.8 173.28\n- DIST_L10   1     69.18 1673.0 174.71\n&lt;none&gt;                   1603.9 174.81\n- GRAZE      1     78.00 1681.9 174.94\n- YR.ISOL    1    126.48 1730.3 176.22\n- AREA_L10   1    867.44 2471.3 192.26\n\nStep:  AIC=173.22\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1     10.76 1629.3 171.52\n&lt;none&gt;                   1618.5 173.22\n- DIST_L10   1     85.56 1704.1 173.54\n- GRAZE      1     98.23 1716.8 173.87\n- YR.ISOL    1    117.80 1736.3 174.38\n- AREA_L10   1   1088.05 2706.6 194.35\n\nStep:  AIC=171.52\nABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n&lt;none&gt;                  1629.3 171.52\n- GRAZE     1     93.97 1723.3 172.04\n- YR.ISOL   1    107.73 1737.0 172.40\n- DIST_L10  1    114.60 1743.9 172.57\n- AREA_L10  1   1161.66 2791.0 193.74"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#the-selected-model",
    "href": "lectures/L09/Lecture-09.html#the-selected-model",
    "title": "Regression: predictive modelling",
    "section": "The selected model",
    "text": "The selected model\n\n\n\nsummary(full_fit)\n\n\nCall:\nlm(formula = ABUND ~ ., data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.3445  -3.4647   0.1991   2.8689  14.1844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -159.27533  109.13660  -1.459   0.1527    \nYR.ISOL        0.09334    0.05392   1.731   0.0916 .  \nGRAZE         -1.40912    1.03653  -1.359   0.1820    \nALT            0.01657    0.02810   0.589   0.5590    \nAREA_L10       8.09629    1.78591   4.533 5.63e-05 ***\nLDIST_L10      2.05115    3.23927   0.633   0.5304    \nDIST_L10      -6.18596    4.83189  -1.280   0.2082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.497 on 38 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6239 \nF-statistic: 13.17 on 6 and 38 DF,  p-value: 5.277e-08\n\n\n\n\nsummary(step_fit)\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + AREA_L10 + DIST_L10, data = loyn_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1683  -3.1961   0.3374   3.4834  14.2021 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -135.17508  102.72850  -1.316    0.196    \nYR.ISOL        0.08323    0.05118   1.626    0.112    \nGRAZE         -1.50496    0.99082  -1.519    0.137    \nAREA_L10       8.61888    1.61392   5.340 3.98e-06 ***\nDIST_L10      -4.87726    2.90770  -1.677    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.382 on 40 degrees of freedom\nMultiple R-squared:  0.6701,    Adjusted R-squared:  0.6371 \nF-statistic: 20.31 on 4 and 40 DF,  p-value: 3.365e-09"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#assumptions---round-2",
    "href": "lectures/L09/Lecture-09.html#assumptions---round-2",
    "title": "Regression: predictive modelling",
    "section": "Assumptions - Round 2",
    "text": "Assumptions - Round 2\n\ncheck_model(step_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\", \"vif\"))"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#prepare-the-test-data",
    "href": "lectures/L09/Lecture-09.html#prepare-the-test-data",
    "title": "Regression: predictive modelling",
    "section": "Prepare the test data",
    "text": "Prepare the test data\nSince the test data has not been transformed, we need to do that first.\nWe then predict onto the training and test dataset using the reduced stepwise model.\n\nloyn_test &lt;- loyn_test %&gt;%\n    mutate(\n        AREA_L10 = log10(AREA),\n        LDIST_L10 = log10(LDIST),\n        DIST_L10 = log10(DIST)\n    ) %&gt;%\n    select(-AREA, -LDIST, -DIST)\n\nloyn_train$pred &lt;- predict(step_fit, newdata = loyn_train)\nloyn_test$pred &lt;- predict(step_fit, newdata = loyn_test)"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#plotting-observed-vs-predicted",
    "href": "lectures/L09/Lecture-09.html#plotting-observed-vs-predicted",
    "title": "Regression: predictive modelling",
    "section": "Plotting observed vs predicted",
    "text": "Plotting observed vs predicted\n\n\nCode\np1 &lt;- ggplot(loyn_train, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Training\") +\n  theme_classic()\n\np2 &lt;- ggplot(loyn_test, aes(ABUND, pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(x = \"Observed ABUND\", y = \"Predicted ABUND\",\n       title = \"Test\") +\n  theme_classic()\n\np1 + p2"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#calculating-metrics---error",
    "href": "lectures/L09/Lecture-09.html#calculating-metrics---error",
    "title": "Regression: predictive modelling",
    "section": "Calculating metrics - error",
    "text": "Calculating metrics - error\nGenerally the training dataset will have lower error and higher linearity than the test dataset. If this difference is very large – it suggests the model is not applicable to the test data and overfitting.\nWith error – lower is better. The following all measure error in the same units as the response variable (number of birds in each forest patch).\nMean error\n\nmean(loyn_train$ABUND - loyn_train$pred) |&gt; round(2)\n\n[1] 0\n\nmean(loyn_test$ABUND - loyn_test$pred) |&gt; round(2)\n\n[1] -1.65\n\n\nWe expect the ME for the training dataset to be near 0 – a well-fitted linear regression model will have positive and negative residuals balance each other out.\nMean absolute error\n\nmean(abs(loyn_train$ABUND - loyn_train$pred)) |&gt; round(2)\n\n[1] 4.43\n\nmean(abs(loyn_test$ABUND - loyn_test$pred)) |&gt; round(2)\n\n[1] 5.21\n\n\nRoot mean squared error (caret package)\n\nRMSE(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n[1] 6.02\n\nRMSE(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n[1] 6.72"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#calculating-metrics-linearity",
    "href": "lectures/L09/Lecture-09.html#calculating-metrics-linearity",
    "title": "Regression: predictive modelling",
    "section": "Calculating metrics – linearity",
    "text": "Calculating metrics – linearity\nWith linearity – higher is better.\nBoth training and test datasets perform similarly, which is a good sign the model is not overfitting.\nPearson’s correlation coefficient r\n\ncor(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n[1] 0.82\n\ncor(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n[1] 0.82\n\n\nR2\nCan either square the correlation coefficient or use the R2() function from the caret package.\n\nR2(loyn_train$ABUND, loyn_train$pred) |&gt; round(2)\n\n[1] 0.67\n\nR2(loyn_test$ABUND, loyn_test$pred) |&gt; round(2)\n\n[1] 0.68\n\n\nLin’s concordance correlation coefficient (CCC) (epiR package)\n\nepi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est |&gt; round(2)\n\n[1] 0.8\n\nepi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est |&gt; round(2)\n\n[1] 0.81"
  },
  {
    "objectID": "lectures/L09/Lecture-09.html#conclusions",
    "href": "lectures/L09/Lecture-09.html#conclusions",
    "title": "Regression: predictive modelling",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nCode\n# put all data into a tible and kable it\ntibble(\n    Dataset = c(\"Training\", \"Test\"),\n    ME = c(\n        mean(loyn_train$ABUND - loyn_train$pred),\n        mean(loyn_test$ABUND - loyn_test$pred)\n    ),\n    MAE = c(\n        mean(abs(loyn_train$ABUND - loyn_train$pred)),\n        mean(abs(loyn_test$ABUND - loyn_test$pred))\n    ),\n    RMSE = c(\n        RMSE(loyn_train$ABUND, loyn_train$pred),\n        RMSE(loyn_test$ABUND, loyn_test$pred)\n    ),\n    cor = c(\n        cor(loyn_train$ABUND, loyn_train$pred),\n        cor(loyn_test$ABUND, loyn_test$pred)\n    ),\n    R2 = c(\n        R2(loyn_train$ABUND, loyn_train$pred),\n        R2(loyn_test$ABUND, loyn_test$pred)\n    ),\n    LCCC = c(\n        epi.ccc(loyn_train$ABUND, loyn_train$pred)$rho.c$est,\n        epi.ccc(loyn_test$ABUND, loyn_test$pred)$rho.c$est\n    )\n) %&gt;%\n    knitr::kable(digits = 2)\n\n\n\n\n\nDataset\nME\nMAE\nRMSE\ncor\nR2\nLCCC\n\n\n\n\nTraining\n0.00\n4.43\n6.02\n0.82\n0.67\n0.80\n\n\nTest\n-1.65\n5.21\n6.72\n0.82\n0.68\n0.81\n\n\n\n\n\n\nThe model fit for the training dataset is marginally better (slight overfitting, but not a concern)\nSmall differences are also expected due to the small sample size or the chosen set.seed()\nThe model predicts bird abundance in forest patches in SE Victoria well (r = 0.82, LCCC = 0.81, MAE = 5.21 birds/patch, RMSE = 6.72 birds/patch)."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#workflow",
    "href": "lectures/L08/Lecture-08.html#workflow",
    "title": "Regression: model development",
    "section": "Workflow",
    "text": "Workflow\n\nModel development\n\nExplore: visualise, summarise\nTransform predictors: linearise, reduce skewness/leverage\nModel: fit, check assumptions, interpret, transform. Repeat.\n\nVariable selection\n\nVIF: remove predictors with high variance inflation factor\nModel selection: stepwise selection, AIC, principle of parsimony, assumption checks\n\nPredictive modelling\n\nPredict: Use the model to predict new data\nValidate: Evaluate the model’s performance"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#previously-on-envx2001",
    "href": "lectures/L08/Lecture-08.html#previously-on-envx2001",
    "title": "Regression: model development",
    "section": "Previously on ENVX2001…",
    "text": "Previously on ENVX2001…\nWe fitted a multiple linear regression model to the data.\n\nfull_fit &lt;- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(full_fit)\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind\\]"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#question",
    "href": "lectures/L08/Lecture-08.html#question",
    "title": "Regression: model development",
    "section": "Question",
    "text": "Question\n\\[\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind\\]\nAre all the variables/predictors needed?\nPrinciples\nA good model:\n\nHas only useful predictors: principle of parsimony\nHas no redundant predictors: principle of orthogonality (no multicollinearity)\nIs interpretable (principle of transparency; last week), or predicts well (principle of accuracy; next week)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#on-the-principle-of-parsimony",
    "href": "lectures/L08/Lecture-08.html#on-the-principle-of-parsimony",
    "title": "Regression: model development",
    "section": "On the principle of parsimony",
    "text": "On the principle of parsimony\n\nOckham’s razor: “Entities should not be multiplied unnecessarily.”\nOne should prefer the simplest explanation that fits the data if multiple explanations are equally good.\n\n\n“It is vain to do with more what can be done with fewer.”\n\n– William of Ockham (1287–1347)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#what-happens-when-we-add-more-predictors-to-a-model",
    "href": "lectures/L08/Lecture-08.html#what-happens-when-we-add-more-predictors-to-a-model",
    "title": "Regression: model development",
    "section": "What happens when we add more predictors to a model?",
    "text": "What happens when we add more predictors to a model?\nA simple example using polynomial regression.\n\nThe more predictors we include, the more variance we can explain.\nHowever, the more predictors and complexity we include, the more overfitted the model becomes.\n\n\n\nCode\nset.seed(1030)\nxsquared &lt;- function(x) {\n  x^2\n}\n# Generate xy data\nsim_data &lt;- function(xsquared, sample_size = 100) {\n  x &lt;- runif(n = sample_size, min = 0, max = 1)\n  y &lt;- rnorm(n = sample_size, mean = xsquared(x), sd = 0.05)\n  data.frame(x, y)\n}\n# Generate predicted data (model)\ndf &lt;- sim_data(xsquared, sample_size = 60)\nfit &lt;- lm(y ~ 1, data = df)\nfit_1 &lt;- lm(y ~ poly(x, degree = 1), data = df)\nfit_2 &lt;- lm(y ~ poly(x, degree = 2), data = df)\nfit_many &lt;- lm(y ~ poly(x, degree = 20), data = df)\ntruth &lt;- seq(from = 0, to = 1, by = 0.01)\n# Combine the data and model fits into a single data frame\ndf &lt;- data.frame(\n  x = df$x,\n  y = df$y,\n  fit = predict(fit),\n  fit_1 = predict(fit_1),\n  fit_2 = predict(fit_2),\n  fit_many = predict(fit_many)\n)\n\n# Reshape the data frame into long format\ndf_long &lt;- pivot_longer(\n  df,\n  cols = starts_with(\"fit_\"),\n  names_to = \"model\",\n  values_to = \"value\"\n) %&gt;%\n  mutate(\n    model = case_when(\n      model == \"fit\" ~ \"y = b\",\n      model == \"fit_1\" ~ \"y = b + mx\",\n      model == \"fit_2\" ~ \"y = b + mx + nx^2\",\n      model == \"fit_many\" ~ \"y = b + mx + nx^2 + ... + zx^20\",\n      TRUE ~ model\n    )\n  )\n# Plot\np &lt;- ggplot(df_long, aes(x = x, y = value, color = model)) +\n  facet_wrap(~model, ncol = 2, scales = \"free\") +\n  geom_point(aes(y = y), alpha = .4, size = 2) +\n  geom_line(linewidth = .9, linetype = 1) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  geom_blank()\np"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#variance-bias-trade-off",
    "href": "lectures/L08/Lecture-08.html#variance-bias-trade-off",
    "title": "Regression: model development",
    "section": "Variance-bias trade-off",
    "text": "Variance-bias trade-off\n\nIn concept…\n\nAs complexity increases, bias (\\(\\sum{O-\\bar{P}}\\)) decreases (the mean of a model’s predictions is closer to the true mean).\nAs complexity increases, prediction variance (\\(\\frac{\\sum{(P-\\bar{P})}^2}{n}\\)) increases.\nThe goal is to find a model that isn’t too simple or complex with a good balance between bias and variance.\n\n\\[ \\text{Mean Squared Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Immeasureable Error} \\] In practice, the math and relationships are a bit more irregular."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#how-do-we-determine-the-best-model",
    "href": "lectures/L08/Lecture-08.html#how-do-we-determine-the-best-model",
    "title": "Regression: model development",
    "section": "How do we determine the best model?",
    "text": "How do we determine the best model?\nSome model quality measures you should be familiar with:\n\nR2: variance explained by the model with a maximum value of 1 = 100%\nResidual standard error: the mean error of the observed values from the predicted/fitted values (i.e. line of best fit).\nPartial F-test: compare the full model to a reduced model, works well when the number of predictors is small and simple models.\n\nSome other commonly used measures:\n\nInformation criteria: AIC, BIC, etc. (more on this later).\nError measures: useful when the aim for the model is to predict. The best model has the smallest residual error (or other similar metrics)."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#what-models-do-we-try",
    "href": "lectures/L08/Lecture-08.html#what-models-do-we-try",
    "title": "Regression: model development",
    "section": "What models do we try?",
    "text": "What models do we try?\n\nEverything: all possible combinations of predictors.\n\nEach variable can either be included or excluded so the number of possible combinations is \\(2^n\\)\ne.g. 3 predictors (\\(x_1\\), \\(x_2\\), \\(x_3\\)) could have 8 models\n\nNo variables i.e. mean \\(y\\) the null hypothesis\n1 variable: \\(x_1\\); \\(x_2\\); \\(x_3\\)\n2 variables: \\(x_1\\) + \\(x_2\\); \\(x_1\\) + \\(x_3\\); \\(x_2\\) + \\(x_3\\)\n3 variables: \\(x_1\\) + \\(x_2\\) + \\(x_3\\)\n\nSo…not recommended\n\nStepwise regression: add/remove predictors one at a time until removing a variable makes the model worse.\nSelect meaningful predictors based on domain knowledge, correlation, or significance.\nMore complex approaches are available for big data and machine learning."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#air-quality-can-we-reduce-the-number-of-predictors",
    "href": "lectures/L08/Lecture-08.html#air-quality-can-we-reduce-the-number-of-predictors",
    "title": "Regression: model development",
    "section": "Air quality: can we reduce the number of predictors?",
    "text": "Air quality: can we reduce the number of predictors?\nFull model:\n\nsummary(full_fit)\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\nWind has the highest p-value, can we remove it?\nFull model: Multiple R-squared = 0.66, Adjusted R-squared = 0.66"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#section",
    "href": "lectures/L08/Lecture-08.html#section",
    "title": "Regression: model development",
    "section": "",
    "text": "Full model: multiple R-squared = 0.66, adjusted R-squared = 0.66\n\nReduced model: take out Wind\n\nreduced_fit &lt;- lm(log(Ozone) ~ Temp + Solar.R, data = airquality)\nsummary(reduced_fit)\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83864 -0.33727  0.03444  0.29877  1.38210 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.7646527  0.4249016  -4.153 6.58e-05 ***\nTemp         0.0607386  0.0056663  10.719  &lt; 2e-16 ***\nSolar.R      0.0024651  0.0005924   4.161 6.38e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5413 on 108 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6163,    Adjusted R-squared:  0.6092 \nF-statistic: 86.73 on 2 and 108 DF,  p-value: &lt; 2.2e-16\n\n\n\nReduced model: multiple R-squared = 0.62, adjusted R-squared = 0.61\nAdjusted R-squared is lower, but is a 4% difference “worth it”? Is it significant?"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#the-r2-value",
    "href": "lectures/L08/Lecture-08.html#the-r2-value",
    "title": "Regression: model development",
    "section": "The R2 value",
    "text": "The R2 value\nThe R-squared value is the proportion of variance explained by the model.\n\\[ R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\nThe adjusted R-squared value is the proportion of variance explained by the model, adjusted for the number of predictors.\n\\[R^2_{adj} = 1 - \\frac{SS_{res}}{SS_{tot}} \\frac{n-1}{n-p-1} \\]\nwhere \\(n\\) is the number of observations and \\(p\\) is the number of predictors."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#partial-f-test-1",
    "href": "lectures/L08/Lecture-08.html#partial-f-test-1",
    "title": "Regression: model development",
    "section": "Partial F-test",
    "text": "Partial F-test\nHow much of an improvement in adjusted \\(R^2\\) is worth having an extra variable / more complex model?\n\nWe can perform a hypothesis test to determine whether the improvement is significant.\nThe F-test measures the full model against an intercept only model in terms of explained variance (residual sum of squares).\nThe partial F-test compares the full model to a reduced model in terms of the trade-off between model complexity and variance explained (i.e. adjusted \\(R^2\\)).\n\n\\(H_0\\): no significant difference between the full and reduced models\n\\(H_1\\): the full model is significantly better than the reduced model\nCalculating the F-stat:\n\n\n\\[F = \\big| \\frac{SS_{reg,full} - SS_{reg,reduced}}{(df_{res,full} - df_{res,reduced})} \\big | \\div MS_{res, full}\\]"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#partial-f-test-calculation",
    "href": "lectures/L08/Lecture-08.html#partial-f-test-calculation",
    "title": "Regression: model development",
    "section": "Partial F-test: calculation",
    "text": "Partial F-test: calculation\n\\[F = \\big| \\frac{SS_{reg,full} - SS_{reg,reduced}}{(df_{res,full} - df_{res,reduced})} \\big | \\div MS_{res, full}\\] where:\n\n\\(SS_{reg,full}\\) is the sum of squares of the full model (total of predictors)\n\\(SS_{reg,reduced}\\) is the sum of squares of the reduced model (total of predictors)\n\\(df_{res,full}\\) is the degrees of freedom of the residuals of the full model\n\\(df_{res,reduced}\\) is the degrees of freedom of the residuals of the reduced model\n\\(MS_{res, full}\\) is the mean square of the residuals of the full model\n\n\n\n\nfull &lt;- anova(full_fit) %&gt;% broom::tidy()\nfull\n\n# A tibble: 4 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Temp          1 45.8  45.8       177.   2.07e-24\n2 Solar.R       1  5.07  5.07       19.6  2.29e- 5\n3 Wind          1  3.97  3.97       15.4  1.58e- 4\n4 Residuals   107 27.7   0.259      NA   NA       \n\n\n\n\nreduced &lt;- anova(reduced_fit) %&gt;% broom::tidy()\nreduced\n\n# A tibble: 3 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Temp          1 45.8  45.8       156.   1.05e-22\n2 Solar.R       1  5.07  5.07       17.3  6.38e- 5\n3 Residuals   108 31.6   0.293      NA   NA       \n\n\n\nEach row is the individual effect of each predictor on the response \\(log(Ozone)\\) (whilst holding all other predictors constant)."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#by-hand",
    "href": "lectures/L08/Lecture-08.html#by-hand",
    "title": "Regression: model development",
    "section": "By hand",
    "text": "By hand\n\\[F = \\big| \\frac{SS_{reg,full} - SS_{reg,reduced}}{(df_{res,full} - df_{res,reduced})} \\big | \\div MS_{res, full}\\]\n\n\\(SS_{reg,full} = 45.8 + 5.07 + 3.97 = 54.84\\)\n\\(SS_{reg,reduced} = 45.8 + 5.07 = 50.87\\)\n\\(df_{res,full} = 107\\)\n\\(df_{res,reduced} = 108\\)\n\\(MS_{res, full} = 0.259\\)\n\n\\(F = |\\frac{54.84 - 50.87}{(107-108)}| \\div 0.259 = 15.33\\)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#in-r-manually",
    "href": "lectures/L08/Lecture-08.html#in-r-manually",
    "title": "Regression: model development",
    "section": "In R (manually)",
    "text": "In R (manually)\n\nss_full &lt;- sum(full$sumsq[1:3])\nss_reduced &lt;- sum(reduced$sumsq[1:2])\ndf_full &lt;- full$df[4]\ndf_reduced &lt;- reduced$df[3]\nms_full &lt;- full$meansq[4]\nF &lt;- abs((ss_full - ss_reduced) / ((df_full - df_reduced))) / ms_full\nF     # F-statistic\n\n[1] 15.35026\n\npf(F, df1 = 1, df2 = df_full, lower.tail = FALSE) # corresponding p-value\n\n[1] 0.0001576806\n\n\n(There is a slight difference due to rounding)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#in-r-with-functions",
    "href": "lectures/L08/Lecture-08.html#in-r-with-functions",
    "title": "Regression: model development",
    "section": "In R with functions",
    "text": "In R with functions\n\nanova(reduced_fit, full_fit) # reduced goes first else will see negative df\n\nAnalysis of Variance Table\n\nModel 1: log(Ozone) ~ Temp + Solar.R\nModel 2: log(Ozone) ~ Temp + Solar.R + Wind\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    108 31.645                                 \n2    107 27.675  1    3.9703 15.35 0.0001577 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe partial F-test is significant (p-value &lt; 0.05), so we can reject the null hypothesis and conclude that the full model is significantly better, even if adjusted R2 improves by 4%."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#but-wait",
    "href": "lectures/L08/Lecture-08.html#but-wait",
    "title": "Regression: model development",
    "section": "But wait…",
    "text": "But wait…\nLooking back at the original model, we can see that the partial regression coefficients are the same as the partial F-test results!\n\nanova(reduced_fit, full_fit) # partial F-test\n\nAnalysis of Variance Table\n\nModel 1: log(Ozone) ~ Temp + Solar.R\nModel 2: log(Ozone) ~ Temp + Solar.R + Wind\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    108 31.645                                 \n2    107 27.675  1    3.9703 15.35 0.0001577 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(full_fit)$coefficients\n\n                Estimate   Std. Error    t value     Pr(&gt;|t|)\n(Intercept) -0.262132313 0.5535668608 -0.4735332 6.367975e-01\nTemp         0.049171124 0.0060875025  8.0773888 1.069103e-12\nSolar.R      0.002515177 0.0005567301  4.5177673 1.616797e-05\nWind        -0.061562470 0.0157129654 -3.9179409 1.576806e-04\n\n\nThis is because the reduced model is nested within the full model so the partial F-test is equivalent to a partial regression coefficient test."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#nested-models",
    "href": "lectures/L08/Lecture-08.html#nested-models",
    "title": "Regression: model development",
    "section": "Nested models",
    "text": "Nested models\n\nPrevious example is a simple example of a nested model.\nA model is nested within another model if the predictors in the first model are a subset of the predictors in the second model.\nThis makes comparing the two models easier, as we can compare the regression coefficients of the two models.\n\nExample\n\nIf the original model is y ~ a + b + c:\n\nNested: y ~ a + b\nNested: y ~ a\nNot nested: y ~ a + b + d – because d is not in the full model\n\n\n\n\n\n\n\n\nImportant\n\n\nPartial F-tests will only make sense/work for nested models!"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#about",
    "href": "lectures/L08/Lecture-08.html#about",
    "title": "Regression: model development",
    "section": "About",
    "text": "About\n\nloyn &lt;- read.csv(\"data/loyn.csv\")\nstr(loyn)\n\n'data.frame':   56 obs. of  7 variables:\n $ ABUND  : num  5.3 2 1.5 17.1 13.8 14.1 3.8 2.2 3.3 3 ...\n $ AREA   : num  0.1 0.5 0.5 1 1 1 1 1 1 1 ...\n $ YR.ISOL: int  1968 1920 1900 1966 1918 1965 1955 1920 1965 1900 ...\n $ DIST   : int  39 234 104 66 246 234 467 284 156 311 ...\n $ LDIST  : int  39 234 311 66 246 285 467 1829 156 571 ...\n $ GRAZE  : int  2 5 5 3 5 3 5 5 4 5 ...\n $ ALT    : int  160 60 140 160 140 130 90 60 130 130 ...\n\n\n\nCan we predict the abundance of birds in forest patches cleared for agriculture, based on patch size, area, grazing and other variables?\nLoyn (1987)\n\nDIST: Distance to nearest patch (km)\nLDIST: Distance to a larger patch (km)\nAREA: Patch area (ha)\nGRAZE: Grazing pressure 1 (light) – 5 (heavy) – ALT: Altitude (m)\nYR.ISOL: Years since isolation (years)\nABUND: Density of forest birds in a forest patch (birds/patch)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#data-exploration",
    "href": "lectures/L08/Lecture-08.html#data-exploration",
    "title": "Regression: model development",
    "section": "Data exploration",
    "text": "Data exploration\n\nloyn %&gt;%\n  pivot_longer(-ABUND) %&gt;%\n  ggplot(aes(x = value, y = ABUND)) +\n  geom_point() +\n  facet_wrap(~name, scales = \"free\") +\n  labs(y = \"ABUND\")\n\n\n\nThe predictors are on very different scales, which can cause problems for the model.\nThe relationships don’t look particularly linear…and outliers.\nWe will perform log10 transforms of AREA, LDIST, and DIST."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#log10-transformation",
    "href": "lectures/L08/Lecture-08.html#log10-transformation",
    "title": "Regression: model development",
    "section": "Log10 transformation",
    "text": "Log10 transformation\n\n\nCode\n# perform transformations\nloyn &lt;- loyn %&gt;%\n  mutate(AREA_L10 = log10(AREA),\n         LDIST_L10 = log10(LDIST),\n         DIST_L10 = log10(DIST))\n\n# View distributions again\nloyn %&gt;%\n  select(-ALT, -GRAZE, -YR.ISOL) %&gt;%\n  pivot_longer(-ABUND) %&gt;%\n  mutate(name = factor(name, levels = unique(name))) %&gt;%  # Preserve original order\n  ggplot(aes(x = value, y = ABUND)) +\n  geom_point() +\n  facet_wrap(~name, scales = \"free\") +\n  labs(y = \"ABUND\")"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#checking-assumptions---no-transformation",
    "href": "lectures/L08/Lecture-08.html#checking-assumptions---no-transformation",
    "title": "Regression: model development",
    "section": "Checking assumptions - no transformation",
    "text": "Checking assumptions - no transformation\n\n\nCode\nloyn_fit &lt;- lm(ABUND ~ YR.ISOL + GRAZE + ALT + AREA + LDIST + DIST, data = loyn)\nsummary(loyn_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + ALT + AREA + LDIST + DIST, \n    data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.6638  -4.6409  -0.0883   4.2858  20.1042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -1.097e+02  1.133e+02  -0.968  0.33791   \nYR.ISOL      6.693e-02  5.684e-02   1.177  0.24472   \nGRAZE       -3.447e+00  1.107e+00  -3.114  0.00308 **\nALT          4.772e-02  3.089e-02   1.545  0.12878   \nAREA         8.866e-04  4.657e-03   0.190  0.84980   \nLDIST        1.418e-03  1.310e-03   1.082  0.28451   \nDIST         3.811e-03  5.418e-03   0.703  0.48514   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.947 on 49 degrees of freedom\nMultiple R-squared:  0.5118,    Adjusted R-squared:  0.452 \nF-statistic: 8.561 on 6 and 49 DF,  p-value: 2.24e-06"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#section-1",
    "href": "lectures/L08/Lecture-08.html#section-1",
    "title": "Regression: model development",
    "section": "",
    "text": "Code\nperformance::check_model(loyn_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#checking-assumptions---transformation",
    "href": "lectures/L08/Lecture-08.html#checking-assumptions---transformation",
    "title": "Regression: model development",
    "section": "Checking assumptions - transformation",
    "text": "Checking assumptions - transformation\n\n\nCode\nloyn_fit &lt;- lm(ABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10, data = loyn)\nsummary(loyn_fit)\n\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + \n    DIST_L10, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6506  -2.9390   0.5289   2.5353  15.2842 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -125.69725   91.69228  -1.371   0.1767    \nYR.ISOL        0.07387    0.04520   1.634   0.1086    \nGRAZE         -1.66774    0.92993  -1.793   0.0791 .  \nALT            0.01951    0.02396   0.814   0.4195    \nAREA_L10       7.47023    1.46489   5.099 5.49e-06 ***\nLDIST_L10     -0.64842    2.12270  -0.305   0.7613    \nDIST_L10      -0.90696    2.67572  -0.339   0.7361    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.384 on 49 degrees of freedom\nMultiple R-squared:  0.6849,    Adjusted R-squared:  0.6464 \nF-statistic: 17.75 on 6 and 49 DF,  p-value: 8.443e-11"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#section-2",
    "href": "lectures/L08/Lecture-08.html#section-2",
    "title": "Regression: model development",
    "section": "",
    "text": "Code\nperformance::check_model(loyn_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#leverage",
    "href": "lectures/L08/Lecture-08.html#leverage",
    "title": "Regression: model development",
    "section": "Leverage",
    "text": "Leverage\n\nThe leverage plot shows the influence of each observation (i.e. point) on the model.\nPoints with high leverage can have a large effect on the model when removed.\nIdentified by the Cook’s distance statistic – named after the American statistician R. Dennis Cook, who introduced the concept in 1977.\n\n\n\n\n\n\n\nTip\n\n\nThe leverage plot is a useful tool for identifying outliers and influential points, but can also be used to check for other issues such as heteroskedasticity (equal variances) and non-linearity!"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#reading-the-leverage-plot",
    "href": "lectures/L08/Lecture-08.html#reading-the-leverage-plot",
    "title": "Regression: model development",
    "section": "Reading the leverage plot",
    "text": "Reading the leverage plot\n\npar(mfrow = c(1,2))\nplot(loyn_fit, which = c(4,5))\n\n\n\nVisually, points with Cook’s distance &gt; 0.5 are considered influential by default, but this is a somewhat arbitrary threshold.\nIn practice, you should use a threshold that is appropriate for your data and model."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#outlier-detection-using-performance",
    "href": "lectures/L08/Lecture-08.html#outlier-detection-using-performance",
    "title": "Regression: model development",
    "section": "Outlier detection using performance",
    "text": "Outlier detection using performance\n\nperformance::check_model(loyn_fit, check = c(\"outliers\", \"pp_check\"))\n\n\n\nperformance::check_outliers(loyn_fit)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.919).\n- For variable: (Whole model)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#collinearity",
    "href": "lectures/L08/Lecture-08.html#collinearity",
    "title": "Regression: model development",
    "section": "Collinearity",
    "text": "Collinearity\n\nTwo predictors that have a perfect linear relationship (i.e. \\(r\\) = 1 or -1) breaks the assumption of collinearity\nEven strong correlations between predictors can lead to unstable estimates and large standard errors.\nVariance inflation factors (VIFs) are a measure of collinearity in the model.\n\n\ncorrplot::corrplot(cor(loyn), method = \"number\")"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#calculating-vif",
    "href": "lectures/L08/Lecture-08.html#calculating-vif",
    "title": "Regression: model development",
    "section": "Calculating VIF",
    "text": "Calculating VIF\n\ncar::vif(loyn_fit) |&gt; round(2) # numbers\n\n  YR.ISOL     GRAZE       ALT  AREA_L10 LDIST_L10  DIST_L10 \n     1.80      2.52      1.47      1.91      2.01      1.65 \n\nplot(performance::check_collinearity(loyn_fit)) # visual\n\n\n\n\\(1\\) = no correlation with other predictors.\n\\(&gt;10\\) is a sign for high, not tolerable correlation of model predictors (which need to be removed and the model refitted)."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#the-best-model",
    "href": "lectures/L08/Lecture-08.html#the-best-model",
    "title": "Regression: model development",
    "section": "The best model?",
    "text": "The best model?\nIf we remove the least significant variable…\n\n\nCode\nfull6 &lt;- loyn_fit\npart5 &lt;- update(full6, . ~ . - LDIST_L10)\npart4 &lt;- update(part5, . ~ . - DIST_L10)\npart3 &lt;- update(part4, . ~ . - ALT)\npart2 &lt;- update(part3, . ~ . - YR.ISOL)\npart1 &lt;- update(part2, . ~ . - GRAZE)\n\nformulas &lt;- c(part1$call$formula, \n              part2$call$formula, \n              part3$call$formula, \n              part4$call$formula, \n              part5$call$formula, \n              loyn_fit$call$formula)\nformulas &lt;-\n  c(\"ABUND ~ AREA_L10\",\n    \"ABUND ~ AREA_L10 + GRAZE\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10 + LDIST_L10\")\n\nrs &lt;- bind_rows(glance(part1), \n          glance(part2), \n          glance(part3), \n          glance(part4),\n          glance(part5), \n          glance(full6)) %&gt;%\n        mutate(Model = formulas) %&gt;%\n        select(Model, r.squared, adj.r.squared)\n\nknitr::kable(rs, digits = 2)\n\n\n\n\n\n\n\n\n\n\nModel\nr.squared\nadj.r.squared\n\n\n\n\nABUND ~ AREA_L10\n0.55\n0.54\n\n\nABUND ~ AREA_L10 + GRAZE\n0.65\n0.64\n\n\nABUND ~ AREA_L10 + GRAZE + YR.ISOL\n0.67\n0.65\n\n\nABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT\n0.68\n0.66\n\n\nABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10\n0.68\n0.65\n\n\nABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10 + LDIST_L10\n0.68\n0.65\n\n\n\n\n\n\nR-squared increases with addition of predictors.\nAdj. R-squared varies with addition of predictors."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#the-problem",
    "href": "lectures/L08/Lecture-08.html#the-problem",
    "title": "Regression: model development",
    "section": "The problem",
    "text": "The problem\n\nOther combinations of predictors exist but are not shown.\nNeed automated way to select the best model – 6 predictors gives us 2^6 = 64 models to choose from!\nOptions:\n\nBackward elimination\nForward selection\nCombination (R default)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#steps-for-backward-elimination",
    "href": "lectures/L08/Lecture-08.html#steps-for-backward-elimination",
    "title": "Regression: model development",
    "section": "Steps for backward elimination",
    "text": "Steps for backward elimination\n\nStart with full model.\nFor each predictor, test the effect of its removal on the model fit.\nRemove the predictor that has the least effect on the model fit i.e. the least informative predictor, unless it is nonetheless supplying significant information about the response.\nRepeat steps 2 and 3 until no predictors can be removed without significantly affecting the model fit.\n\nIn backward selection, the model fit is assessed using the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). Here we focus on the AIC."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#about-aic",
    "href": "lectures/L08/Lecture-08.html#about-aic",
    "title": "Regression: model development",
    "section": "About AIC",
    "text": "About AIC\n\nMost popular model selection criterion (can be used for non-nested models)\nDeveloped by Hirotsugu Akaike under the name of “an information criterion” (AIC)\nFounded on information theory which is concerned with the transmission, processing, utilization, and extraction of information.\n\n\\[AIC = 2k - 2\\ln(L)\\]"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#about-aic-1",
    "href": "lectures/L08/Lecture-08.html#about-aic-1",
    "title": "Regression: model development",
    "section": "About AIC",
    "text": "About AIC\n\\[AIC = 2k - 2\\ln(L)\\]\n\n\\(k\\) is the number of parameters in the model (predictors + intercept)\n\\(L\\) is the maximum value of the likelihood function\n\nIf we predict using the (current) model, what is the probability density of the prediction compared to the original distribution?\n\\(ln(L)\\) = goodness of fit (higher is better)\n\nFor the number of parameters in the model (\\(2k\\)) subtract the goodness of fit \\(2ln(L)\\)\n\nThe smaller the AIC, the better the model fits the data.\nA relative measure and unitless, so it is not worth trying to interpret alone.\n\nCan be calculated with the AIC() function in R"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#about-aic---fyi",
    "href": "lectures/L08/Lecture-08.html#about-aic---fyi",
    "title": "Regression: model development",
    "section": "About AIC - FYI",
    "text": "About AIC - FYI\nIn linear regression, AIC is sometimes calculated as:\n\\[AIC = n\\log(\\frac{RSS}{n}) + 2k\\]\nwhere \\(RSS\\) is the residual sum of squares, \\(2k\\) is the number of parameters in the model, and \\(n\\) is the number of observations.\n\nThe difference between this equation and the previous equation is a constant, or scaling\nThe step() function in R uses this equation (hence you may notice a difference in AIC values!)"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#back-to-our-example",
    "href": "lectures/L08/Lecture-08.html#back-to-our-example",
    "title": "Regression: model development",
    "section": "Back to our example",
    "text": "Back to our example\n\nback_step &lt;- step(loyn_fit, direction = \"backward\")\n\nStart:  AIC=214.14\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + LDIST_L10 + DIST_L10\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1      3.80 2000.7 212.25\n- DIST_L10   1      4.68 2001.5 212.27\n- ALT        1     27.02 2023.9 212.90\n&lt;none&gt;                   1996.8 214.14\n- YR.ISOL    1    108.83 2105.7 215.11\n- GRAZE      1    131.07 2127.9 215.70\n- AREA_L10   1   1059.75 3056.6 235.98\n\nStep:  AIC=212.25\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n- DIST_L10  1     12.64 2013.3 210.60\n- ALT       1     35.12 2035.8 211.22\n&lt;none&gt;                  2000.7 212.25\n- YR.ISOL   1    121.64 2122.3 213.55\n- GRAZE     1    132.44 2133.1 213.84\n- AREA_L10  1   1193.04 3193.7 236.44\n\nStep:  AIC=210.6\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10\n\n           Df Sum of Sq    RSS    AIC\n- ALT       1     57.84 2071.1 210.19\n&lt;none&gt;                  2013.3 210.60\n- GRAZE     1    123.48 2136.8 211.94\n- YR.ISOL   1    134.89 2148.2 212.23\n- AREA_L10  1   1227.11 3240.4 235.25\n\nStep:  AIC=210.19\nABUND ~ YR.ISOL + GRAZE + AREA_L10\n\n           Df Sum of Sq    RSS    AIC\n&lt;none&gt;                  2071.1 210.19\n- YR.ISOL   1    129.81 2200.9 211.59\n- GRAZE     1    188.45 2259.6 213.06\n- AREA_L10  1   1262.97 3334.1 234.85"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#section-3",
    "href": "lectures/L08/Lecture-08.html#section-3",
    "title": "Regression: model development",
    "section": "",
    "text": "Printing back_step reveals the final model:\n\nback_step\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + AREA_L10, data = loyn)\n\nCoefficients:\n(Intercept)      YR.ISOL        GRAZE     AREA_L10  \n -134.26065      0.07835     -1.90216      7.16617"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#backward-elimination-coefficients",
    "href": "lectures/L08/Lecture-08.html#backward-elimination-coefficients",
    "title": "Regression: model development",
    "section": "Backward elimination: coefficients",
    "text": "Backward elimination: coefficients\nFull model\n\n\nCode\nsjPlot::tab_model(\n  loyn_fit, back_step, \n  show.ci = FALSE, \n  show.aic = TRUE,\n  dv.labels = c(\"Full model\",\n                \"Reduced model\")\n)\n\n\n\n\n\n \nFull model\nReduced model\n\n\nPredictors\nEstimates\np\nEstimates\np\n\n\n(Intercept)\n-125.70\n0.177\n-134.26\n0.126\n\n\nYR ISOL\n0.07\n0.109\n0.08\n0.077\n\n\nGRAZE\n-1.67\n0.079\n-1.90\n0.034\n\n\nALT\n0.02\n0.419\n\n\n\n\nAREA L10\n7.47\n&lt;0.001\n7.17\n&lt;0.001\n\n\nLDIST L10\n-0.65\n0.761\n\n\n\n\nDIST L10\n-0.91\n0.736\n\n\n\n\nObservations\n56\n56\n\n\nR2 / R2 adjusted\n0.685 / 0.646\n0.673 / 0.654\n\n\nAIC\n375.064\n371.109\n\n\n\n\n\nThe reduced model retains more explanatory power than the full model!"
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#model-selection",
    "href": "lectures/L08/Lecture-08.html#model-selection",
    "title": "Regression: model development",
    "section": "Model selection",
    "text": "Model selection\nModel development\n\nStart with full model and check assumptions (e.g. normality, homoscedasticity, linearity, etc.).\nLook for additional issues (e.g. multicollinearity, outliers, etc.) – correlations, leverage, VIF plots.\nConsider transformations (e.g. log, sqrt, etc.).\nTest assumptions again.\n\nModel selection\n\nUse VIF as an initial step to get rid of highly correlated predictors.\nPerform variable selection using backward elimination (good and fast), because:\n\nUsing R2 as a criterion is not recommended (it is not a good measure of model fit, only a good measure of variance explained).\nUsing partial F-test is good, but slow."
  },
  {
    "objectID": "lectures/L08/Lecture-08.html#next-lecture-model-training-and-prediction",
    "href": "lectures/L08/Lecture-08.html#next-lecture-model-training-and-prediction",
    "title": "Regression: model development",
    "section": "Next lecture: model training and prediction",
    "text": "Next lecture: model training and prediction\n\nHow to incorporate calibration and validation into your workflow\nDetermining prediction intervals and performance metrics"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#about-me",
    "href": "lectures/L07/Lecture-07.html#about-me",
    "title": "Regression: modelling",
    "section": "About me",
    "text": "About me\n\n\n\n\nResearch topics: spatial modelling and mapping, precision agriculture, winter grains\nTimeline at USYD\n\nBSc (Hons) in Agricultural Science\nPhD in Digital Agriculture\nPostdoc in Spatial Modelling\nAssociate Lecturer in Agricultural Data Science\n\n\n\n\n\n\n\nFaba beans at Trangie"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#learning-outcomes",
    "href": "lectures/L07/Lecture-07.html#learning-outcomes",
    "title": "Regression: modelling",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nLO1. demonstrate proficiency in designing sample schemes and analysing data from them using using R\nLO2. describe and identify the basic features of an experimental design; replicate, treatment structure and blocking structure\nLO3. demonstrate proficiency in the use or the statistical programming language R to an ANOVA and fit regression models to experimental data\nLO4. demonstrate proficiency in the use or the statistical programming language R to use multivariate methods to find patterns in data\nLO5. interpret the output and understand conceptually how its derived of a regression, ANOVA and multivariate analysis that have been calculated by R\nLO6. write statistical and modelling results as part of a scientific report\nLO7. appraise the validity of statistical analyses used publications."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#refresher-from-envx1002",
    "href": "lectures/L07/Lecture-07.html#refresher-from-envx1002",
    "title": "Regression: modelling",
    "section": "Refresher from ENVX1002",
    "text": "Refresher from ENVX1002\n\nRegression modelling is for one continuous numerical response (\\(y\\)) and one or more numerical predictors (\\(x_1\\), \\(x_2\\), \\(x_n\\))\nCan be for linear or nonlinear relationships – focus on linear in ENVX2001\nTo help us:\n\nUnderstand the relationship between variables\nPredict new values of \\(y\\) based on \\(x\\)\nTest hypotheses about the relationship between variables\n\nFit a ‘line of best fit’ that minimises the sum of the squared residuals (least-squares)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#workflow",
    "href": "lectures/L07/Lecture-07.html#workflow",
    "title": "Regression: modelling",
    "section": "Workflow",
    "text": "Workflow\n\nModel development\n\nExplore: visualise, summarise\nModel: fit, check assumptions, interpret – (transform, repeat).\nTransform predictors\n\nVariable selection\n\nVIF: remove predictors with high variance inflation factor\nModel selection: stepwise selection, AIC, principle of parsimony, assumption checks\n\nPredictive modelling\n\nPredict: Use the model to predict new data\nValidate: Evaluate the model’s performance"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#brief-history",
    "href": "lectures/L07/Lecture-07.html#brief-history",
    "title": "Regression: modelling",
    "section": "Brief history",
    "text": "Brief history\n  \nAdrien-Marie Legendre, Carl Friedrich Gauss, Francis Galton\n\n\n\n\n\n\nNote\n\n\nMany other people contributed to the development of regression analysis, but these three are the most well-known."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#brief-history-1",
    "href": "lectures/L07/Lecture-07.html#brief-history-1",
    "title": "Regression: modelling",
    "section": "Brief history",
    "text": "Brief history\n\nMethod of least squares first theorised by Adrien-Marie Legendre in 1805\nTechnique of least squares first used by Carl Friedrich Gauss in 1809 (to fit a parabola to the orbit of the asteroid Ceres)\nModel fitting first published by Francis Galton in 1886 (predicting the height of a child from the height of the parents)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#example-child-vs-parent-height",
    "href": "lectures/L07/Lecture-07.html#example-child-vs-parent-height",
    "title": "Regression: modelling",
    "section": "Example: child vs parent height",
    "text": "Example: child vs parent height\nGalton, F. (1886). Regression Towards Mediocrity in Hereditary Stature Journal of the Anthropological Institute, 15, 246-263\n\n\n\n\nCode\nlibrary(HistData)\ndata(Galton)\nstr(Galton)\n\n\n'data.frame':   928 obs. of  2 variables:\n $ parent: num  70.5 68.5 65.5 64.5 64 67.5 67.5 67.5 66.5 66.5 ...\n $ child : num  61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 ...\n\n\n\n928 children of 205 pairs of parents\nAverage height of both parents and their child’s height measured in inches\nSize classes were binned (hence data looks discrete)\n\n\n\n\nCode\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3) + \n  geom_smooth(method = \"lm\") +\n  labs(subtitle = paste(\"Correlation:\", round(cor(Galton$parent, Galton$child), 2)))"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#defining-a-linear-relationship",
    "href": "lectures/L07/Lecture-07.html#defining-a-linear-relationship",
    "title": "Regression: modelling",
    "section": "Defining a linear relationship",
    "text": "Defining a linear relationship\n\nPearson correlation coefficient (\\(r\\)) measures the linear correlation between two variables (ranges from -1 to 1)\nUseful for distinguishing strength (weak/moderate/strong) and direction (positive/negative) of the association\nDoes not distinguish different patterns – i.e. is the relationship actually linear?\n\n\\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} \\]\n\ncor(Galton$parent, Galton$child) |&gt; round(2)\n\n[1] 0.46"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#anscombes-quartet",
    "href": "lectures/L07/Lecture-07.html#anscombes-quartet",
    "title": "Regression: modelling",
    "section": "Anscombe’s quartet",
    "text": "Anscombe’s quartet\n\n\nCode\nlibrary(tidyverse)\nanscombe %&gt;%\n  pivot_longer(everything(), cols_vary = \"slowest\",\n    names_to = c(\".value\", \"set\"), names_pattern = \"(.)(.)\") %&gt;%\n  ggplot(aes(x = x, y = y)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    facet_wrap(~set, ncol = 4)\n\n\n\nAll of these data have a correlation coefficient of about 0.8 – always visualise your data."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#simple-linear-regression-model",
    "href": "lectures/L07/Lecture-07.html#simple-linear-regression-model",
    "title": "Regression: modelling",
    "section": "Simple linear regression model",
    "text": "Simple linear regression model\nWe want to predict a response \\(Y\\) based on a predictor \\(x\\) for \\(i\\) number of observations:\n\\[Y_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}\\]\nwhere\n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\n\n\\(Y_i\\), the response, is an observed value of the dependent variable.\n\\(\\beta_0\\), the constant, is the population intercept and is fixed.\n\\(\\beta_1\\) is the population slope parameter, and like \\(\\beta_0\\), is also fixed.\n\\(\\epsilon_i\\) is the error associated with predictions of \\(y_i\\), and unlike \\(\\beta_0\\) or \\(\\beta_1\\), it is not fixed.\n\nBecause \\(\\epsilon_i\\) is the only part of the equation that is not fixed, we associate it with the residual error (\\(observed-predicted\\)). It would also cover other aspects of error (e.g. sampling error, parallax error) but these are hard to discern."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#fitting-the-model",
    "href": "lectures/L07/Lecture-07.html#fitting-the-model",
    "title": "Regression: modelling",
    "section": "Fitting the model",
    "text": "Fitting the model\n\n\\(\\color{royalblue}{\\hat{y}_i}\\) is the predicted value of \\(y_i\\):\n\n\\[\\color{royalblue}{\\hat{y}_i} = \\beta_0 + \\beta_1 x_i\\]\n\nThe residual is the difference between the observed value of the response and the predicted value:\n\n\\[\\hat\\epsilon_i = y_i - \\color{royalblue}{\\hat{y}_i}\\]\n\nTherefore:\n\n\\[\\hat\\epsilon_i = y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)}\\]\n\nWe use the method of least squares and minimise the sum of the squared residuals (SS):\n\n\\[\\sum_{i=1}^n \\hat\\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2\\]"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#section",
    "href": "lectures/L07/Lecture-07.html#section",
    "title": "Regression: modelling",
    "section": "",
    "text": "Finding the minimum SS requires solving the following problem:\n\\[\\color{firebrick}{argmin_{\\beta_0, \\beta_1}} \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2\\]\nWe can find \\(\\beta_0\\) and \\(\\beta_1\\) analytically. We first find \\(\\beta_1\\):\n\\[ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} \\] And then substitute \\(\\beta_1\\) into the equation for \\(\\beta_0\\):\n\\[ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\]"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#numerical-fitting",
    "href": "lectures/L07/Lecture-07.html#numerical-fitting",
    "title": "Regression: modelling",
    "section": "Numerical fitting",
    "text": "Numerical fitting\nComputers use “random guesses” to find set of parameters that minimises objective function (SS) – more computationally efficient and applies beyond linear regression.\n\nsource"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#fitting-a-model-in-r-is-easy-with-lm",
    "href": "lectures/L07/Lecture-07.html#fitting-a-model-in-r-is-easy-with-lm",
    "title": "Regression: modelling",
    "section": "Fitting a model in R is easy with lm()",
    "text": "Fitting a model in R is easy with lm()\n\nfit &lt;- lm(child ~ parent, data = Galton)\n\nThat’s it – the model has been fitted.\nBut there is a process similar to HATPC (hypothesis, assumptions, test, p-value, conclusions)."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#define-the-hypothesis",
    "href": "lectures/L07/Lecture-07.html#define-the-hypothesis",
    "title": "Regression: modelling",
    "section": "Define the hypothesis",
    "text": "Define the hypothesis\n\\[H_0: \\beta_1=0\\]\n\\[H_1: \\beta_1 \\neq 0\\]\nThe null model is a line with no slope (i.e. flat or horizontal) at the mean of the child height (\\(\\bar{y}\\) = 68 inches).\n\n\nCode\nlibrary(dplyr)\nnull_model &lt;- Galton %&gt;%\n  lm(child ~ 1, data = .) %&gt;%\n  broom::augment(Galton)\nlin_model &lt;- Galton %&gt;%\n  lm(child ~ parent, data = .) %&gt;%\n  broom::augment(Galton)\nmodels &lt;- bind_rows(null_model, lin_model) %&gt;%\n  mutate(model = rep(c(\"Null model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 1\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 1\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\") +\n  theme_classic()"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions",
    "href": "lectures/L07/Lecture-07.html#assumptions",
    "title": "Regression: modelling",
    "section": "Assumptions",
    "text": "Assumptions\nThe data must meet certain criteria, which we often call assumptions. They can be remembered using LINE:\n\nLinearity. The relationship between \\(y\\) and \\(x\\) is linear.\nIndependence. The errors \\(\\epsilon\\) are independent.\nNormal. The errors \\(\\epsilon\\) are normally distributed.\nEqual Variance. At each value of \\(x\\), the variance of \\(y\\) is the same i.e. homoskedasticity, or constant variance.\n\n\n\n\n\n\n\nTip\n\n\nAll but the independence assumption can be assessed using diagnostic plots."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions-with-base-r-plot",
    "href": "lectures/L07/Lecture-07.html#assumptions-with-base-r-plot",
    "title": "Regression: modelling",
    "section": "Assumptions with base R plot()",
    "text": "Assumptions with base R plot()\n\npar(mfrow= c(2, 2)) # plots combined into 2x2 grid\nplot(fit)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions-with-ggfortify-package-and-autoplot",
    "href": "lectures/L07/Lecture-07.html#assumptions-with-ggfortify-package-and-autoplot",
    "title": "Regression: modelling",
    "section": "Assumptions with ggfortify package and autoplot()",
    "text": "Assumptions with ggfortify package and autoplot()\n\nlibrary(ggfortify)\nautoplot(fit)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions-using-performance",
    "href": "lectures/L07/Lecture-07.html#assumptions-using-performance",
    "title": "Regression: modelling",
    "section": "Assumptions using performance",
    "text": "Assumptions using performance\n(Also provides a guide on what to check for in the assumption plot)\n\nlibrary(performance)\nperformance::check_model(fit) # check all assumptions\nperformance::check_model(fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-linearity",
    "href": "lectures/L07/Lecture-07.html#assumption-linearity",
    "title": "Regression: modelling",
    "section": "Assumption: Linearity",
    "text": "Assumption: Linearity\n\n\nPrior knowledge and visual inspection comes into play. Does the relationship look approximately linear?\n\n\nCode\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\nThe linearity assumption can be checked again by looking at a plot of the residuals against \\(x\\) (i.e. parent height).\n\nperformance::check_model(fit, check = \"linearity\")\n\n\n\n\n\n\n\n\n\n\nWhere the green reference line is &gt; 0, the model underestimates, and where it is &lt; 0, it overestimates.\nIf the linearity assumption is violated, we should not be fitting a linear model – transform or use a nonlinear model."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-independence",
    "href": "lectures/L07/Lecture-07.html#assumption-independence",
    "title": "Regression: modelling",
    "section": "Assumption: Independence",
    "text": "Assumption: Independence\nThis assumption is addressed during experimental design, but issues like correlation between errors and patterns occurring due to time are possible if:\n\nObservations of the same subject are related i.e. multicollinearity\nTime-series data, if the same subjects are sampled i.e. autocorrelation"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-normality",
    "href": "lectures/L07/Lecture-07.html#assumption-normality",
    "title": "Regression: modelling",
    "section": "Assumption: Normality",
    "text": "Assumption: Normality\n\n\nFor a given value of \\(x\\), the residuals should be normally distributed. In a scatterplot of \\(x\\) and \\(y\\), the points would appear evenly distributed (linear and no fanning).\n\n\n\nperformance::check_model(fit, check = c(\"normality\", \"qq\"))\n\n\n\n\n\n\n\n\n\n\nHow to interpret a QQ plot\nQQ plot interpretation"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assessing-normality-using-residuals",
    "href": "lectures/L07/Lecture-07.html#assessing-normality-using-residuals",
    "title": "Regression: modelling",
    "section": "Assessing normality using residuals",
    "text": "Assessing normality using residuals\n\nLight-tailed: small variance in residuals, resulting in a narrow distribution\nHeavy-tailed: many extreme positive and negative residuals, resulting in a wide distribution\nLeft-skewed (n shape): more data falls to the left of the mean\nRight-skewed (u shape): more data falls to the right of the mean\n\n\n\nHeavy-tailed, left-skewed.\n\n\nCode\nset.seed(1028)\nx &lt;- rnorm(100)\ny &lt;- 2 + 5 * x + rchisq(100, df = 3) * -1\ndf &lt;- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"qq\")))\n\n\n\n\n\n\n\n\n\n\nLight-tailed, right-skewed.\n\n\nCode\nset.seed(1028)\nx &lt;- rnorm(100)\ny &lt;- 2 + 5 * x + rnbinom(100, 10, .5)\ndf &lt;- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"qq\")))"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#asumption-equal-variances",
    "href": "lectures/L07/Lecture-07.html#asumption-equal-variances",
    "title": "Regression: modelling",
    "section": "Asumption: Equal variances",
    "text": "Asumption: Equal variances\n\nperformance::check_model(fit, check = c(\"homogeneity\", \"outliers\"))\n\n\nOutliers are not a strict assumption, but they will affect the model fit."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#what-is-a-standardised-residual",
    "href": "lectures/L07/Lecture-07.html#what-is-a-standardised-residual",
    "title": "Regression: modelling",
    "section": "What is a standardised residual?",
    "text": "What is a standardised residual?\n\nThe standardised residual is the residual divided by the standard error of the residual (normalised).\n\n\\[Standardised\\ residual = \\frac{Residual}{Standard\\ error\\ of\\ the\\ residual}\\]\n\nThe mean of the residuals is 0 in linear regression\nA standardised residual of 2 or above suggests the point is an outlier (far from the regression line)\nSpread should be random i.e. no pattern (fanning, W), which indicates equal variances"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#anova-and-linear-regression",
    "href": "lectures/L07/Lecture-07.html#anova-and-linear-regression",
    "title": "Regression: modelling",
    "section": "ANOVA and linear regression",
    "text": "ANOVA and linear regression\nANOVA is a variation of linear regression – both partition variance into sum of squares for residuals (variance explained) and sum of squares for error (variance not explained) aka the components of the F-statistic.\n\n\nANOVA Output\n\nfit &lt;- lm(child ~ parent, data = Galton)\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: child\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nparent      1 1236.9 1236.93  246.84 &lt; 2.2e-16 ***\nResiduals 926 4640.3    5.01                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nparent Sum Sq: the variation that parent explains in the child variable\nResiduals Mean Sq: variation (per degree of freedom) that the model does not explain\nThe F-value is the ratio, i.e. does parent explain enough variation in child to be considered significant?\n\n\\[\\text{F-value} = \\frac{\\text{parent Sum Sq}}{\\text{Residuals Mean Sq}} = \\frac{1236.9}{5.01} = 246.84 \\]\n\nRegression Output\n\nfit &lt;- lm(child ~ parent, data = Galton)\nsummary(fit)\n\n# F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#anova-and-linear-regression-1",
    "href": "lectures/L07/Lecture-07.html#anova-and-linear-regression-1",
    "title": "Regression: modelling",
    "section": "ANOVA and linear regression",
    "text": "ANOVA and linear regression\nANOVA is a variation of linear regression – both partition variance into sum of squares for residuals (variance explained) and sum of squares for error (variance not explained) aka the components of the F-statistic.\nANOVA Output\nThe ANOVA suggests that the main effect of parent is statistically significant and large (F(1, 926) = 246.84, p &lt; .001)\nRegression Output\nWe fitted a linear model (estimated using OLS) to predict child with parent (formula: child ~ parent). The model explains a statistically significant and moderate proportion of variance (R2 = 0.21, F(1, 926) = 246.84, p &lt; .001). Within this model, the effect of parent is statistically significant and positive (\\(\\beta_1\\) = 0.65, 95% CI [0.57, 0.73], t(926) = 15.71, p &lt; .001).\n\n\n\n\n\n\nNote\n\n\nFor simple linear regression, the significance of the predictor (i.e. child) is the same as the model significance."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#model-fit-1",
    "href": "lectures/L07/Lecture-07.html#model-fit-1",
    "title": "Regression: modelling",
    "section": "Model fit",
    "text": "Model fit\n\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\[\\widehat{child} = 23.9 + 0.65 \\cdot parent\\]\nFor every unit change in parent (i.e. 1 inch), we expect a 0.65 unit change in child.\nHow much variation is explained? R2 = 0.21 = 21%\n\nMultiple R2: proportion of variance in the response variable explained by the model.\nAdjusted R2: as above but adjusted for the number of predictors in the model.\n\nFor multiple linear regression\nIt only increases if the new term improves the model more than would be expected by chance\nAlways lower than multiple R2"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#making-predictions",
    "href": "lectures/L07/Lecture-07.html#making-predictions",
    "title": "Regression: modelling",
    "section": "Making predictions",
    "text": "Making predictions\nWhat is the predicted child height for a parent height of 70 inches?\n\nchild &lt;- 23.9 + 0.65 * 70\nchild\n\n[1] 69.4\n\n\nWe use predict() to make predictions – it takes in the lm() model, recreates the equation and applies it to new data.\n\npredict(fit, data.frame(parent = 70)) # using 70 as this is the value we want to sub in and predict\n\n       1 \n69.18187 \n\n\n\n\n\n\n\n\nNote\n\n\nHow good is our prediction actually? What if we had more parents and children, would the equation still hold up? We cover this in Week 9."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#what-if-assumptions-are-not-met",
    "href": "lectures/L07/Lecture-07.html#what-if-assumptions-are-not-met",
    "title": "Regression: modelling",
    "section": "What if assumptions are not met?",
    "text": "What if assumptions are not met?\nViolations of…\n\nLinearity can cause systematically wrong predictions\nHomoskedasticity makes it difficult to estimate “true” standard deviation of errors (i.e. noisy estimates)\nNormality can compromise inferences and hypothesis testing"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#how-do-we-solve-these-problems",
    "href": "lectures/L07/Lecture-07.html#how-do-we-solve-these-problems",
    "title": "Regression: modelling",
    "section": "How do we solve these problems?",
    "text": "How do we solve these problems?\n\nUse less restrictive (but more complicated) models, e.g. generalised linear models, non-parametric techniques (ENVX3002)\nPerform variance corrections (complicated)\nTransform the response variable (\\(y\\)) to stabilise variance and correct normality\nTransform the predictor variable (\\(x\\)) if issues still exist in the diagnostics\n\n\n\n\n\n\n\nNote\n\n\nWe can also perform transformations to improve the model fit, but beware of overfitting – we want to make reasonable predictions, not fit the data!"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#example-air-quality",
    "href": "lectures/L07/Lecture-07.html#example-air-quality",
    "title": "Regression: modelling",
    "section": "Example: air quality",
    "text": "Example: air quality\n\n\nDaily air quality measurements in New York, May to September 1973.\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nWe start with one variable: is ozone concentration influenced by temperature?\n\n\nCode\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-checks",
    "href": "lectures/L07/Lecture-07.html#assumption-checks",
    "title": "Regression: modelling",
    "section": "Assumption checks",
    "text": "Assumption checks\n\nfit &lt;- lm(Ozone ~ Temp, data = airquality)\nperformance::check_model(fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions\n\n\nIs a simple linear model appropriate? Depends on your threshold for what is acceptable."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#backtransforming-fyi",
    "href": "lectures/L07/Lecture-07.html#backtransforming-fyi",
    "title": "Regression: modelling",
    "section": "Backtransforming – FYI",
    "text": "Backtransforming – FYI\nA log transformation (natural or a base) is relatively easy to back-transform.\n\\[\\widehat{log(Ozone)}=\\color{royalblue}{-1.8380 + 0.0675 \\times Temp}\\] \\[\\widehat{Ozone}=e^{-1.8380 + 0.0675 \\times Temp}=e^{-1.8380} \\times e^{0.0675 \\times Temp}\\] But given we are focused on a 1-unit change of Temp, \\(\\widehat{Ozone}\\) changes by \\(e^{0.0675} = 1.07\\) times.\nIf this had been a sqrt() transformation…\n\\[\\widehat{\\sqrt{Ozone}}=-1.8380 + 0.0675 \\times Temp\\] \\[\\widehat{Ozone}=(-1.8380 + 0.0675 \\times Temp)^2 = 3.3782−(0.2481×Temp)+(0.0675×Temp)^2\\]"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#interpreting-log-transformations-fyi",
    "href": "lectures/L07/Lecture-07.html#interpreting-log-transformations-fyi",
    "title": "Regression: modelling",
    "section": "Interpreting log transformations – FYI",
    "text": "Interpreting log transformations – FYI\n\n\nLog-linear: \\(Log(Y)=\\beta_0+\\beta_1x\\)\n\nAn increase of \\(x\\) by 1 unit corresponds to a \\(\\beta_1\\) unit increase in \\(log(Y)\\)\nAn increase of \\(x\\) by 1 unit corresponds to approximately a \\(\\beta_1 \\times 100\\%\\) increase in \\(Y\\)\n\n\n\n\n\nLinear-log: \\(Y=\\beta_0+\\beta_1log(x)\\)\n\nAn increase of \\(1\\%\\) in \\(x\\) corresponds to a \\(\\frac{\\beta_1}{100}\\) increase in \\(Y\\)\n\n\n\n\n\nLog-log: \\(Log(Y)=\\beta_0+\\beta_1log(x)\\)\n\nAn increase of \\(1\\%\\) in \\(x\\) corresponds to a \\(\\beta_1\\%\\) increase in \\(Y\\)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#percent-change-with-ln-transformation-fyi",
    "href": "lectures/L07/Lecture-07.html#percent-change-with-ln-transformation-fyi",
    "title": "Regression: modelling",
    "section": "Percent change with \\(ln\\) transformation – FYI",
    "text": "Percent change with \\(ln\\) transformation – FYI\nInterpreting as a percent change can be more meaningful - it can be done with any log transformation (substitute \\(e\\) below for 10 or any other base), but the quick approximation only works with natural log transformations.\nIf \\(y\\) has been transformed with a natural log (log(y)), for a one-unit increase in \\(x\\) the percent change in \\(y\\) (not log(y)) is calculated with:\n\\[\\Delta y \\% = 100 \\cdot (e^{\\beta_1}-1)\\]\nIf \\(\\beta_1\\) is small (i.e. \\(-0.25 &lt; \\beta_1 &lt; 0.25\\)), then: \\(e^{\\beta_1} \\approx 1 + \\beta_1\\). So \\(\\Delta y \\% \\approx 100 \\cdot \\beta_1\\).\n\n\n\n\n\n\n\n\n\n\nβ\nExact \\((e^{\\beta} - 1)\\)%\nApproximate \\(100 \\cdot \\beta\\)\n\n\n\n\n-0.25\n-22.13\n-25\n\n\n-0.1\n-9.52\n-10\n\n\n0.01\n1.01\n1\n\n\n0.1\n10.52\n10\n\n\n0.25\n28.41\n25\n\n\n0.5\n64.87\n50\n\n\n2\n638.91\n200\n\n\n\n\n\n\\(y\\) transformed: a one-unit increase in \\(x\\) is approximately a \\(\\beta_1\\)% change in \\(y\\).\n\\(x\\) transformed: a 1% increase in \\(x\\) is approximately a \\(0.01 \\cdot \\beta_1\\) change in \\(y\\).\nBoth \\(x\\) and \\(y\\) transformed: a 1% increase in x is approximately a \\(\\beta_1\\)% change in y."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#transforming-ozone",
    "href": "lectures/L07/Lecture-07.html#transforming-ozone",
    "title": "Regression: modelling",
    "section": "Transforming Ozone",
    "text": "Transforming Ozone\nLet’s transform Ozone using the natural log (log()).\n\nfit_log &lt;- lm(log(Ozone) ~ Temp, data = airquality)\n\n\n\n\nBefore\n\n\nCode\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (ppb)\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(subtitle = paste(\"Correlation:\", round(cor(airquality$Temp, airquality$Ozone), 2)))\n\n\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\nCode\nggplot(airquality, aes(x = Temp, y = log(Ozone))) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"log(Ozone) (ppb)\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(subtitle = paste(\"Correlation:\", round(cor(airquality$Temp, log(airquality$Ozone)), 2)))"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-linearity-1",
    "href": "lectures/L07/Lecture-07.html#assumption-linearity-1",
    "title": "Regression: modelling",
    "section": "Assumption: Linearity",
    "text": "Assumption: Linearity\n\n\nBefore\n\n\nCode\nautoplot(fit, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\nCode\nautoplot(fit_log, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-normality-1",
    "href": "lectures/L07/Lecture-07.html#assumption-normality-1",
    "title": "Regression: modelling",
    "section": "Assumption: Normality",
    "text": "Assumption: Normality\n\n\nBefore\n\n\nCode\nautoplot(fit, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\nCode\nautoplot(fit_log, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumption-equal-variances",
    "href": "lectures/L07/Lecture-07.html#assumption-equal-variances",
    "title": "Regression: modelling",
    "section": "Assumption: Equal variances",
    "text": "Assumption: Equal variances\n\n\nBefore\n\n\nCode\nautoplot(fit, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\nCode\nautoplot(fit_log, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#is-transforming-better",
    "href": "lectures/L07/Lecture-07.html#is-transforming-better",
    "title": "Regression: modelling",
    "section": "Is transforming better?",
    "text": "Is transforming better?\n\n\nBefore\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\nAfter\n\nsummary(fit_log)\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.14469 -0.33095  0.02961  0.36507  1.49421 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.83797    0.45100  -4.075 8.53e-05 ***\nTemp         0.06750    0.00575  11.741  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5848 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.5473,    Adjusted R-squared:  0.5434 \nF-statistic: 137.8 on 1 and 114 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#multiple-linear-regression",
    "href": "lectures/L07/Lecture-07.html#multiple-linear-regression",
    "title": "Regression: modelling",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe MLR model\n\\[Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + \\epsilon\\]\nwhere\n\na response variable (\\(Y\\)) which we wish to predict using predictor variables (\\(x_k\\))\n\\(\\beta_0\\) is the y-intercept\n\\(\\beta_k\\) is the partial regression coefficient associated with the \\(k^{th}\\) predictor variable\n\\(\\epsilon\\) is error and \\(\\epsilon \\sim N(0,\\ \\sigma^2)\\)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#can-we-use-more-predictors",
    "href": "lectures/L07/Lecture-07.html#can-we-use-more-predictors",
    "title": "Regression: modelling",
    "section": "Can we use more predictors?",
    "text": "Can we use more predictors?\n\npsych::pairs.panels(airquality)\n\n\nCan we improve the current model by adding wind and solar radiation as additional predictors?"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#can-we-use-more-predictors-1",
    "href": "lectures/L07/Lecture-07.html#can-we-use-more-predictors-1",
    "title": "Regression: modelling",
    "section": "Can we use more predictors?",
    "text": "Can we use more predictors?\nFrom:\n\\[log(size)_i = \\beta_0 + \\beta_1Temp_i + \\epsilon_i\\]\nTo:\n\\[log(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i\\]"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#can-we-use-more-predictors-2",
    "href": "lectures/L07/Lecture-07.html#can-we-use-more-predictors-2",
    "title": "Regression: modelling",
    "section": "Can we use more predictors?",
    "text": "Can we use more predictors?\n\\[log(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i\\]\n\nmulti_fit &lt;- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#assumptions-1",
    "href": "lectures/L07/Lecture-07.html#assumptions-1",
    "title": "Regression: modelling",
    "section": "Assumptions",
    "text": "Assumptions\n\nperformance::check_model(multi_fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions\n\n\nThere is one additional assumption for multiple linear regression. Collinearity is when two or more predictors are very highly correlated. If the predictors are basically identical, the model cannot distinguish how much variability each explains. (Correlations in previous slides look fine)."
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#hypothesis",
    "href": "lectures/L07/Lecture-07.html#hypothesis",
    "title": "Regression: modelling",
    "section": "Hypothesis",
    "text": "Hypothesis\nFor multiple linear regression, there are two hypothesis tests:\n\nIndividual predictors, where the significance of each predictor is tested via t-tests\n\n\\[H_0: \\beta_k = 0\\] \\[H_1: \\beta_k \\neq 0\\]\n\nThe overall model, which is tested with an F-test (to get F-stat). \\(H_0\\) is an intercept-only model (i.e. the mean), so if at least one predictor is useful, the model is better than the intercept-only model.\n\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\] \\[H_1: \\text{At least one } \\beta_k \\neq 0\\]"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#model-fit-2",
    "href": "lectures/L07/Lecture-07.html#model-fit-2",
    "title": "Regression: modelling",
    "section": "Model Fit",
    "text": "Model Fit\n\nsummary(multi_fit)\n\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\nModel equation:\n\\[\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind\\]"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#interpretation",
    "href": "lectures/L07/Lecture-07.html#interpretation",
    "title": "Regression: modelling",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind\\]\nHolding all other variables constant:\n\nA one degree (°F) increase in Temp is associated with a 4.9% increase in Ozone concentration.\nA one unit increase in Solar.R is associated with a 0.25% increase in Ozone concentration.\nA one unit increase in Wind is associated with a 6.2% decrease in Ozone concentration.\n\nAutomating extracting the model equation into latex using extract_eq() from the package equatiomatic:\n\nequatiomatic::extract_eq(multi_fit, use_coefs = TRUE, coef_digits = 3) |&gt; print()\n\n$$\n\\operatorname{\\widehat{log(Ozone)}} = -0.262 + 0.049(\\operatorname{Temp}) + 0.003(\\operatorname{Solar.R}) - 0.062(\\operatorname{Wind})\n$$"
  },
  {
    "objectID": "lectures/L07/Lecture-07.html#is-mlr-model-better",
    "href": "lectures/L07/Lecture-07.html#is-mlr-model-better",
    "title": "Regression: modelling",
    "section": "Is MLR model better?",
    "text": "Is MLR model better?\n\nsjPlot::tab_model(fit_log, multi_fit, digits = 4, show.ci = FALSE)\n\n\n\n\n \nlog(Ozone)\nlog(Ozone)\n\n\nPredictors\nEstimates\np\nEstimates\np\n\n\n(Intercept)\n-1.8380\n&lt;0.001\n-0.2621\n0.637\n\n\nTemp\n0.0675\n&lt;0.001\n0.0492\n&lt;0.001\n\n\nSolar R\n\n\n0.0025\n&lt;0.001\n\n\nWind\n\n\n-0.0616\n&lt;0.001\n\n\nObservations\n116\n111\n\n\nR2 / R2 adjusted\n0.547 / 0.543\n0.664 / 0.655\n\n\n\n\n\n\nThe adjusted \\(R^2\\) is higher for the MLR model…\nInterpretation of \\(R^2\\) is the same as for simple linear regression: how much of the variation in the response variable is explained by the model\nAre all the variables/predictors needed? (next week)"
  },
  {
    "objectID": "lectures/L05/index.html",
    "href": "lectures/L05/index.html",
    "title": "Lecture 05",
    "section": "",
    "text": "Lecture 05 – Experimental design\nDownload PDF file.Download PDF\nDownload Lecture Notes"
  },
  {
    "objectID": "lectures/L03/index.html",
    "href": "lectures/L03/index.html",
    "title": "Lecture 03",
    "section": "",
    "text": "Lecture 03a – t-tests Full Screen | PDF\n\nLecture 03b – One-way ANOVA Full Screen | PDF"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#william-gosset-1908",
    "href": "lectures/L03/Lecture-03a.html#william-gosset-1908",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "William Gosset (1908)",
    "text": "William Gosset (1908)\n\n\n\n\n\nWilliam Gosset\n\n\n\n\nWorked as a statistician at Guinness Brewery in Dublin\nPublished under the pseudonym “Student” (company policy prevented use of real name)\nDeveloped the \\(t\\)-distribution and \\(t\\)-tests for small sample sizes\nNeeded to compare barley varieties with limited data"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#ronald-fisher-1925",
    "href": "lectures/L03/Lecture-03a.html#ronald-fisher-1925",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Ronald Fisher (1925)",
    "text": "Ronald Fisher (1925)\n\n\n\n\n\nRonald Fisher\n\n\n\n\nExtended and formalised Gosset’s work\nIntroduced the concept of degrees of freedom\nDeveloped Analysis of Variance (ANOVA) – next lecture!\nEstablished many foundations of modern statistical inference"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#the-problem",
    "href": "lectures/L03/Lecture-03a.html#the-problem",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "The problem",
    "text": "The problem\n\nWeights of two breeds of cattle are to be compared\n12 samples were randomly taken from Breed 1, and 15 from Breed 2\nAre there differences in the mean weight between the two breeds?\n\n\n\ncattle &lt;- read.csv(\"data/cattle.csv\")\ncattle\n\n   Breed1 Breed2\n1   187.6  148.1\n2   180.3  146.2\n3   198.6  152.8\n4   190.7  135.3\n5   196.3  151.2\n6   203.8  146.3\n7   190.2  163.5\n8   201.0  146.6\n9   194.7  162.4\n10  221.1  140.2\n11  186.7  159.4\n12  203.1  181.8\n13     NA  165.1\n14     NA  165.0\n15     NA  141.6"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#reshaping-the-data",
    "href": "lectures/L03/Lecture-03a.html#reshaping-the-data",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Reshaping the data",
    "text": "Reshaping the data\nThe data is in wide format – we need to convert it to long format for analysis.\n\ncattle &lt;- read.csv(\"data/cattle.csv\") |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"breed\",\n    values_to = \"weight\"\n  ) |&gt;\n  mutate(breed = as.factor(breed)) |&gt;\n  drop_na()\n\ncattle\n\n# A tibble: 27 × 2\n   breed  weight\n   &lt;fct&gt;   &lt;dbl&gt;\n 1 Breed1   188.\n 2 Breed2   148.\n 3 Breed1   180.\n 4 Breed2   146.\n 5 Breed1   199.\n 6 Breed2   153.\n 7 Breed1   191.\n 8 Breed2   135.\n 9 Breed1   196.\n10 Breed2   151.\n# ℹ 17 more rows"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#visualising-the-data",
    "href": "lectures/L03/Lecture-03a.html#visualising-the-data",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Visualising the data",
    "text": "Visualising the data\n\nggplot(cattle, aes(breed, weight)) +\n  geom_boxplot() +\n  labs(x = \"Breed\", y = \"Weight (kg)\")"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#two-sample-t-test-model",
    "href": "lectures/L03/Lecture-03a.html#two-sample-t-test-model",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Two-sample \\(t\\)-test model",
    "text": "Two-sample \\(t\\)-test model\nObserved data = Group Mean + Random Error (residuals)\n\\[y_{ij} = \\mu_i + \\varepsilon_{ij}\\]\n\n\\(i = 1, 2\\) (group); \\(j = 1, 2, \\ldots, n_i\\) (replicate)\n\nIn the cattle example:\n\n\\(\\mu_1\\) = mean body weight (kg) for Breed 1\n\\(\\mu_2\\) = mean body weight (kg) for Breed 2"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#hypotheses",
    "href": "lectures/L03/Lecture-03a.html#hypotheses",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nNull hypothesis: \\(H_0: \\mu_1 = \\mu_2\\)\nAlternative hypothesis: \\(H_1: \\mu_1 \\neq \\mu_2\\)\n\n\nTest statistic:\n\\[t = \\frac{\\bar{y}_1 - \\bar{y}_2}{\\sqrt{s^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} = \\frac{\\bar{y}_1 - \\bar{y}_2}{SE(\\bar{y}_1 - \\bar{y}_2)} = \\frac{\\Delta \\text{ in mean}}{SE \\text{ of the } \\Delta \\text{ in mean}}\\]\nDegrees of freedom: \\(n_1 + n_2 - 2\\)"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#shape-changes-with-degrees-of-freedom",
    "href": "lectures/L03/Lecture-03a.html#shape-changes-with-degrees-of-freedom",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Shape changes with degrees of freedom",
    "text": "Shape changes with degrees of freedom\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 300)\n\nt_data &lt;- map_dfr(c(1, 2, 5, 10, 30), function(df) {\n  tibble(x = x, density = dt(x, df), df = paste(\"df =\", df))\n})\n\nnormal_data &lt;- tibble(x = x, density = dnorm(x), df = \"Normal\")\n\nbind_rows(t_data, normal_data) |&gt;\n  mutate(df = fct_relevel(df, \"df = 1\", \"df = 2\", \"df = 5\", \"df = 10\", \"df = 30\", \"Normal\")) |&gt;\n  ggplot(aes(x, density, colour = df, linetype = df)) +\n  geom_line(linewidth = 1) +\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"dashed\")) +\n  labs(x = \"t\", y = \"Density\", colour = NULL, linetype = NULL) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nNote\n\n\nAs degrees of freedom increase, the \\(t\\)-distribution approaches the standard normal distribution."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#one-tailed-vs-two-tailed-tests",
    "href": "lectures/L03/Lecture-03a.html#one-tailed-vs-two-tailed-tests",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "One-tailed vs two-tailed tests",
    "text": "One-tailed vs two-tailed tests\n\n\nTwo-tailed: tests for a difference in either direction (\\(\\mu_1 \\neq \\mu_2\\))\nOne-tailed: tests for a difference in a specific direction (\\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\))\nIn most cases, we use two-tailed tests unless we have a strong a priori reason for one direction."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#group-means-and-standard-deviations",
    "href": "lectures/L03/Lecture-03a.html#group-means-and-standard-deviations",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Group means and standard deviations",
    "text": "Group means and standard deviations\n\ncattle_summary &lt;- cattle |&gt;\n  group_by(breed) |&gt;\n  summarise(\n    n = n(),\n    mean_wt = mean(weight, na.rm = TRUE),\n    sd_wt = sd(weight, na.rm = TRUE)\n  )\n\ncattle_summary\n\n# A tibble: 2 × 4\n  breed      n mean_wt sd_wt\n  &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Breed1    12    196.  10.6\n2 Breed2    15    154.  12.3"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#computing-the-t-statistic",
    "href": "lectures/L03/Lecture-03a.html#computing-the-t-statistic",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Computing the \\(t\\)-statistic",
    "text": "Computing the \\(t\\)-statistic\n\n# Extract values\nn1 &lt;- cattle_summary$n[1]\nn2 &lt;- cattle_summary$n[2]\nm1 &lt;- cattle_summary$mean_wt[1]\nm2 &lt;- cattle_summary$mean_wt[2]\ns1 &lt;- cattle_summary$sd_wt[1]\ns2 &lt;- cattle_summary$sd_wt[2]\n\n# Pooled variance\nsp2 &lt;- ((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2)\n\n# t-statistic\nt_stat &lt;- (m1 - m2) / sqrt(sp2 * (1/n1 + 1/n2))\nt_stat\n\n[1] 9.462409\n\n\n\n\n# Degrees of freedom\ndf &lt;- n1 + n2 - 2\n\n# p-value (two-tailed)\np_val &lt;- 2 * pt(-abs(t_stat), df)\np_val\n\n[1] 9.663383e-10"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#the-easy-way",
    "href": "lectures/L03/Lecture-03a.html#the-easy-way",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "The easy way",
    "text": "The easy way\n\nt.test(weight ~ breed, data = cattle, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  weight by breed\nt = 9.4624, df = 25, p-value = 9.663e-10\nalternative hypothesis: true difference in means between group Breed1 and group Breed2 is not equal to 0\n95 percent confidence interval:\n 33.23011 51.71989\nsample estimates:\nmean in group Breed1 mean in group Breed2 \n             196.175              153.700 \n\n\n\n\n\n\n\n\nTip\n\n\nvar.equal = TRUE performs the pooled (Student’s) \\(t\\)-test, which assumes equal variances. This matches our manual calculation."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#three-key-assumptions",
    "href": "lectures/L03/Lecture-03a.html#three-key-assumptions",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Three key assumptions",
    "text": "Three key assumptions\n\nEqual variances: \\(\\sigma_1^2 \\approx \\sigma_2^2\\)\nNormality: \\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\)\nIndependence of observations"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#checking-equal-variances",
    "href": "lectures/L03/Lecture-03a.html#checking-equal-variances",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Checking equal variances",
    "text": "Checking equal variances\nThe only difference between the two groups should be the location of the mean – all else should remain the same.\nGeneral guide: \\(\\frac{\\text{larger SD}}{\\text{smaller SD}} &lt; 2.0\\)\n\nmax(cattle_summary$sd_wt) / min(cattle_summary$sd_wt)\n\n[1] 1.158756\n\n\n\nThe ratio is well under 2, so we can assume equal variances."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#checking-normality-visual",
    "href": "lectures/L03/Lecture-03a.html#checking-normality-visual",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Checking normality: visual",
    "text": "Checking normality: visual\nggplot(cattle, aes(breed, weight)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot\", x = \"Breed\", y = \"Weight (kg)\")\nggplot(cattle, aes(weight)) +\n  geom_histogram(bins = 8) +\n  labs(title = \"Histogram\", x = \"Weight (kg)\", y = \"Count\")"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#checking-normality-shapiro-wilk-test",
    "href": "lectures/L03/Lecture-03a.html#checking-normality-shapiro-wilk-test",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Checking normality: Shapiro-Wilk test",
    "text": "Checking normality: Shapiro-Wilk test\n\nshapiro.test(cattle$weight)\n\n\n    Shapiro-Wilk normality test\n\ndata:  cattle$weight\nW = 0.93704, p-value = 0.103\n\n\n\nIf \\(p &gt; 0.05\\), the data is not significantly different from a normal distribution, i.e. we can assume normality."
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#what-if-assumptions-are-not-met",
    "href": "lectures/L03/Lecture-03a.html#what-if-assumptions-are-not-met",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "What if assumptions are not met?",
    "text": "What if assumptions are not met?\n\nEqual variances not met – use Welch’s \\(t\\)-test (the default in R’s t.test() when var.equal = FALSE)\nNormality not met – if \\(N &gt; 30\\), we can often assume normality anyway (Central Limit Theorem)\nIndependence not met – consider a paired \\(t\\)-test (e.g. before/after measurements on the same subjects)"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#key-points",
    "href": "lectures/L03/Lecture-03a.html#key-points",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Key points",
    "text": "Key points\n\nThe two-sample \\(t\\)-test compares means of two independent groups\nThe test statistic follows a \\(t\\)-distribution with \\(n_1 + n_2 - 2\\) degrees of freedom\nAlways check assumptions: equal variances, normality, independence\nUse t.test() in R for quick analysis\n\n\nNext up\n\nWhat if we have more than two groups? We need ANOVA!"
  },
  {
    "objectID": "lectures/L03/Lecture-03a.html#thanks",
    "href": "lectures/L03/Lecture-03a.html#thanks",
    "title": "Lecture 03a – \\(t\\)-Tests",
    "section": "Thanks!",
    "text": "Thanks!\nQuestions?\nThis presentation is based on the SOLES Quarto reveal.js template and is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling",
    "text": "Simple random sampling\n\n\nEach unit has an equal chance of being selected.\n\n\nNot always the case, but still a good technique."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-1",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-1",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling",
    "text": "Simple random sampling\nEach unit has an equal chance of being selected.\nNot always the case, but still a good technique."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-potential-problems",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-potential-problems",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling: potential problems",
    "text": "Simple random sampling: potential problems\n\nImagine tossing 10 random points onto a landscape.\n\n\nBy pure chance…\n\nWe might miss some important areas entirely\nOr sample some areas too much\n\n\n\nThis is more likely when:\n\nSample size is small\nThe landscape has distinct zones"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-sampling-theoretical-example",
    "href": "lectures/L02/Lecture-02b.html#simple-random-sampling-theoretical-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random sampling: theoretical example",
    "text": "Simple random sampling: theoretical example\n\nIf an area has:\n\n80% grassland\n20% wetland\n\n\n\nWith simple random sampling:\n\nWe expect ~8 samples in grassland, ~2 in wetland\nBut by chance, we might get:\n\n10 grassland, 0 wetland!\nOr 6 grassland, 4 wetland\n\n\n\n\nBut what if we have more information about the population?"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#soil-carbon-example",
    "href": "lectures/L02/Lecture-02b.html#soil-carbon-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Soil carbon example",
    "text": "Soil carbon example\n\nSoil carbon\n\n\n\nDifferent land types\n\nLand type A covers 62% of the area, land type B covers 38%\nType A has a higher chance of being selected with simple random sampling\nCan we use this information to our advantage?"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Stratified random sampling",
    "text": "Stratified random sampling\n3 steps\n\nDivide the population into homogeneous subgroups (strata).\nSample from each stratum using simple random sampling.\nPool (or combine) the estimates from each stratum to get an overall population estimate.\n\n\nReal-world example\nIf studying plant biodiversity in a national park:\n\nStep 1: Divide park into strata (e.g., forest, grassland, wetland)\nStep 2: Take random samples within each habitat type\nStep 3: Combine data to estimate overall biodiversity, giving proper weight to each habitat’s area"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#strata-rules",
    "href": "lectures/L02/Lecture-02b.html#strata-rules",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Strata rules",
    "text": "Strata rules\nStrata are…\n\nMutually exclusive and collectively exhaustive (simple explanation: every sample belongs to exactly one stratum – no overlaps, no leftovers)\nHomogeneous - Samples within a stratum should be similar to each other (less variable than the overall population)\nEach stratum must be sampled - The goal is to ensure every important group is represented"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#good-vs.-poor-stratification-choices",
    "href": "lectures/L02/Lecture-02b.html#good-vs.-poor-stratification-choices",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Good vs. poor stratification choices",
    "text": "Good vs. poor stratification choices\n\nEveryday examples\n\n\nGood strata\n\nUniversity students: Undergrad, Masters, PhD\nForest types: Deciduous, Coniferous, Mixed\nIncome levels: Low, Medium, High\n\n\nPoor strata choices\n\nInterests: Sports fans, Music lovers, Foodies (a person can be in multiple groups)\nWater quality: Clean, Somewhat polluted (too subjective, not clearly defined)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#advantages",
    "href": "lectures/L02/Lecture-02b.html#advantages",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Advantages",
    "text": "Advantages\nWe address:\n\nBias. Each stratum is sampled, so the sample is representative of the population.\nAccuracy. Each stratum is represented by a minimum number of sampling units.\nInsight. We can compare strata and make inferences about the population.\n\n\nDoes this make simple random sampling obsolete?\n\nNo. Still a good technique.\nWith large enough samples, the two methods will converge.\nChance of not selecting a unit from a stratum is always there, but reduces as the sample size increases."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#what-are-we-trying-to-achieve-with-our-calculations",
    "href": "lectures/L02/Lecture-02b.html#what-are-we-trying-to-achieve-with-our-calculations",
    "title": "Lecture 02b – Sampling designs II",
    "section": "What are we trying to achieve with our calculations?",
    "text": "What are we trying to achieve with our calculations?\nThe statistical journey\n\nOnce we have our stratified sample, we need to:\n\nEstimate the population central tendency: Calculate the pooled mean\nQuantify our uncertainty: Calculate the pooled standard error\nCreate an inference tool: Build a confidence interval\nMake decisions: Compare estimates, test hypotheses\n\nAll of these steps must account for our stratified design."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#the-statistical-workflow-for-stratified-sampling",
    "href": "lectures/L02/Lecture-02b.html#the-statistical-workflow-for-stratified-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "The statistical workflow for stratified sampling",
    "text": "The statistical workflow for stratified sampling\nFour key steps:\n\nPooled Mean (\\(\\bar{y}_{s}\\)): Sum of (stratum weight × stratum mean)\n\nBest estimate of the population parameter\n\nPooled Standard Error: \\[SE(\\bar{y}_{s}) = \\sqrt{\\sum w_i^2 \\times \\frac{s_i^2}{n_i}}\\]\n\nAccounts for stratum weights and within-stratum variability\n\nt-Critical Value: Based on \\(df = n - L\\) and α = 0.05\n\nAccounts for sample size in uncertainty estimates\n\nConfidence Interval: \\[\\text{Pooled mean} \\pm (t-\\text{critical} \\times SE(\\bar{y}_{s}))\\]\n\nRange likely containing true population mean"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#accounting-for-strata-using-weight",
    "href": "lectures/L02/Lecture-02b.html#accounting-for-strata-using-weight",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Accounting for strata using “weight”",
    "text": "Accounting for strata using “weight”\nWeighted estimates\n\nWe need to “weigh” the estimates from each stratum to account for the different stratum sizes and inclusion probabilities.\nMost of the time, we use the stratum size as the weight to calculate weighted estimates.\nThe overall population estimate is the sum of the weighted estimates from each stratum, i.e. we pool the individual strata information into a single, overall population estimate.\n\n\nExample\n\nA forest contains two types of trees: A and B, with 60% and 40% of the population, respectively.\nWe want to estimate the mean height of the trees.\nTake 10 height measurements, of which 7 are randomly selected from type A and 3 are randomly selected from type B.\nThe pooled estimate for the mean height of the trees is: \\[0.6 \\times \\text{average height of A} + 0.4 \\times \\text{average height of B}\\]"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#soil-carbon-data",
    "href": "lectures/L02/Lecture-02b.html#soil-carbon-data",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Soil carbon data",
    "text": "Soil carbon data\nOur case study\nSoil carbon content was measured at 7 locations across the area. The amounts were: 48, 56, 90, 78, 86, 71, 42 tonnes per hectare (t/ha).\n\n\nSetting up the data in R\nWe know which land type each sample came from:\n\nlandA &lt;- c(90, 78, 86, 71)  # stratum A samples (62% of the area)\nlandB &lt;- c(48, 56, 42)      # stratum B samples (38% of the area)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#pooled-mean-bar-y_s",
    "href": "lectures/L02/Lecture-02b.html#pooled-mean-bar-y_s",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Pooled mean \\(\\bar y_{s}\\)",
    "text": "Pooled mean \\(\\bar y_{s}\\)\n\nThe pooled mean is our best estimate of the overall population mean, taking into account the different stratum sizes.\n\n\n\\[\\bar{y}_{s} = \\sum_{i=1}^L \\bar{y}_i \\times w_i\\]\nIn simple terms:\n\nWe calculate the mean for each stratum separately (\\(\\bar{y}_i\\))\nWe multiply each stratum’s mean by its weight (\\(w_i\\))\nWe add these weighted means together to get the overall pooled mean"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#calculating-pooled-mean-soil-carbon-example",
    "href": "lectures/L02/Lecture-02b.html#calculating-pooled-mean-soil-carbon-example",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Calculating pooled mean: soil carbon example",
    "text": "Calculating pooled mean: soil carbon example\n\nWe first define the weights \\(w_i\\) for each stratum based on their area:\n\nweight &lt;- c(0.62, 0.38)  # 62% of area is land type A, 38% is land type B\n\n\n\nThen we calculate the weighted mean:\n\nweighted_mean &lt;- mean(landA) * weight[1] + mean(landB) * weight[2]\nweighted_mean\n\n[1] 68.86833\n\n\nThis is like saying: “62% of our land has soil carbon like land type A, and 38% has soil carbon like land type B, so our overall estimate takes both into account in these proportions.”"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#pooled-standard-error-of-the-mean-sebar-y_s",
    "href": "lectures/L02/Lecture-02b.html#pooled-standard-error-of-the-mean-sebar-y_s",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Pooled standard error of the mean \\(SE(\\bar y_{s})\\)",
    "text": "Pooled standard error of the mean \\(SE(\\bar y_{s})\\)\nThe formula looks similar to a standard error…\n\\[SE(\\bar y_{s}) = \\sqrt{\\color{blue}{{\\sum_{i=1}^L w_i^2}} \\times \\frac{s_i^2}{n_i}}\\]\n\n\n\n\n\n\nWhat’s different?\n\n\n\nInstead of a single variance term, we use the sum of weighted variances from each stratum\nThe \\(\\color{blue}{w_i^2}\\) term ensures we account for the relative size of each stratum\nEach stratum contributes its own variance (\\(s_i^2\\)) and sample size (\\(n_i\\))"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#t-critical-value",
    "href": "lectures/L02/Lecture-02b.html#t-critical-value",
    "title": "Lecture 02b – Sampling designs II",
    "section": "\\(t\\)-critical value",
    "text": "\\(t\\)-critical value\nDegrees of freedom \\(df\\)\n\\[df = n - L\\]\nwhere \\(n\\) is the total number of samples and \\(L\\) is the number of strata.\n\n\nThe degrees of freedom tells us how much “free information” we have for making estimates\nFor stratified sampling, we lose one degree of freedom for each stratum\nExample: If we have 12 samples in 3 strata:\n\nThe degrees of freedom is \\(12 - 3 = 9\\)\nThink of it this way: 9 samples can be placed anywhere, but we must have at least 1 sample in each of the 3 strata\n\n\n\n\nIn R\n\ndf &lt;- length(landA) + length(landB) - 2\nt_crit &lt;- qt(0.975, df)\nt_crit\n\n[1] 2.570582"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "95 % Confidence interval for stratified random sampling",
    "text": "95 % Confidence interval for stratified random sampling\nThe formula\n\\[95\\%\\ CI = \\bar y_{s} \\pm t^{0.025}_{n-L} \\times SE(\\bar y_{s})\\]\nwhere \\(L\\) is the number of strata, \\(n\\) is the total number of samples, and \\(\\bar y_{s}\\) is the weighted mean of the strata.\nIn simple terms:\n\nWe’re creating a range where we’re 95% confident the true population mean lies\nWe start with our best estimate (the pooled mean \\(\\bar y_{s}\\))\nWe add and subtract a “margin of error” (which depends on our sample size and variability)\nThe margin of error = \\(t\\)-critical value × standard error\n\nVisualising this:\nLower bound ← [Pooled mean - Margin of error] ... [Pooled mean + Margin of error] → Upper bound"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling-1",
    "href": "lectures/L02/Lecture-02b.html#confidence-interval-for-stratified-random-sampling-1",
    "title": "Lecture 02b – Sampling designs II",
    "section": "95 % Confidence interval for stratified random sampling",
    "text": "95 % Confidence interval for stratified random sampling\nPutting it all together\n\nvarA &lt;- var(landA) / length(landA)  # variance of the mean for A\nvarB &lt;- var(landB) / length(landB)  # variance of the mean for B\nweighted_var &lt;- weight[1]^2 * varA + weight[2]^2 * varB\nweighted_se &lt;- sqrt(weighted_var)\nci &lt;- c(\n  L95 = weighted_mean - t_crit * weighted_se,\n  u95 = weighted_mean + t_crit * weighted_se\n)\nci\n\n     L95      u95 \n61.04864 76.68803"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#simple-random-vs.-stratified-random-sampling",
    "href": "lectures/L02/Lecture-02b.html#simple-random-vs.-stratified-random-sampling",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Simple random vs. stratified random sampling",
    "text": "Simple random vs. stratified random sampling\nWhat if we had used stratified random sampling instead of simple random sampling (and collected the same amount of data)?\nWhat differences can you see?\n\n\nCode\nlibrary(tidyverse)\n# Manually printing the results below as SRS data is in previous lecture\ncompare &lt;- tibble(\n  Design = c(\"Simple Random\", \"Stratified Random\"),\n  Mean = c(67.29, 68.9), \n  `Var (mean)` = c(50.83, 9.30),\n  L95 = c(49.85, 61), \n  U95 = c(84.73, 76.7), \n  df = c(6, 5))\nknitr::kable(compare)\n\n\n\n\n\nDesign\nMean\nVar (mean)\nL95\nU95\ndf\n\n\n\n\nSimple Random\n67.29\n50.83\n49.85\n84.73\n6\n\n\nStratified Random\n68.90\n9.30\n61.00\n76.70\n5"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#visual-comparison-of-95-confidence-intervals",
    "href": "lectures/L02/Lecture-02b.html#visual-comparison-of-95-confidence-intervals",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Visual comparison of 95% confidence intervals",
    "text": "Visual comparison of 95% confidence intervals\n\n# Creating a visual comparison of confidence intervals\nggplot(compare, aes(x = Design, y = Mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = L95, ymax = U95), width = 0.2, size = 1) +\n  labs(title = \"95% Confidence Intervals by Sampling Design\",\n       y = \"Soil Carbon (tonnes/ha)\",\n       x = \"\") +\n  theme_minimal(base_size = 14) +\n  annotate(\"text\", x = 2, y = 55, \n           label = \"Stratified sampling gives a\\nnarrower confidence interval\\n(more precise estimate)\", \n           color = \"blue\")\n\n\n\nKey insights:\n\nBoth methods give similar estimates of the mean\nStratified sampling produces a much narrower confidence interval\nThe variance of the mean is about 5 times smaller with stratified sampling\nThis means stratified sampling is much more precise with the same number of samples"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#efficiency",
    "href": "lectures/L02/Lecture-02b.html#efficiency",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Efficiency",
    "text": "Efficiency\nWhat is sampling efficiency?\n\nA measure of how much “bang for your buck” you get with different sampling methods\nCalculated as a ratio: \\[\\text{Efficiency} = \\frac{\\text{Variance of SRS}}{\\text{Variance of Stratified}}\\]\n\n\nIn simple terms:\n\nEfficiency &gt; 1: Stratified sampling is better (more precise with same sample size)\nEfficiency = 5 means: You’d need 5 times as many samples with simple random sampling to get the same precision as stratified sampling\n\n\n\nIn R\n\nefficiency &lt;- 50.83 / 9.30\nefficiency\n\n[1] 5.465591\n\n\nHow many samples would we have had to collect using simple random sampling to achieve the same precision as our stratified sample?\n\nround(7 * efficiency, 0)\n\n[1] 38\n\n\nSo we would need about 38 samples with simple random sampling to get the same precision that we achieved with just 7 samples using stratified sampling!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#tips-on-implementation",
    "href": "lectures/L02/Lecture-02b.html#tips-on-implementation",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Tips on implementation",
    "text": "Tips on implementation\n\nThe most difficult part is to identify the strata and assign the sampling units to the strata\nCommon stratification variables in environmental science:\n\nSpatial: elevation bands, soil types, vegetation zones\nTemporal: seasons, time of day, growth stages\nManagement: treatment types, land-use history\n\nStrata sampling size: allocate samples to strata based on the size of the strata, either proportional to:\n\nthe size of the strata (e.g. 60% of area = 60% of samples)\nthe variance of the strata (more samples where variation is higher)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#the-change-in-mean-delta-bar-y",
    "href": "lectures/L02/Lecture-02b.html#the-change-in-mean-delta-bar-y",
    "title": "Lecture 02b – Sampling designs II",
    "section": "The change in mean \\(\\Delta \\bar y\\)",
    "text": "The change in mean \\(\\Delta \\bar y\\)\nImportant considerations\n\nWe want to measure change in soil carbon over time\nKey question: How do we select sites for the second measurement?\n\nReturn to the same sites?\nSelect completely new sites?\n\nThis choice affects our statistical analysis (covariance)"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#monitoring-estimates",
    "href": "lectures/L02/Lecture-02b.html#monitoring-estimates",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Monitoring estimates",
    "text": "Monitoring estimates\nChange in mean \\(\\Delta \\bar y\\)\n\nThe difference between the means of the two sets of measurements.\n\n\\[\\Delta \\bar y = \\bar y_2 - \\bar y_1\\]\nwhere \\(\\bar y_2\\) and \\(\\bar y_1\\) are the means of the second and first set of measurements, respectively."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#uncertainty-in-change-estimates",
    "href": "lectures/L02/Lecture-02b.html#uncertainty-in-change-estimates",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Uncertainty in change estimates",
    "text": "Uncertainty in change estimates\n\nVariance of the change in mean \\(Var(\\Delta{\\bar y})\\)\nThis tells us how precise our estimate of the change is. It depends on:\n\\[Var(\\Delta{\\bar y}) = Var(\\bar y_2) + Var(\\bar y_1) - 2 \\times Cov(\\bar y_2, \\bar y_1)\\]\nIn simple terms:\n\nThe uncertainty in our change estimate comes from the uncertainties in both measurements\nHowever, if we sample the same sites twice, they are related to each other (covariance)\nThis relationship usually reduces the overall uncertainty in our change estimate\n\n\n\nImportant: Visiting the same sites twice (paired sampling) usually gives more precise estimates of change than visiting different sites each time!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#covariance-and-site-selection",
    "href": "lectures/L02/Lecture-02b.html#covariance-and-site-selection",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Covariance and site selection",
    "text": "Covariance and site selection\nQuick decision guide\n\nSame sites? Use paired approach:\n\nSites are the same in both visits\nUse paired t-test\nAccount for covariance between visits\n\nDifferent sites? Use independent approach:\n\nNew random sites in second visit\nUse two-sample t-test\nNo covariance between visits"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#what-is-covariance",
    "href": "lectures/L02/Lecture-02b.html#what-is-covariance",
    "title": "Lecture 02b – Sampling designs II",
    "section": "What is covariance?",
    "text": "What is covariance?\n\nCovariance measures how two measurements relate to each other:\nExample with soil carbon:\n\nSite 1: First visit = 90 t/ha, Second visit = 95 t/ha\nSite 2: First visit = 48 t/ha, Second visit = 52 t/ha\nSite 3: First visit = 71 t/ha, Second visit = 75 t/ha\n\n\n\nWhat do you notice? Sites with high carbon in the first measurement still have high carbon in the second measurement (positive covariance).\nWhy this matters: Knowing the first measurement helps us predict the second one, reducing uncertainty in our estimate of change.\n\n\nPractical takeaway: When measuring change over time, returning to the same sites usually gives more precise results because it removes site-to-site variation."
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#calculating-the-95-ci-for-the-change-in-mean",
    "href": "lectures/L02/Lecture-02b.html#calculating-the-95-ci-for-the-change-in-mean",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Calculating the 95% CI for the change in mean",
    "text": "Calculating the 95% CI for the change in mean\nThe formula looks similar to before:\n\\[95\\%\\ CI = \\Delta \\bar y \\pm t^{0.025}_{n-1} \\times SE(\\Delta \\bar y)\\]\n\nIn plain language:\n\nWe have our best estimate of the change (the difference between the two means)\nWe add and subtract a margin of error to create a range\nWe’re 95% confident that the true change falls within this range\n\n\n\nThe standard error of the change \\(SE(\\Delta \\bar y)\\)\n\nThis tells us how precise our estimate of the change is\nIt’s complicated to calculate by hand, especially when we visit the same sites twice\nIf we visit the same sites twice, we need to account for their relationship (covariance)\n\n\n\nGood news! You don’t need to calculate this by hand!\n\nR can do these calculations for you using the t.test() function\nFor same sites: use paired = TRUE option\nFor different sites: use paired = FALSE option\nWe’ll practice this in the lab!"
  },
  {
    "objectID": "lectures/L02/Lecture-02b.html#thanks",
    "href": "lectures/L02/Lecture-02b.html#thanks",
    "title": "Lecture 02b – Sampling designs II",
    "section": "Thanks!",
    "text": "Thanks!\nQuestions?\nThis presentation is based on the SOLES Quarto reveal.js template and is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "lectures/L01/index.html",
    "href": "lectures/L01/index.html",
    "title": "Lecture 01",
    "section": "",
    "text": "Lecture 01a – Welcome Full Screen | PDF\n\nLecture 01b – The beginning is the end: a revision Full Screen | PDF"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#staff",
    "href": "lectures/L01/Lecture-01a.html#staff",
    "title": "Lecture 01a – Welcome",
    "section": "Staff",
    "text": "Staff\n\n\n\n\n\n\nA. Prof Aaron GreenvilleUnit CoordinatorWeeks 4-6\n\n\n\n\n\n\n\nDr Liana PozzaLecturerWeeks 7-9\n\n\n\n\n\n\n\nDr Januar HariantoLecturerWeeks 1-3\n\n\n\n\n\n\n\nProf Mathew CrowtherProfessorWeeks 9-11"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#does-migration-come-at-a-cost",
    "href": "lectures/L01/Lecture-01a.html#does-migration-come-at-a-cost",
    "title": "Lecture 01a – Welcome",
    "section": "Does migration come at a cost?",
    "text": "Does migration come at a cost?\nIn a partially migratory elk population in Alberta, Canada, researchers compared calf birth weights between resident and migrant herds.\n\n\nCode\nelk &lt;- read.csv(\"data/elk_calf_clean.csv\")\nelk$mig_status &lt;- factor(elk$mig_status,\n  levels = c(\"Resident\", \"Eastern\"),\n  labels = c(\"Resident\", \"Migrant\")\n)\n\nggplot(elk, aes(x = mig_status, y = birth_wt, fill = mig_status)) +\n  geom_boxplot(show.legend = FALSE) +\n  scale_fill_viridis_d(end = 0.9) +\n  labs(x = NULL, y = \"Birth weight (kg)\") +\n  theme_minimal(base_size = 18)\n\n\n\nYou have already learnt to compare groups like this with t-tests (Weeks 1–3)."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#can-dogs-smell-human-stress",
    "href": "lectures/L01/Lecture-01a.html#can-dogs-smell-human-stress",
    "title": "Lecture 01a – Welcome",
    "section": "Can dogs smell human stress?",
    "text": "Can dogs smell human stress?\nEighteen pet dogs were exposed to odours from humans who were either relaxed or stressed. Researchers then measured how long each dog took to approach food bowls, looking for signs of anxiety.\n\n\nCode\ndogs &lt;- read.csv(\"data/dog_odour_clean.csv\")\ndogs$treatment &lt;- factor(dogs$treatment,\n  levels = c(\"Baseline\", \"Relaxed\", \"Stressed\")\n)\n\ndog_means &lt;- aggregate(latency ~ dog_id + treatment, data = dogs, FUN = mean)\n\nggplot(dog_means, aes(x = treatment, y = latency, colour = treatment)) +\n  geom_jitter(width = 0.2, size = 3, alpha = 0.6) +\n  stat_summary(fun = mean, geom = \"crossbar\", width = 0.5,\n    linewidth = 0.8, colour = \"black\") +\n  scale_colour_viridis_d(end = 0.9) +\n  labs(x = \"Odour treatment\", y = \"Mean latency (s)\") +\n  theme_minimal(base_size = 18) +\n  theme(legend.position = \"none\")\n\n\n\nWhat if there are more than 2 groups to compare? We use ANOVA (Weeks 3–6)."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#how-big-is-that-penguin",
    "href": "lectures/L01/Lecture-01a.html#how-big-is-that-penguin",
    "title": "Lecture 01a – Welcome",
    "section": "How big is that penguin?",
    "text": "How big is that penguin?\nHeavier penguins tend to have longer flippers – but how strong is that relationship?\n\n\nCode\npenguins &lt;- palmerpenguins::penguins\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(colour = species), alpha = 0.6) +\n  geom_smooth(method = \"lm\") +\n  scale_colour_viridis_d(end = 0.9) +\n  labs(x = \"Flipper length (mm)\", y = \"Body mass (g)\") +\n  theme_minimal(base_size = 18)\n\n\n\nWe can model this relationship with regression, but how do we validate this against other models? You will learn to do this with model selection (Weeks 7–9)."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#darwins-finches-evolution-you-can-measure",
    "href": "lectures/L01/Lecture-01a.html#darwins-finches-evolution-you-can-measure",
    "title": "Lecture 01a – Welcome",
    "section": "Darwin’s finches: evolution you can measure",
    "text": "Darwin’s finches: evolution you can measure\nPeter and Rosemary Grant spent 40 years measuring beak morphology in Galápagos finches, documenting natural selection in real time.\n\n\nCode\nfinches &lt;- read.csv(\"data/darwin_finches_clean.csv\")\n\nggplot(finches, aes(x = beak_length, y = beak_depth, colour = species)) +\n  geom_point(alpha = 0.5) +\n  stat_ellipse(linewidth = 1) +\n  scale_colour_viridis_d(end = 0.9) +\n  labs(x = \"Beak length (mm)\", y = \"Beak depth (mm)\", colour = \"Species\") +\n  theme_minimal(base_size = 18)\n\n\n\nSometimes, patterns jump out to us in complex datasets. You will learn to find patterns like these with multivariate methods (Weeks 10–12)."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#lectures",
    "href": "lectures/L01/Lecture-01a.html#lectures",
    "title": "Lecture 01a – Welcome",
    "section": "Lectures",
    "text": "Lectures\nWednesdays 10am - 12pm, Chemistry Lecture Theatre 3 (CLT03)\n\nCopyright The University of Sydney"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#labs",
    "href": "lectures/L01/Lecture-01a.html#labs",
    "title": "Lecture 01a – Welcome",
    "section": "Labs",
    "text": "Labs\nLabs are held at the South Eveleigh Precinct\n\nCredit: Michael Wheatland"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#directions-from-carslaw",
    "href": "lectures/L01/Lecture-01a.html#directions-from-carslaw",
    "title": "Lecture 01a – Welcome",
    "section": "Directions from Carslaw",
    "text": "Directions from Carslaw\nIf the map does not load, click here"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#transport-options",
    "href": "lectures/L01/Lecture-01a.html#transport-options",
    "title": "Lecture 01a – Welcome",
    "section": "Transport options",
    "text": "Transport options\nBuses\nCourtesy buses are available from Fisher Library to Redfern Station. You must then walk to the precint (10 minutes).\nDriving\nFree parking is available around Henderson Road, but it is extremely crowded. We do not recommend driving to the precinct.\nWalking\nWalking to the South Eveleigh Precinct takes about 20 minutes from Carslaw. You can save approximately 5 minutes by using Redfern station’s community access gates."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#attendance",
    "href": "lectures/L01/Lecture-01a.html#attendance",
    "title": "Lecture 01a – Welcome",
    "section": "Attendance",
    "text": "Attendance\nLectures are compulsory, but not a hurdle\n\nYou are expected to attend 80% of lectures, and attendance is recorded via Ed Discussion Boards\nAttendance is not a hurdle. We use attendance data to determine if students are engaging with the course\n\nLabs are compulsory with 80% minimum attendance\n\nLabs are the heart of this unit. No exceptions except through special consideration\nWe will not record your attendance if you are not present for more than 50% of the lab session (we round things up)"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#assessments",
    "href": "lectures/L01/Lecture-01a.html#assessments",
    "title": "Lecture 01a – Welcome",
    "section": "Assessments",
    "text": "Assessments\nCheck Unit Outline\n\n\n\n\n\n\n\n\n\nWeek\nAssessment\nWeight\nType\n\n\n\n\n3\nEarly Feedback Task\n1%\nIndividual\n\n\n5\nProject 1: Describing data\n10%\nIndividual\n\n\n10\nProject 2: Analysing experimental data\n20%\nIndividual\n\n\n13\nProject 3: Presentation (multivariate)\n20%\nGroup\n\n\n-\nQuizzes (weekly, multiple due dates)\n4%\nIndividual\n\n\n-\nExam (2 hours, MCQs + Short Answers)\n45%\nIndividual"
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#put-in-the-hours",
    "href": "lectures/L01/Lecture-01a.html#put-in-the-hours",
    "title": "Lecture 01a – Welcome",
    "section": "Put in the hours",
    "text": "Put in the hours\n\nThis is a 6 credit point unit, which means that you are expected to spend 120 – 150 hours in total, including exam prep time (~10 h per week)!\nPractice makes perfect. Tutorials and Labs help you apply the concepts you learn in lectures – complete all the exercises, and practice with the bonus questions provided."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#ask-questions",
    "href": "lectures/L01/Lecture-01a.html#ask-questions",
    "title": "Lecture 01a – Welcome",
    "section": "Ask questions",
    "text": "Ask questions\nDon’t be afraid to seek help. We are happy when students show genuine interest to learn and will do our best to support you. Here are some ways to ask questions:\n\nEd Discussion is the best place to ask questions. We are way more responsive on Ed than on email.\nDrop-in sessions are available. Check Ed Discussion for times and links."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#by-the-end-of-this-course-we-want-you-to-be-able-to",
    "href": "lectures/L01/Lecture-01a.html#by-the-end-of-this-course-we-want-you-to-be-able-to",
    "title": "Lecture 01a – Welcome",
    "section": "By the end of this course, we want you to be able to:",
    "text": "By the end of this course, we want you to be able to:\n\nLO1 demonstrate proficiency in designing sample schemes and analysing data from them using R.\nLO2 describe and identify the basic features of an experimental design: replicate, treatment structure and blocking structure.\nLO3 demonstrate proficiency in the use or the statistical programming language R to apply an ANOVA and fit regression models to experimental data.\nLO4 demonstrate proficiency in the use or the statistical programming language R to use multivariate methods to find patterns in data.\nLO5 interpret the output and understand conceptually how its derived of a regression, ANOVA and multivariate analysis that have been calculated by R.\nLO6 write statistical and modelling results as part of a scientific report.\nLO7 appraise the validity of statistical analyses used publications."
  },
  {
    "objectID": "lectures/L01/Lecture-01a.html#take-a-break",
    "href": "lectures/L01/Lecture-01a.html#take-a-break",
    "title": "Lecture 01a – Welcome",
    "section": "Take a break",
    "text": "Take a break\nWe will resume the second half of the lecture in 10 minutes. Come on down if you have questions!"
  },
  {
    "objectID": "labs/Lab11/Lab11-clustering.html",
    "href": "labs/Lab11/Lab11-clustering.html",
    "title": "Lab 11",
    "section": "",
    "text": "In this lab, we will learn how to:\n\nCalculate distances (non-Euclidean) using R.\nUse k-means clustering to determine sampling strata in R.\nVisualise clusters in R and in Google earth."
  },
  {
    "objectID": "labs/Lab11/Lab11-clustering.html#learning-outcomes",
    "href": "labs/Lab11/Lab11-clustering.html#learning-outcomes",
    "title": "Lab 11",
    "section": "",
    "text": "In this lab, we will learn how to:\n\nCalculate distances (non-Euclidean) using R.\nUse k-means clustering to determine sampling strata in R.\nVisualise clusters in R and in Google earth."
  },
  {
    "objectID": "labs/Lab11/Lab11-clustering.html#specific-goals",
    "href": "labs/Lab11/Lab11-clustering.html#specific-goals",
    "title": "Lab 11",
    "section": "Specific goals",
    "text": "Specific goals\nBy the end of this lab, you should be able to:\n\nDecide if a dataset is suitable for cluster analysis\nPerform a k-means cluster analysis in R\nInterpret the results of a k-means cluster analysis"
  },
  {
    "objectID": "labs/Lab11/Lab11-clustering.html#a-new-tool",
    "href": "labs/Lab11/Lab11-clustering.html#a-new-tool",
    "title": "Lab 11",
    "section": "A new tool",
    "text": "A new tool\nWelcome to week 11. Last week, we learned about Principle Component Analysis (PCA), which works like a multidimensional camera. This week, we will learn about Cluster Analysis, which you can think of as a multidimensional paintbrush.\nBy the end of this lab, you will know how split a multidimensional dataset into neat subcategories using computer algorithms. The different ways of making these categories are called clustering techniques.\nWe will learn about a common clustering technique called k-means clustering."
  },
  {
    "objectID": "labs/Lab11/Lab11-clustering.html#preparation",
    "href": "labs/Lab11/Lab11-clustering.html#preparation",
    "title": "Lab 11",
    "section": "Preparation",
    "text": "Preparation\nTo prepare for this lab, please activate the following packages:\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.2.0     ✔ purrr     1.2.1\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.2     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(vegan)\n\nLoading required package: permute\n\n\nThe ‘vegan’ package is especially useful for advanced multivariate statistics. Check out this link for more information.\nPlease also download these files: fairfax.csv, fossils.csv, mccue2.csv\nJust like last week, we strongly recommend that you go through this lab with pen and paper in hand.\nWe’ll talking about painting, after all.\n\n\n\n\n\n\n\n\n\nPaintbrushes belonging to Charlyn Marina Khater, Lebanese designer and painter. We can use colour to represent different subcategories within our data. From Wikimedia Commons (2014), by Fox212121."
  },
  {
    "objectID": "labs/Lab11/Lab11-clustering.html#part-1-no-pictures-just-yet",
    "href": "labs/Lab11/Lab11-clustering.html#part-1-no-pictures-just-yet",
    "title": "Lab 11",
    "section": "Part 1: No Pictures Just Yet",
    "text": "Part 1: No Pictures Just Yet\nBefore we reach for our paintbrushes, we have to understand the fundamental idea behind splitting datasets into subcategories. Why might we want to do this?\nThe simple reason is that being humans, we enjoy subcategories. They are useful. Think about the way books are organised in a library, or pet food is organised in a pet store. Similar items belong together – that way, they are easier to find.\n\n\n\n\n\n\nNoteWhen do we need subcategories in scientific research?\n\n\n\n\n\nThink about the way we collect our data in a stratified sampling design, where we first divide our study site into different strata and then sample randomly within each one (see lab 2 for more information).\nHow do we decide what these strata should be in the first place? We may need to split our study site into a collection of sensible sub-sites.\n\n\n\nSometimes, the way we create subcategories is very obvious. For example, it is common practice for restaurants to offer their foods and drinks on separate menus. There is very little ambiguity over how to do this – if it comes in a glass and you can slurp it, it is a drink (soups and consumes might add some difficulty, but usually not much), and it belongs on the drink menu.\nSometimes, however, things are not so easy. For example, how do you organise books on your bookshelf? Do you group books by subject – science, history, art, home and gardening? Do you group them by length? By how likely you are to read them in the next few days? A combination of the above?\n\n\n\n\n\n\n\n\n\nOld books collected by the Basking Ridge Historical Society. Some of these books have no titles on their cover pages. From Wikimedia Commons (2008), by William Hoiles from Basking Ridge, NJ, USA.\nThe difference between sorting food/drink items on a menu and sorting books on your bookshelf is that in the case of food/drink, there is only one distinguishing characteristic – to slurp or not to slurp; but in the case of books, there are multiple characteristics to consider, none of which take obvious precedent over the others.\nIn this case, the food/drink divide is a univariate problem, while book organisation is a multivariate problem.\n\nExercise: Distances\n\n\n\n\n\n\n\n\n\nA page with marginalia from the first printed edition of Euclid’s ‘Elements’, printed by Erhard Ratdolt in 1482. The ‘Elements’ is one of the oldest surviving mathematical treatises. It was written in 300BC, and exclusively tackles the subject of angles, distances, and geometric arrangements on a flat surface, a field of maths known today as ‘Euclidean Geometry’. From Folger Shakespeare Library Digital Image Collection, original link: http://luna.folger.edu/luna/servlet/s/2c163w.\nHere’s an exercise. Think about your favourite foods. Picture them in your mind. Hungry? Perfect.\nIf you are attending this lab in person, you will see a pair of signs on the classroom walls. One of these signs will read “Sweet/Savory”, and the other “Pungent/Mild”. Align yourself on these axes according to your food preferences. If you have no preference on either front, stay in the middle of the room.\nOnce you have made your choice, look around you. The people nearby are the ones who like similar foods to you. The people on the other side of the room are those who like different foods to you.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf you are completing this lab remotely, try the same exercise with your friends.\n\n\n\n\n\n\n\n\n\n\n\n\nFood basket. Vegetables come in a range of flavours and textures. Different people prefer different ones. From Wikimedia Commons (2012), by liz west from Boxborough, MA.\nNotice what we did here – we represented food preferences using physical distance. Food preference is an abstract concept, but physical distance is something we can measure concretely. By rearranging ourselves in a physical space, we saved the trouble of having to ask every other person in the room: “Do you like savory foods?” or “Do you prefer mild flavours?” to find out who likes the same types of foods we do – all we need to do is look at who is nearby.\nThis way of using distance to represent similarity is the key to a subtopic of multivariate statistics called distance-based ordination. These techniques work differently to the PCA methods we learned about last week, though they share the common goal of reducing dimensions in our data.\n\n\n\n\n\n\nWarningWhen should I use distance-based ordination instead of PCA?\n\n\n\n\n\nAs a rule of thumb, distance-based ordination is better for datasets where you expect non-linear relationships between response variables. Many types of ecological data fall under this category – for example, species abundance data.\n\n\n\nIn our food example just now, we used the intuitive definition of ‘distance’ – the straight-line distance between two points on a flat surface. This type of distance is called Euclidean distance. We can certainly use Euclidean distance in multivariate analysis, but it is not the only type of distance metric we can use. In fact, sometimes we can even get away with using what are known as semi-metrics or dissimilarity measures instead of true metrics.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf you are interested in the mathematics behind metrics and semi-metrics, check out this page by the University of Washington.\n\n\n\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nImagine if we added a third characteristic of food: Crunchy/Soft. How would you modify the previous exercise to accomodate this extra variable?\nWhat happens if we add even more variables?\n\n\nIn the next example, we will use a semi-metric known as the Bray-Curtis Dissimilarity. The Bray-Curtis Dissimilarity is very popular in ecological studies. It is especially useful when analysing species abundance data.\n\n\n\n\n\n\n\n\n\nPhoto of a Platystrophia biforata fossil from Schlotheim (1820). Platystrophia belongs to a group of animals called brachiopods, which were common in the late Ordovician Period (~ 450 mya). When fossilised, brachiopods look like clams. When living, many brachipods have a long, fleshy pedicle/stalk that anchors them to the seabed. Specimen stored in the Estonian Museum of Natural History. From Wikimedia Commons (2015), uploaded by Tõnis Saadre.\n\n\nPractice: Fossils\nThe Cincinnati region of Ohio, USA, is a hot spot for fossils of ancient marine organisms. These fossils date from the Late Ordovician Period, around 450 million years ago.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nRead the dataset ‘fossils.csv’ into R, and name it fossils.\nWhen you read in this dataset using the read.csv() function, include the extra argument: row.names = 1.\n\n\n\n\n\n\nTip\n\n\n\n\n\nWe use the row.names = 1 argument for two reasons: 1. our dataset has an ‘ID’ column that does not fit in with the rest of our data, and 2. the columns of our dataset have names, but its rows do not.\nThe row.names = 1 argument solves both problems at once – it removes the first column in our dataset (which is the misfit ‘ID’ column), and use its entries as the names for our rows.\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nRead in the data, including the row.names = 1 argument:\n\nfossils &lt;- read.csv('data/fossils.csv', row.names = 1)\n\n\n\n\nEach row in our data is an individual rock sample with its own ‘ID’. In each sample, we may find a different collection of fossilised animals. This is the standard way to organise abundance data – samples as rows, and genera/species as columns.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nLook at sample 2D002, the second row in our dataset. What is the most abundant animal genus in this sample?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nIt seems like ‘Rafin’ (which stands for Rafinesquina, a type of brachiopod) is the most common genus in sample 2D002, with a count of 7 individuals.\nAlso notice there are many genera with 0 abundance in this sample. Zero data is a common theme in ecological surveys, which is another reason why we prefer to use distance-based ordination techniques instead of PCA for these types of datasets.\n\n\n\nNow, suppose we asked the question: which rock samples are similar to each other, and which are different? Take a moment to think about how you would answer this question. Would you compare the samples one at a time, or is there a better way?\n\n\n\n\n\n\n\n\n\nTrilobite fossils (the ones in this picture are not from Cincinnati). Trilobites are a famous group of extinct arthropods from the palaeozoic era (539~251 mya). From Wikimedia Commons (2006), by Muzejní komplex Národního muzea.\nThere is a better way. We can use the Bray-Curtis dissimilarity measure. We do this using the vegdist() function. This function is part of the ‘vegan’ package, and is very useful for calculating all kinds of distance metrics and dissimilarity measures.\nHere is how we do it:\n\nfossils_BC_dissimilarity &lt;- vegdist(fossils, method = 'bray') \n# Specify `method = '...'` to tell R which distance metric or semi-metric you want to use. \n# In this case, we used `method = 'bray'` for Bray-Curtis dissimilarity, a semi-metric.\n\nYou will see a new item pop up in your R environment under the ‘Values’ tab called ‘fossils_BC_dissimilarity’. This is the dissimilarity matrix generated by vegdist(). To turn this matrix into a table we can inspect, use the as.matrix() function.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nUse the as.matrix() function to turn fossils_BC_dissimilarity into a table, and rename it fossil_BC_dissimilarity_table.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nfossil_BC_dissimilarity_table &lt;- as.matrix(fossils_BC_dissimilarity)\n\n\n\n\nYou should see a new item appear in your R environment, this time under the ‘Data’ tab, called ‘fossil_BC_dissimilarity_table’. Click on this item to open it in a new window.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nWhat do you think this table means?\nWhy are there cells with values of 0?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis table tells us the dissimilarity between our samples. The higher the number, the less similar the two samples are. You can picture these numbers as the distances between samples, just like the distances between you and your classmates in the food exercise from earlier.\nCells with values of 0 mean the samples are identical. It is no accident that these cells appear along the top-left/bottom-right diagonal – each sample is identical to itself.\n\n\n\nBased on this table, we can answer a few simple questions. For example:\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nWhich rock sample is the least similar to sample 2D001? Which one is the most similar (besides itself)?\n\n\n\n\n\n\nTip\n\n\n\n\n\nA useful shortcut to pick out the largest number in any row is the which.max() function.\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe least similar sample to 2D001 is the one with the greatest Bray-Curtis dissimilarity index in the first row.\nLet’s use which.max() to see where this number is:\n\nwhich.max(fossil_BC_dissimilarity_table[1,])\n\n2S025 \n  103 \n\n\nIt seems that sample 2S025, in column 103, is the least similar to sample 2D001. They share a Bray-Curtis dissimilarity of ~0.93.\nSimilarly, which.min() tells us where to find the smallest number in the first row.\n\nwhich.min(fossil_BC_dissimilarity_table[1,])\n\n2D001 \n    1 \n\n\nWhich returns no other than sample 2D001, because it has a Bray-Curtis dissimilarity of 0 with itself. To get around this problem, let’s exclude the first column:\n\nwhich.min(fossil_BC_dissimilarity_table[1,-1])\n\n2D002 \n    1 \n\n# [1,-1] filters for the first row, excluding the first column (due to the minus sign).\n\nWhich tells us that sample 2D002 is in fact the most similar to sample 2D001, with a Bray-Curtis dissimilarity of 0.~43.\n\n\n\nGive it a try yourself:\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nWhich rock sample is the least similar to sample 2D003? Which one is the most similar (besides itself)?\n\n\n\n\n\n\n\n\nWarningSimilarity or dissimilarity?\n\n\n\n\n\nSo far, we have talked extensively about Bray-Curtis dissimilarity. However, we could just as easily talk about Bray-Curtis similarity.\nTo convert from one to the other, use the following trick:\nBC_{similarity} = 1-BC_{dissimilarity}\n\n\n\nEarlier, we mentioned that the Bray-Curtis dissimilarity is only one of many metrics/semi-metrics that exist. You may be itching to try some different metrics, just to see what they are like:\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nGenerate a dissimilarity matrix with vegdist(fossils, method = ...) using a distance metric or semi-metric other than the Bray-Curtis dissimilarity. Some fun ones to try are: method = 'manhattan', method = 'euclidean', method = 'jaccard'.\nRename this matrix fossils_’‘_dissimilarity_table, where’’ is the method you used (e.g. fossils_euclidean_dissimilarity_table)\n\n\n\n\n\n\nTip\n\n\n\n\n\nType ?vegdist into your console to see what other distance metrics exist.\nRemember you can make your matrix into a nice table with the as.matrix() function.\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nLet’s try method = 'euclidean'\n\nfossils_euclidean_dissimilarity_table &lt;- as.matrix(vegdist(fossils, method = 'euclidean'))\n\nUnlike when we used Bray-Curtis dissimilarity, the numbers are no longer between 0 and 1. This is because Euclidean distance is an unbounded metric.\n\n\n\nThat’s a lot of numbers. Time for pictures, what do you say?\n\n\n\n\n\n\n\n\n\nA watercolour painting titled: ‘Nature and peoples life in cht’, by artist Mong kyaw sing marma. Cluster Analysis uses the distance matrices we learned about just now to colour-code multivariate datasets. From Wikimedia Commons (2019)."
  },
  {
    "objectID": "labs/Lab11/Lab11-clustering.html#part-2-paintbrushes-out",
    "href": "labs/Lab11/Lab11-clustering.html#part-2-paintbrushes-out",
    "title": "Lab 11",
    "section": "Part 2: Paintbrushes Out",
    "text": "Part 2: Paintbrushes Out\nNow that we have learned what happens behind the scenes of a Cluster Analysis, it is time for the real deal. Dissimilarity measures are well and good, but they are only the means to an end.\nThe ultimate goal of Cluster Analysis is… well, clusters. Let’s get some of those going.\n\nExercise: k-means, a non-hierarchical clustering method.\nFor this exercise, we will bring back the iris dataset from last week.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nCheck the structure of the iris dataset using the str() function. Remember that this dataset is already built into R.\n\n\nLast week, we saw that the iris dataset has 4 numeric variables. For the sake of visualisation, let’s pick 2 of them to be our x and y axes respectively.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nUse ggplot() to make a scatter plot of the iris dataset. You can choose any two of the four numeric variables: Sepal.Length, Sepal.Width, Petal.Length, or Petal.Width to be your x and y axes.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nI chose sepal length and petal width as my axes:\n\nggplot(data = iris, \n       aes(x = Sepal.Length, y = Petal.Width)) +\n  geom_point(shape = 1, stroke = 1)+\n  theme_classic()\n\n\n\n\n\n\n\n# shape = 1, stroke = 1 arguments give the graph its rimmed circles. \n# Stick with normal circles if you like them better; it's up to you.\n\n\n\n\nWe can already see that some points seem to group together, and we can try to guess where these groups start and end – but that is not easy to do by eye. Also, remember we are only seeing a 2-dimensional picture of a 4-dimensional situation. If you chose a different set of axes from me, you might have seen a different picture with different groups.\nWe need the help of the multivariate paintbrush – k-means clustering.\nK-means is a non-hierarchical clustering technique. You can think of it as colouring a random set of starting points and letting those colours spread from one point to the next until every point is coloured – similar to the way patches of watercolour paint spread out over a piece of paper. In reality, the process is a little more complicated than this – your demonstrators and lectures will happily elaborate.\n\n\n\n\n\n\nTipHierarchical vs non-hierarchical\n\n\n\n\n\nWhen should you use hierarchical clustering as opposed to non-hierarchical clustering?\nHierarchical clustering is more ‘powerful’ in the sense that you can see how every site/sample relates to every other site/sample at once – from the closest relations to the furthest ones, each one nested in the next like the branches of a phylogenetic tree.\nK-means, being a non-hierarchical technique, offers an entirely different advantage – it allows you to specify exactly how many clusters you want to create. This is useful if you have logistic constraints and/or prior knowledge about your data.\n\n\n\nTo perform k-means clustering, we can use the kmeans() function. Let’s apply it to the iris dataset – remember to first exclude the ‘Species’ column. We will specify 3 clusters for now.\n\niris_k_means_3_clusters &lt;- kmeans(iris[,-5], centers = 3, nstart = 20)\n# [,-5] excludes the fifth column, which is our 'Species' column.\n# kmeans(..., centers = 3) tells R we want 3 clusters.\n# kmeans(..., nstart = 20) tells R we want to try 20 different initial configurations\n\nDone! Let’s see the results.\nThe k-means analysis has assigned each of our samples to one of three clusters. This information is stored in the ‘cluster’ column.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nPick out the ‘cluster’ column from iris_k_means_3_clusters. Remember you can pick columns by name using the $ operator.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\niris_k_means_3_clusters$cluster\n\n  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n[112] 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2\n[149] 2 1\n\n\nWe see that the first batch of samples were sorted into cluster 3, the next batch into 2, etc. There are some alternating assignments in between, which is interesting.\n\n\n\nNow we can bring back our original scatter plot and colour-code it based on this ‘cluster’ column:\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nRe-make your scatter plot from earlier, but this time colour-coded by cluster.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nYou can introduce colours to your plot in one of two ways: directly in the geom_point() section, or in the aes() section, which is the way I prefer.\nFirst, make iris_k_means_3_clusters$cluster into a factor:\n\niris_clusters &lt;- as.factor(iris_k_means_3_clusters$cluster)\n\nThen use it to colour-code your plot:\n\nggplot(data = iris, \n       aes(x = Sepal.Length, y = Petal.Width,\n       colour = iris_clusters)) +\n  geom_point(shape = 1, stroke = 1,)+\n  scale_colour_manual(values = c('steelblue','red','black'))+\n  theme_classic() \n\n\n\n\n\n\n\n\n\n\n\nNotice something strange? K-means is supposed to group the closest points together so that each cluster is completely separated from the others, but why are some of the clusters mixed up?\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nWhy do you think the clusters in your graph are not separated properly? Did k-means fail?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe good news is that k-means did not fail! In fact, you could say it was our scatter plots that ‘failed’.\nThe reason some of the clusters look mixed-up is because our scatter plots were only 2-dimensional. K-means, on the other hand, was operating in 4 dimensions.\nTry picking a different set of axes for your scatter plot. Here is one with sepal length and petal length as the x and y axes respectively:\n\niris_clusters &lt;- as.factor(iris_k_means_3_clusters$cluster)\nggplot(data = iris, \n       aes(x = Sepal.Length, y = Petal.Length,\n       colour = iris_clusters)) +\n  geom_point(shape = 1, stroke = 1,)+\n  scale_colour_manual(values = c('steelblue','red','black'))+\n  theme_classic() \n\n\n\n\n\n\n\n\nFrom this perspective, you can see that the clusters are much better separated. If we could plot in 4 dimensions, we would see that the clusters are in fact perfectly separated.\n\n\n\nWhat do these 3 clusters correspond to? Time for the big reveal:\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nRe-make your scatter plot again, but this time colour-coded by species.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nTo colour-code by species, use the argument colour = Species in the aes() section.\n\niris_clusters &lt;- as.factor(iris_k_means_3_clusters$cluster)\nggplot(data = iris, \n       aes(x = Sepal.Length, y = Petal.Width,\n       colour = Species)) +\n  geom_point(shape = 1, stroke = 1,)+\n  scale_colour_manual(values = c('black','red','steelblue'))+\n  theme_classic() \n\n\n\n\n\n\n\n\nLook familiar?\n\n\n\nIt turns out the 3 k-means clusters correspond nicely with the 3 iris species!\nWhy is this interesting? Remember that we actually removed the species column before applying k-means clustering, so k-means had no idea which sample belonged to which species. It was guessing blind, and still more or less reached the right conclusion.\nMagic?\nWell, sort of. The truth is that the iris dataset is very suitable for k-means clustering because each species is morphological distinct from the others. In other cases, k-means might not work so well. However, even a less-than-perfect k-means analysis can still be useful in the real world.\nSide note: this exercise was inspired by a guided demonstration of k-means clustering from R bloggers.\n\n\n\n\n\n\nWarningHow many clusters should I choose?\n\n\n\n\n\nOne of the main questions we glossed over in this exercise was how to choose the right number of clusters.\nPersonally, I cheated a little – I knew there were 3 iris species in the dataset, so I picked k = 3 clusters. In the real world, we might not have this kind of information.\nOne way to pick the right number of clusters mathematically is to look at the within sum of squares (wss) as we incrementally increase the number of clusters. Once we see diminishing returns, we stop.\nVisually, we can use a skree plot – just like we did for our PCAs last week. The code is a little bit complicated, but I’ve left it here for your interest:\n\nwss &lt;- (nrow(iris[,3:4])-1)*sum(apply(iris[,3:4],2,var)) \nfor (i in 2:15) wss[i] &lt;- sum(kmeans(iris[,3:4], centers=i,nstart=20)$withinss)\nplot(1:15, wss, type=\"b\", xlab=\"Number of Clusters\", ylab=\"Within groups sum of squares\")\n\n\n\n\n\n\n\n\nIn this case, 2 clusters would have sufficed. But I wanted 3 to drop the cool reveal :P\n\n\n\n\n\n\n\n\n\n\n\n\nA field of sunflowers in Cardejón, Spain. Dividing a single field into multiple sampling strata is one instance where k-means clustering can be useful. From Wikipedia (2012), by Diego Delso.\n\n\nPractice: Using k-means to stratify a field\nWhen you sample in a field, your sites may vary by slope, elevation, clay content, vegetation type, biomass, etc. You want to sort these sites into distinct strata so that you can sample similar sites together, but how do you do this?\nConsider the following example.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nRead the file fairfax.csv into R, rename it fairfax_field_soil_properties, and check its structure.\nJust like with the fossils dataset, please include the argument row.names = 1 when you read in your data using the read.csv() function.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nfairfax_field_soil_properties &lt;- read.csv(\"data/fairfax.csv\", row.names = 1)\n\nstr(fairfax_field_soil_properties)\n\n'data.frame':   77625 obs. of  8 variables:\n $ x        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ y        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ clay1    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ clay2    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ clay3    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ clay4    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ valsSlope: num  NA NA NA NA NA NA NA NA NA NA ...\n $ valsDem  : num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nWe see this dataset has a HUGE number of observations (&gt; 70,000), many of which are ’NA’s. We will not be able to detect any patterns by looking through this dataset manually.\nWhat we can see from the structure of the dataset is that it contains 8 numeric variables. The variables ‘x’ and ‘y’ are co-ordinates, ‘clay 1-4’ are clay contents at different depths (in %), valsSlope is the slope at each point in the field (as a gradient), and valsDem is the elevation of each point in the field (in metres).\n\n\n\nThe first thing we can do is remove the ’NA’s from our dataset using the na.omit() function:\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nUse the na.omit() function to remove the ’NA’s from fairfax_field_soil_properties. Rename this new file fairfax_field_soil_properties_tidy, and check its structure.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nfairfax_field_soil_properties_tidy &lt;- na.omit(fairfax_field_soil_properties)\n\nstr(fairfax_field_soil_properties_tidy)\n\n'data.frame':   30600 obs. of  8 variables:\n $ x        : num  603371 603399 603428 603456 603484 ...\n $ y        : num  6160358 6160358 6160358 6160358 6160358 ...\n $ clay1    : num  24.9 24.9 25.4 25.4 25.4 ...\n $ clay2    : num  29.2 29.2 28.9 28.9 28.9 ...\n $ clay3    : num  31 31 29.4 29.4 29.4 ...\n $ clay4    : num  18 18 14.5 14.5 14.5 ...\n $ valsSlope: num  0.0163 0.01353 0.00949 0.01218 0.01519 ...\n $ valsDem  : num  362 362 363 363 363 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:47025] 1 2 3 4 5 6 7 8 9 10 ...\n  ..- attr(*, \"names\")= chr [1:47025] \"1\" \"2\" \"3\" \"4\" ...\n\n\nSuccess!\n\n\n\n\n\n\n\n\n\nWarningWhen can I remove ’NA’s, and when must I keep them?\n\n\n\n\n\nThe most important question ask yourself when you decide to remove ‘NA’ values is: Why is the value ‘NA’ in the first place? Usually, you will have the reason recorded in your dataset somewhere – under a ‘metadata’ tab, for instance.\nHere are the situations where it is usually OK to remove ’NA’s:\n\nIf you made no attempt to collect any data at that point.\nIf removing the rows/columns containing an ‘NA’ does not remove other, non-‘NA’ values.\n\nHere are the situations where you may want to think twice about removing ’NA’s:\n\nIf you made an attempt to collect data, but detected nothing (in which case, your ’NA’s should actually be 0s).\nIf removing the rows/columns containing an ‘NA’ inadvertently removes other, non-‘NA’ values.\n\n\n\n\nLet’s plot our data. Since the ‘x’ and ‘y’ columns are co-ordinates, what we will end up with is a map of the field.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nMake a scatter plot of fairfax_field_soil_properties_tidy, with column ‘x’ as the x-axis, and ‘y’ as the y-axis. Your plot should look like a map.\n\n\n\n\n\n\nTip\n\n\n\n\n\nThere are lots of sample points in our data! To see the shape of the field more clearly, include the arguments: size = 0.5 and alpha = 0.5 in the geom_point() section; the former decreases point size, and the latter increases point transparency.\nIf you are working on a small screen, you may need to shrink your point size even further (try 0.1, for example).\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nggplot(fairfax_field_soil_properties_tidy, \n       aes(x = x, y = y)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  theme_classic()\n\n\n\n\n\n\n\n\nThat is the shape of our field.\n\n\n\nWe want to paint this map into different colours using cluster analysis, which will tell us how we should stratify the field. Before we do this, take a moment to look back on the structure of the dataset. Do you notice anything strange?\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nLook back at the structure of fairfax_field_soil_properties_tidy. Do you notice anything different about this dataset compared to, say, the iris dataset?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nUnlike the iris dataset, which has 1 categorical variable and 4 numeric variables, fairfax_field_soil_properties_tidy has 6 numeric variables (8, if you include the x and y co-ordinates).\nMore importantly, these 6 numeric variables are measured in different units! Clay content is a percentage, slope is a gradient, and elevation is measured in metres. We have to standardise these units before performing a cluster analysis.\n\n\n\nWhen our measurements come in different units, we first have to standardise them before performing a cluster analysis (and many other multivariate analyses, for that matter). We can do this easily using the scale() function.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nSelect columns 3-8 of fairfax_field_soil_properties_tidy, and apply the scale() function to them. Name this new dataset fairfax_field_soil_properties_scaled\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nfairfax_field_soil_properties_scaled &lt;- scale(fairfax_field_soil_properties_tidy[,3:8])\n\nTo be specific, scale() performs two separate operations on each column of your dataset: first it centres (subtracts each value from the column mean), then it standardises (divides each value by the column standard deviation).\n\n\n\nNow we are ready to paint the map. How many colours should we choose? It’s up to you; but I’m going with 3, just like last time. I will also include the nstart = 20 argument again, as 20 ~ 25 initial configurations is standard practice for k-means clustering.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nPerform a non-hierarchical cluster analysis on fairfax_field_soil_properties_scaled using the kmeans() function. Name this analysis fairfax_k_means_?_clusters, where ‘?’ is the number of clusters you choose to use.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nI went with 3 clusters:\n\nfairfax_k_means_3_clusters &lt;- kmeans(fairfax_field_soil_properties_scaled, 3, nstart = 20)\n\n\n\n\nNow to plot our results:\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nExtract the cluster column from fairfax_k_means_3_clusters, and use this column to colour-code your original scatter plot.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nExtract the cluster column and turn it into a factor:\n\nfairfax_clusters &lt;- as.factor(fairfax_k_means_3_clusters$cluster)\n\nUse it to colour-code your map:\n\nggplot(fairfax_field_soil_properties_tidy, \n       aes(x = x, y = y, \n           colour = fairfax_clusters)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  scale_colour_manual(values = c('red','steelblue','black'))+\n  theme_classic()\n\n\n\n\n\n\n\n\nDone :)\n\n\n\nNow we have painted similar regions in the same colour. If we decide to undertake stratified random sampling on this field in the future, these can be our 3 different strata.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nWhy are the clusters on this map poorly separated? i.e. Why are the red regions interspersed with the black and blue regions, instead of each colour forming its own cluster as you would expect from a cluster analysis?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nRecall that we encountered a similar problem with the iris dataset. However, this time the reason is completely different.\nThe reason this time is that our plot does not have response variables on the x and y axes. Instead, it is a map of the real world with co-ordinates on the x and y axes.\nHere is the same plot if our axes had been the response variables clay2 and clay3 instead:\n\nggplot(fairfax_field_soil_properties_tidy, \n       aes(x = clay2, y = clay3, \n           colour = fairfax_clusters)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  scale_colour_manual(values = c('red','steelblue','black'))+\n  theme_classic()\n\n\n\n\n\n\n\n\nYou can see that the three clusters are, in fact, nice and distinct. If we could plot all 6 response variables on a 6D plot, we would see that the clusters are perfectly separated.\n\n\n\nThat’s all for k-means clustering!\n\n\n\n\n\n\n\n\n\nThe vibrant Clarkia pulchella, a woody annual plant native to North America. In the next exercise, we will look at genetic data gathered from a closely related species, Clarkia springvillensis. From Wikimedia Commons (2016), by Dinkum."
  },
  {
    "objectID": "labs/Lab11/Lab11-clustering.html#part-3-from-scatter-plots-to-dendrograms",
    "href": "labs/Lab11/Lab11-clustering.html#part-3-from-scatter-plots-to-dendrograms",
    "title": "Lab 11",
    "section": "Part 3: From Scatter Plots to Dendrograms",
    "text": "Part 3: From Scatter Plots to Dendrograms\nThe last part of our lab moves away from non-hierarchical clustering and into hierarchical clustering. There are many hierarchical clustering methods, but the one we will use today is called the Unweighted Pair Group Method with Arithmetic Mean, or UPGMA.\n\n\n\n\n\n\n\n\n\nA dendrogram, or ‘tree’. Trees are useful ways to depict ranked relationships between objects; in this case, the relationship between different mammal species. From Wikimedia Commons (2007), by Fred Hsu.\n\nExercise: Hierarchical clustering\nIn 1996, Kimberlie McCue and her research team sampled the genetic material of 240 Clarkia springvillensis plants along the North fork of the Tule River.1\n1 McCue, K. A., Buckler, E. S., & Holtsford, T. P. (1996). A Hierarchical View of Genetic Structure in the Rare Annual Plant Clarkia springvillensis. Conservation Biology, 10(5), 1425–1434. JSTOR. https://doi.org/10.2307/2386917These 240 plants came from 8 different sub-populations. 3 of these sub-populations came from a site called Bear Creek (BC), another 3 came from Springville Clarkia Ecological Reserve (SCER), and the last 2 sub-populations came from a site called Gauging Station (GS).\nThe question we want to answer is whether sub-populations from the same site share similar genetic markers.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nRead the dataset mccue2.csv into R, rename it Clarkia_genetics, and check its structure. Once again, please include the row.names = 1 argument.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\nClarkia_genetics &lt;- read.csv(\"data/mccue2.csv\", row.names = 1)\nstr(Clarkia_genetics)\n\n'data.frame':   8 obs. of  8 variables:\n $ BC1  : num  0 0.08 0.095 0.225 0.082 0.065 0.071 0.029\n $ BC2  : num  0.08 0 0.03 0.213 0.095 0.068 0.052 0.045\n $ BC3  : num  0.095 0.03 0 0.244 0.082 0.051 0.037 0.066\n $ GS1  : num  0.225 0.213 0.244 0 0.179 0.196 0.184 0.187\n $ GS2  : num  0.082 0.095 0.082 0.179 0 0.093 0.079 0.085\n $ SCER1: num  0.065 0.068 0.051 0.196 0.093 0 0.069 0.05\n $ SCER2: num  0.071 0.052 0.037 0.184 0.079 0.069 0 0.068\n $ SCER3: num  0.029 0.045 0.066 0.187 0.085 0.05 0.068 0\n\n\nThat’s interesting… we have our sub-populations as columns (BC1, BC2, etc.), and every number is a decimal. What is going on?\n\n\n\nThe structure of this dataset seems rather strange. Let’s print out the whole table to see what is happening:\n\n\n\n\n\n\nNoteThe Clarkia_genetics dataset\n\n\n\n\n\n\nClarkia_genetics\n\n        BC1   BC2   BC3   GS1   GS2 SCER1 SCER2 SCER3\nBC1   0.000 0.080 0.095 0.225 0.082 0.065 0.071 0.029\nBC2   0.080 0.000 0.030 0.213 0.095 0.068 0.052 0.045\nBC3   0.095 0.030 0.000 0.244 0.082 0.051 0.037 0.066\nGS1   0.225 0.213 0.244 0.000 0.179 0.196 0.184 0.187\nGS2   0.082 0.095 0.082 0.179 0.000 0.093 0.079 0.085\nSCER1 0.065 0.068 0.051 0.196 0.093 0.000 0.069 0.050\nSCER2 0.071 0.052 0.037 0.184 0.079 0.069 0.000 0.068\nSCER3 0.029 0.045 0.066 0.187 0.085 0.050 0.068 0.000\n\n\n\n\n\nWhat do you make of it?\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nTake a look at the Clarkia_genetics table. What do you think the values in this table represent?\n\n\n\n\n\n\nTip\n\n\n\n\n\nNotice the long diagonal of 0s stretching from the top left to the bottom right corner.\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis table is a dissimilarity matrix! Each number represents how different the genetic markers are between any two sub-populations. This is why we have the sub-population names as both the row and column headings.\n\n\n\nIn fact, our data already comes in the form of a dissimilarity matrix. Dr McCue and her team used a special dissimilarity measured called the Cavalli-Sforza chord genetic distance to generate this matrix.\nBecause of this, we do not need to scale our data or generate our own dissimilarity matrices like we did for the previous exercises. Instead, we can jump straight into clustering.\nThe function we will use is called hclust, ‘h’ for ‘hierarchical’.\nThe following line of code should do the trick:\n\nhc &lt;- hclust(as.dist(Clarkia_genetics), method = \"average\")\n\nTo see our results, we use the base-R function plot():\n\nplot(hc, \n     main=\"Dendrogram of *Clarkia* Sub-Populations\",  \n      ylab=\"Cavalli-Sforza chord distance\")\n\n\n\n\n\n\n\n\nHow do we interpret this graph?\nIf you are familiar with how to read a phylogenetic tree, then you can read this dendrogram in the same way – just mentally replace the phrase “common ancestor” with “shared node”, and the phrase “closely related” with “similar”; then you pretty much have the right idea.\nIf you are not familiar with phylogenetic trees, consider this your official introduction to them. Start from the tips of the tree, which in this case would be our eight sub-populations. To see how similar one sub-population is to another, trace both of them back until you find the branch where they split from one another. The further back this branch is, the less similar the two sub-populations are.\nIn biology, this ‘splitting branch’ is called the last common ancestor of two taxa. In Cluster Analysis, this branch… does not really have a name, but I like to call it the last shared node between two samples. The difference, of course, is that there is no evolution going on in Cluster Analysis, so the concept of ‘ancestors’ and ‘descendants’ do not apply.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nBased on the above dendrogram, would you say that Clarkia springvillensis sub-populations in the same site shared similar genetic markers?\n\n\n\n\n\n\n\n\nTipWhat we think\n\n\n\n\n\nWe think there is no obvious similarity between C. springvillensis sub-populations in the same site. Otherwise, we would see all the BC sub-populations grouped together, and all the SCER sub-populations grouped together.\nWhat we actually see, however, is that the BC1 sub-population is more genetically similar to the SCER1 and SCER3 sub-populations than it is to other BC sub-populations.\nThe GS sites represent a clear “outgroup”, in that GS1 and GS2 are very different from every other sub-population, and also from each other.\n\n\n\n\n\n\n\n\n\n\n\n\nAn open forest dominated by Mountain Ash (Eucalyptus regnans). Mountain Ash are native to Tasmania, and adult Mountain Ashes can grow to be some of the tallest trees in the world, beaten only by the redwoods of North America. From Wikimedia Commons (2000), by the Commonwealth Scientific and Industrial Research Organisation (CSIRO).\n\n\nPractice: Clustering Cars\nNow it is your turn to perform a clustering procedure of your own. Just like last week, we recommend that you collaborate with your peers on this exercise, and ask your demonstrators for help if you get lost.\nGood luck!\n\n\n\n\n\n\nNoteCase study\n\n\n\nPerform a clustering procedure on the mtcars dataset. This dataset is built into R just like iris. To find out more, type ?mtcars into your console.\nWe will leave you to decide what type of clustering procedure to perform, how much of the dataset to use, and what the aim of your analysis is.\n\n\nThat’s all for this lab. Have a good rest of the week!"
  },
  {
    "objectID": "labs/Lab09/Lab09-regression-assessment.html",
    "href": "labs/Lab09/Lab09-regression-assessment.html",
    "title": "ENVX2001 Lab - Predictive modelling",
    "section": "",
    "text": "In this lab, you will work towards achieving learning outcomes"
  },
  {
    "objectID": "labs/Lab09/Lab09-regression-assessment.html#learning-outcomes",
    "href": "labs/Lab09/Lab09-regression-assessment.html#learning-outcomes",
    "title": "ENVX2001 Lab - Predictive modelling",
    "section": "",
    "text": "In this lab, you will work towards achieving learning outcomes"
  },
  {
    "objectID": "labs/Lab09/Lab09-regression-assessment.html#lab-objectives",
    "href": "labs/Lab09/Lab09-regression-assessment.html#lab-objectives",
    "title": "ENVX2001 Lab - Predictive modelling",
    "section": "Lab Objectives",
    "text": "Lab Objectives\nIn this lab, we will:\n\n[]\n[]\n\n\n\n\n\n\n\nTip\n\n\n\nPlease work on this exercise by creating your own R Markdown file."
  },
  {
    "objectID": "labs/Lab09/Lab09-regression-assessment.html#exercise-1",
    "href": "labs/Lab09/Lab09-regression-assessment.html#exercise-1",
    "title": "ENVX2001 Lab - Predictive modelling",
    "section": "Exercise 1:",
    "text": "Exercise 1:"
  },
  {
    "objectID": "labs/Lab09/Lab09-regression-assessment.html#exercise-2",
    "href": "labs/Lab09/Lab09-regression-assessment.html#exercise-2",
    "title": "ENVX2001 Lab - Predictive modelling",
    "section": "Exercise 2:",
    "text": "Exercise 2:"
  },
  {
    "objectID": "labs/Lab09/Lab09-regression-assessment.html#take-home-exercise",
    "href": "labs/Lab09/Lab09-regression-assessment.html#take-home-exercise",
    "title": "ENVX2001 Lab - Predictive modelling",
    "section": "Take home exercise:",
    "text": "Take home exercise:"
  },
  {
    "objectID": "labs/Lab09/Lab09-regression-assessment.html#review",
    "href": "labs/Lab09/Lab09-regression-assessment.html#review",
    "title": "ENVX2001 Lab - Predictive modelling",
    "section": "Review",
    "text": "Review\n\nAttribution"
  },
  {
    "objectID": "labs/Lab07/Lab07-regression-modelling.html",
    "href": "labs/Lab07/Lab07-regression-modelling.html",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "",
    "text": "In this lab, you will work towards achieving learning outcomes L03, and L05.\n\n\nIn this lab, we will:\n\nIdentify best predictors for model - Exercise 1\nFit model and check assumptions - Exercise 1\nInterpret model output - Exercise 1\n\n\n\n\n\n\n\nTip\n\n\n\nPlease work on this exercise by creating your own R Markdown file.\n\n\n\n\n\n\nInstall or update the performance package\n\n\n#install.packages(\"performance\")\nlibrary(performance)\n\nThis package is really good for checking your models. For this lab, we will focus on the check_model() function, which gives us nice pretty diagnostic plots for models:\n\nplot()check_model() from performance\n\n\n\npar(mfrow=c(2,2))\nplot(iris_lm)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n\n\n\nlibrary(performance)\ncheck_model(iris_lm)\nsummary(lm.mod2)"
  },
  {
    "objectID": "labs/Lab07/Lab07-regression-modelling.html#learning-outcomes",
    "href": "labs/Lab07/Lab07-regression-modelling.html#learning-outcomes",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "",
    "text": "In this lab, you will work towards achieving learning outcomes L03, and L05.\n\n\nIn this lab, we will:\n\nIdentify best predictors for model - Exercise 1\nFit model and check assumptions - Exercise 1\nInterpret model output - Exercise 1\n\n\n\n\n\n\n\nTip\n\n\n\nPlease work on this exercise by creating your own R Markdown file.\n\n\n\n\n\n\nInstall or update the performance package\n\n\n#install.packages(\"performance\")\nlibrary(performance)\n\nThis package is really good for checking your models. For this lab, we will focus on the check_model() function, which gives us nice pretty diagnostic plots for models:\n\nplot()check_model() from performance\n\n\n\npar(mfrow=c(2,2))\nplot(iris_lm)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n\n\n\nlibrary(performance)\ncheck_model(iris_lm)"
  },
  {
    "objectID": "labs/Lab07/Lab07-regression-modelling.html#exercise-1-modelling-bird-abundance",
    "href": "labs/Lab07/Lab07-regression-modelling.html#exercise-1-modelling-bird-abundance",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "Exercise 1: Modelling bird abundance",
    "text": "Exercise 1: Modelling bird abundance\nWe will now use the transformed data in loyn for this exercise. If you have not already figured out how to perform the transformation, or if something is wrong, you may use the loyn tab in the mlr.xlsx MS Excel document. Alternatively, the code to convert the data is below.\n\n\n\n\n\n\nTip\n\n\n\nThis is the same data we used in the walkthrough exercise\n\n\n\n# Load library if needed\nlibrary(readxl)\n# reset the data import just in case it has been modified\nloyn &lt;- read_xlsx(\"data/mlr.xlsx\", \"Loyn\")\n# make transformations\n\nloyn &lt;- loyn %&gt;%\n    mutate(\n        L10AREA = log10(AREA),\n        L10DIST = log10(DIST),\n        L10LDIST = log10(LDIST)\n    )\n\n# check\nglimpse(loyn)\n\nRows: 56\nColumns: 10\n$ ABUND    &lt;dbl&gt; 5.3, 2.0, 1.5, 17.1, 13.8, 14.1, 3.8, 2.2, 3.3, 3.0, 27.6, 1.…\n$ AREA     &lt;dbl&gt; 0.1, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2…\n$ YR.ISOL  &lt;dbl&gt; 1968, 1920, 1900, 1966, 1918, 1965, 1955, 1920, 1965, 1900, 1…\n$ DIST     &lt;dbl&gt; 39, 234, 104, 66, 246, 234, 467, 284, 156, 311, 66, 93, 39, 4…\n$ LDIST    &lt;dbl&gt; 39, 234, 311, 66, 246, 285, 467, 1829, 156, 571, 332, 93, 39,…\n$ GRAZE    &lt;dbl&gt; 2, 5, 5, 3, 5, 3, 5, 5, 4, 5, 3, 5, 2, 1, 5, 5, 3, 3, 3, 2, 2…\n$ ALT      &lt;dbl&gt; 160, 60, 140, 160, 140, 130, 90, 60, 130, 130, 210, 160, 210,…\n$ L10AREA  &lt;dbl&gt; -1.0000000, -0.3010300, -0.3010300, 0.0000000, 0.0000000, 0.0…\n$ L10DIST  &lt;dbl&gt; 1.591065, 2.369216, 2.017033, 1.819544, 2.390935, 2.369216, 2…\n$ L10LDIST &lt;dbl&gt; 1.591065, 2.369216, 2.492760, 1.819544, 2.390935, 2.454845, 2…\n\n\n\nBest single predictor?\n\n\nQuestion 1\nObtain the correlation between ABUND and all of the predictor variables using cor(). Based on these, what would you expect to be the best single predictor of ABUND?\n\n\ncor(loyn)\n\n\nAssumptions and interpretation\n\n\nQuestion 2\nUse multiple linear regression to see whether ABUND can be predicted from L10AREA and GRAZE. Are the assumptions met? Is there a significant relationship? Note: we are using these 2 predictors as they have the largest absolute correlations. Use lm() and specify the model as ABUND ~ L10AREA + GRAZE.\n\n\nlm.mod1 &lt;- lm(ABUND ~ GRAZE + L10AREA, data = loyn)\n\npar(mfrow = c(2, 2))\nplot(lm.mod1)\npar(mfrow = c(1, 1))\n\nsummary(lm.mod1)\n\n\nQuestion 3\nHow good is the model based on the (i) r2 (ii) adjusted r2? Use summary().\n\n\nsummary(lm.mod1)$r.squared\nsummary(lm.mod1)$adj.r.squared\n\n\nQuestion 4\nWhich variable(s) has the most significant effect(s)? (Refer specifically to the t probabilities in the table of predictors and their estimated parameters or coefficients in the output of summary()). Interpret the p-values in terms of dropping predictor variables."
  },
  {
    "objectID": "labs/Lab07/Lab07-regression-modelling.html#question-5",
    "href": "labs/Lab07/Lab07-regression-modelling.html#question-5",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "Question 5",
    "text": "Question 5\nRepeat the multiple regression, but this time include YRS.ISOL as a predictor variable (it has the 3rd largest absolute correlation). This will allow you to assess the effect of YRS.ISOL with the other variables taken into account."
  },
  {
    "objectID": "labs/Lab07/Lab07-regression-modelling.html#exercise-2-california-streamflow",
    "href": "labs/Lab07/Lab07-regression-modelling.html#exercise-2-california-streamflow",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "Exercise 2: California streamflow",
    "text": "Exercise 2: California streamflow\nThe following dataset contains 43 years of annual precipitation measurements (in mm) taken at (originally) 6 sites in the Owens Valley in California. I have reduced this to three variables labelled lake_sabrina (Lake Sabrina), pine_creek (Big Pine Creek), rock_creek (Rock Creek), and the dependent variable stream runoff volume (measured in ML/year) at a site near Bishop, California (labelled runoff_volume).\nNote the variables have already been log-transformed to increase normality of the residuals in the regressions.\nStart with a full model and manually remove the variables one at a time, checking every time whether removal of a variable actually improves the model.\n\n# read in the data\nstream_data &lt;- read_xlsx(\"data/california_streamflow.xlsx\", \"streamflow\")\nnames(stream_data)\n\n[1] \"lake_sabrina\"  \"pine_creek\"    \"rock_creek\"    \"runoff_volume\"\n\n\n\ns.mod_full &lt;-lm(runoff_volume~lake_sabrina + pine_creek + rock_creek, data=stream_data)\ns.mod_full &lt;-lm(runoff_volume~., data=stream_data) ## you can also use the . to indicate use all variables\nsummary(s.mod_full)\n\n\nCall:\nlm(formula = runoff_volume ~ ., data = stream_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09885 -0.03331  0.01025  0.03359  0.09495 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.25716    0.12360  26.352  &lt; 2e-16 ***\nlake_sabrina  0.05631    0.03756   1.499  0.14185    \npine_creek    0.21085    0.06756   3.121  0.00339 ** \nrock_creek    0.43838    0.08798   4.983 1.32e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04861 on 39 degrees of freedom\nMultiple R-squared:  0.8817,    Adjusted R-squared:  0.8726 \nF-statistic: 96.88 on 3 and 39 DF,  p-value: &lt; 2.2e-16\n\n\n\nPartial F-Tests\nThe above analysis tells us that both pine_creek & rock_creek are significant, according to the t-test, in the model and lake_sabrina is not? This involves performing Partial F-Tests as discussed in the lecture.\nThis can be done in R by using anova() on two model objects. To be able to compare the models and run the anova, you need to make objects of all the possible model combinations you want to compare.\n\ns.mod_reduced &lt;- lm(runoff_volume ~ rock_creek + pine_creek, data=stream_data)\nanova(s.mod_reduced, s.mod_full)\n\nThe last row gives the results of the partial F-test.\n\n\nQuestion 1\nShould we remove lake_sabrina from the model?\n\n\nQuestion 2\nIs the p-value for the f-test the same as for the t-test?\n\n\nQuestion 3\nWrite out the hypotheses you are testing.\n\nPerform a Partial F-Test to work out if the removal of lake_sabrina and pine_creek improves upon the full model.\n\ns.mod_reduced2  &lt;- lm(runoff_volume ~ lake_sabrina + pine_creek,data=stream_data)\nanova(s.mod_reduced2, s.mod_full)\n\nAnalysis of Variance Table\n\nModel 1: runoff_volume ~ lake_sabrina + pine_creek\nModel 2: runoff_volume ~ lake_sabrina + pine_creek + rock_creek\n  Res.Df      RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     40 0.150845                                 \n2     39 0.092166  1   0.05868 24.83 1.321e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nQuestion 4\nWhich variable should be added to the model containing rock_creek?\n\n\nQuestion 5\nCould things be even simpler? Perform a partial F-Test to see if a model containing rock_creek alone could be suitable.\n\n\ns.mod_reduced3  &lt;- lm(runoff_volume ~ rock_creek,data=stream_data)\nanova(s.mod_reduced3, s.mod_full)\n\n\nQuestion 6\nWhat is your optimal model?"
  },
  {
    "objectID": "labs/Lab07/Lab07-regression-modelling.html#review",
    "href": "labs/Lab07/Lab07-regression-modelling.html#review",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "Review",
    "text": "Review\n\nSimple linear regressions model the relationship between two variables\n\nWe can also make linear models with more than one predictor\n\nWe can use histograms and correlation matrices to do some preliminary exploration of the data\nIf any variables are skewed, we can transform them\nLooking at a correlation matrix to identify the best predictors (for both simple and multiple linear regression)\nFit model using lm() function\nCheck assumptions:\n\nCollinearity (multiple linear regression only)\nLinearity\nIndependence\nNormality\nEqual variance\n\nUse summary() to look at model output and interpret it\n\nF-test : overall model significance\nCoefficients table : individual predictors’ significance\nR2 : How much variation in the data is explained by the model?\n\n\nThat’s it for today! Great work fitting simple and multiple linear regression! Next week we jump into stepwise selection!\n\nAttribution"
  },
  {
    "objectID": "labs/Lab05/Lab05-exp-designs.html",
    "href": "labs/Lab05/Lab05-exp-designs.html",
    "title": "Lab 5 - Experimental Design",
    "section": "",
    "text": "TipLearning outcomes\n\n\n\nAt the end of this lab students should be able to:\n\ndistinguish between sampling units and experimental units;\nuse R to generate randomisations for CRDs and RCBDs;\nuse R to analyse experiments with blocking, and assess the usefulness of blocking.\n\nAll of the data for this lab is in the Data5.xlsx file."
  },
  {
    "objectID": "labs/Lab05/Lab05-exp-designs.html#exercise-1---randomisation-for-a-crd-using-r-walk-through",
    "href": "labs/Lab05/Lab05-exp-designs.html#exercise-1---randomisation-for-a-crd-using-r-walk-through",
    "title": "Lab 5 - Experimental Design",
    "section": "Exercise 1 - Randomisation for a CRD using R (Walk-through)",
    "text": "Exercise 1 - Randomisation for a CRD using R (Walk-through)\nConsider a glass house experiment conducted on a bench on which it was judged that the growing conditions would be consistent across the bench. The experimenter had five different smoked water treatments for assisting the germination of Banksia seeds. She had 50 dishes on which she could place seeds randomly selected from a uniform batch of Banksia seeds. The dishes were to be placed on the bench and each of the five water solutions allocated randomly to 10 of the dishes.\n\nQuestion 1.1\n(i) What would the degrees of freedom be in a corresponding ANOVA table?\n\n\nQuestion 1.2\n(ii) Suppose that Treatment 1 was a control. In situations where the comparisons of interest are of the other treatments (2 to 5) with the control, and not amongst the other treatments, it can be advantageous to increase the number of control replicates to 20. What are the degrees of freedom in the corresponding ANOVA table?\n\n\nQuestion 1.3\n(iii) The formula for standard error of the difference (SED) is:\nSED=\\sqrt{Resid\\ MS\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)}.\nGiven the SED formula, what percentage improvement (i.e. reduction) has been achieved for the SED between Treatment 1 and 2 when there are equal replicates (i) versus unequal replicates (ii)? Assume the Residual MS is a constant."
  },
  {
    "objectID": "labs/Lab05/Lab05-exp-designs.html#exercise-2---the-balance-between-the-number-of-sampling-units-and-experimental-units-walk-through",
    "href": "labs/Lab05/Lab05-exp-designs.html#exercise-2---the-balance-between-the-number-of-sampling-units-and-experimental-units-walk-through",
    "title": "Lab 5 - Experimental Design",
    "section": "Exercise 2 - The balance between the number of sampling units and experimental units (Walk-through)",
    "text": "Exercise 2 - The balance between the number of sampling units and experimental units (Walk-through)\nHorticulturalists have been studying the nutrient requirements of lettuce growing in a sand medium; in particular they are looking at the nitrate concentration in leaf petioles in response to varying applied nitrogen nutrient levels (5, 11, 18 and 32 mmol/L). Lettuce plants are grown in separate pots, so different nitrogen levels may be applied to individual pots, and one or more leaves sampled per plant. They are also interested in optimising their experimental protocol for future studies; consequently they conducted five separate experiments where they sampled different number of leaves per plant, and differing number of plants, but kept the total number of leaves sampled at 128 in each experiment. The experiments were conducted as follows:\n\nExperiment 1: 16 leaves per plant; 2 plants per treatment;\nExperiment 2: 8 leaves per plant; 4 plants per treatment;\nExperiment 3: 4 leaves per plant; 8 plants per treatment;\nExperiment 4: 2 leaves per plant; 16 plants per treatment;\nExperiment 5: 1 leaf per plant; 32 plants per treatment.\n\nSimilar lettuce plants were used across all five experiments, and in all cases, a CRD was used to allocate the plants amongst the treatments. The datasets are located in the Data5.xlsx file. There are separate sheets for each Experiment called Experiment1, Experiment2, etc.\n\nQuestion 2.1\n(i) What are the experimental units and what are the sampling units in the above experiments. For each experiment, how many experimental units and how many sampling units are used?\n\n\nQuestion 2.2\n(ii) When there are multiple sampling units per experimental unit, in simple situations (as is here), an appropriate method is simply to average the responses over all the sampling units belonging to an experimental unit, and analyse these. So in this case, we need to average the observed nitrate values over all the leaves in the plant. The code below shows how to do this using a function called rowMeans which finds the average in a row across multiple columns. Here we do this for Experiment1.\n\n#\n\n\n\nQuestion 2.3\n(iii) Perform a one-way ANOVA on these average data. (It will also be helpful to obtain the treatment means using the emmeans function, as done in previous topics. What are your conclusions about the effect of varying applied nitrogen levels? You don’t have to look at post-hoc tests, just examine the F-test.\n\n#\n\n\n\nQuestion 2.4\n(iV) Repeat this for Experiments 2 to 5. What are your conclusions in each experiment?\n\nExperiment 2: 8 leaves per plant, 4 plants per treatment\n\n#\n\nExperiment 3: 4 leaves per plant, 8 plants per treatment\n\n#\n\nExperiment 4: 2 leaves per plant, 16 plants per treatment\n\n#\n\nExperiment 5: 1 leaf per plant, 32 plants per treatment. Note, no averaging required as sampling unit = experimental unit.\n\n#\n\n\nQuestion 2.5\n(v) Summarise the results in terms of the balance between sampling plants versus leaves, and what recommendations would you make for future studies?\n\n#\n\n\n\nQuestion 2.6\n(vi) There are two sources of random variation encountered in these experiments:\n-variation between leaves within a plant; and\n-variation between plants within the same treatment group.\nHow do you think your recommendations might change in (iv) if:\n-there was (virtually) no variation between leaves within a plant; only between plants; or\n-there was (virtually) no variation between plants, only variation between leaves within a plant."
  },
  {
    "objectID": "labs/Lab05/Lab05-exp-designs.html#exercise-3---a-paired-t-test-generalises-to-a-one-way-anova-rcbd",
    "href": "labs/Lab05/Lab05-exp-designs.html#exercise-3---a-paired-t-test-generalises-to-a-one-way-anova-rcbd",
    "title": "Lab 5 - Experimental Design",
    "section": "Exercise 3 - A paired t-test generalises to a one-way ANOVA – RCBD",
    "text": "Exercise 3 - A paired t-test generalises to a one-way ANOVA – RCBD\nFifteen farms cooperated in a field trial in which a normal fattening ration for pigs (ration A) was compared with the same rations supplemented with a small trace of copper (ration B). Each farmer set up two pens of pigs, as similar as possible in all respects, and allocated the two rations at random, ration A to one pen and ration B to the other. The mean weight gains per pen (g/day) are stored in the two columns of the copper sheet in the Data5.xlsx file.\nThis description clearly suggests the design is an RCBD, with 15 farms but only 2 treatments. As we have seen, a paired design is the simplest form of RCBD. In this example we will not worry about model assumptions.\n\nQuestion 3.1\n(i) Firstly analyse the data using a paired t-test. Assuming you read the data is as a data frame called copper the code is t.test(x=copper$RationA,y=copper$RationB,paired=T). Interpret the results.\n\n#\n\n\n\nQuestion 3.2\n(ii) Next we will obtain an analysis of the data using ANOVA. To do this analysis however, you will first need to re-arrange the data set into a form suitable for ANOVA. That is, you will need a single column of weight gains, a column of treatment identifiers, and a column indicating the Farm number. The code below will do this for you:\n\ndat&lt;-stack(copper[,2:3])\nfarms&lt;-rep(c(1:15),times=2)\ncoppern&lt;-cbind(farms,dat)\n\nThe first line stacks (using the stack function) the 2 columns of weight gain on top of each other (column name is values) and adds a column indicating which ration is associated with each weight gain value (column name is ind).\nThe second line repeats (using the rep function) the values of farm numbers twice so for each of the weight gains we have an associated farm.\nThe third line joins the two data frames together (using the cbind function) to create the new stacked dataset which can be used by the aov function.\n\nhead(copper)\n\n# A tibble: 6 × 3\n   Farm RationA RationB\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1     422     531\n2     2     526     467\n3     3     476     558\n4     4     499     585\n5     5     422     472\n6     6     503     522\n\ndat&lt;-stack(copper[,2:3])\nfarms&lt;-rep(c(1:15),times=2)\ncoppern&lt;-cbind(farms,dat)\nstr(coppern)\n\n'data.frame':   30 obs. of  3 variables:\n $ farms : int  1 2 3 4 5 6 7 8 9 10 ...\n $ values: num  422 526 476 499 422 503 445 449 299 517 ...\n $ ind   : Factor w/ 2 levels \"RationA\",\"RationB\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nQuestion 3.3\n(iii) The general code for performing an 1-way anova with a randomised complete block design is:\naov(response~block+treatment,data=data)\nIn this example the response is the weight gain, blocks are farms and the treatment is ration. Your names will be different. Having obtained the analysis, check that the results from the ANOVA is identical to the paired t-test:\n\nthe same P-value;\nthe same test statistic (though expressed as an F-statistic instead of a t-statistic, with F = t^2 to that from the paired t-test.\n\n\n#"
  },
  {
    "objectID": "labs/Lab05/Lab05-exp-designs.html#exercise-4---one-way-anova---rcbd",
    "href": "labs/Lab05/Lab05-exp-designs.html#exercise-4---one-way-anova---rcbd",
    "title": "Lab 5 - Experimental Design",
    "section": "Exercise 4 - One-way ANOVA - RCBD",
    "text": "Exercise 4 - One-way ANOVA - RCBD\nThree diets for hamsters were tested for differences in weight gain after a specified period of time. Six inbred lines were used with three hamsters selected from each line. The three diets were assigned at random to the hamsters in each line. The data is in the hamsters sheet in the Data5.xlsx file.\n\nQuestion 4.1\n(i) Are the mean weight gains the same across diets? Are any diets more or less effective than other diets? You will need to use the emmeans package and its emmeans() function. Read in the data and perform post-hoc tests.\n\n#\n\n\n\nQuestion 4.2\n(ii) Re-run the analysis without blocking, i.e. as a CRD. Are the conclusions different? Would you use blocking in the future? What proportion of the variation was explained by blocking?\n\n#"
  },
  {
    "objectID": "labs/Lab03/Lab03-oneway-anova.html",
    "href": "labs/Lab03/Lab03-oneway-anova.html",
    "title": "Lab 03",
    "section": "",
    "text": "In this week’s lab we will work through the experimental design workflow for a one-way ANOVA."
  },
  {
    "objectID": "labs/Lab03/Lab03-oneway-anova.html#learning-outcomes",
    "href": "labs/Lab03/Lab03-oneway-anova.html#learning-outcomes",
    "title": "Lab 03",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nIn this lab, you will work towards achieving learning outcomes…\n\nLab objectives\nAt the end of the lab, students should be able to:\n\nUnderstand the workflow involved in conducting a one-way Analysis of Variance (ANOVA), including data exploration, model fitting, and interpretation of results.\nAnalyse an experiment with a 1-way ANVOVA and interpret the results.\nExplain situations when a 2-sample t-test gives the same results as a 1-way ANOVA."
  },
  {
    "objectID": "labs/Lab03/Lab03-oneway-anova.html#prerequisites",
    "href": "labs/Lab03/Lab03-oneway-anova.html#prerequisites",
    "title": "Lab 03",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe following packages are used for this lab: readxl, ggplot2, dplyr and tidyr. To install these packages, run the following code in the console.\ninstall.packages(c(\"readxl\", \"ggplot2\", \"dplyr\", \"tidyr\"))"
  },
  {
    "objectID": "labs/Lab03/Lab03-oneway-anova.html#exercise-1-diatoms",
    "href": "labs/Lab03/Lab03-oneway-anova.html#exercise-1-diatoms",
    "title": "Lab 03",
    "section": "Exercise 1 – diatoms",
    "text": "Exercise 1 – diatoms\nMedley & Clements (1998) sampled 34 locations along streams for diversity of diatoms. Each site was classified according to the Zn concentration in the water. There were 4 classes; background, low, medium and high. Were there differences between each of the groupings in term of diatom diversity? Let’s find out.\n\nPractice 1 – Data import\nImport the Diatoms worksheet from the diatoms.xlsx file into R.\nIf you have difficulty with this step please refer to this week’s tutorial on importing MS Excel files.\nlibrary(readxl)\n\n# You will need to look at the Excel file and work out \n# the correct worksheet name and range:\ndiatoms &lt;- read_excel(\n    path = \"diatoms.xlsx\",\n    sheet = ...,\n    range = ...\n)\n\n\nWorkflow\nWe will briefly go through a typical analytical workflow which involves data exploration, model fitting, checking of model assumptions1 and interpretation of the results.\n1 Assumptions will be covered formally next week when we look at model residuals.\n\nData exploration\n\nData structure\nChecking the structure of the data is the first step in any data analysis. This is done using the str() function in R.\nstr(diatoms)\nOf particular interest are the variables Stream and Zinc, which have been classified as characters (chr).\n\n\ntibble [34 × 4] (S3: tbl_df/tbl/data.frame)\n $ Stream   : chr [1:34] \"Eagle\" \"Blue\" \"Blue\" \"Blue\" ...\n $ Zinc     : chr [1:34] \"BACK\" \"BACK\" \"BACK\" \"BACK\" ...\n $ Diversity: num [1:34] 2.27 1.7 2.05 1.98 2.2 1.53 0.76 1.89 1.4 2.18 ...\n $ Group    : num [1:34] 1 1 1 1 1 1 1 1 2 2 ...\n\n\nThese variables are most likely factors and should be converted to such. This can be done using as.factor().\n\ndiatoms$Zinc &lt;- as.factor(diatoms$Zinc)\ndiatoms$Stream &lt;- as.factor(diatoms$Stream)\n\n\n\n\n\n\n\nTip\n\n\n\nThe tidyverse approach – We can use the mutate() function to convert the Zinc and Stream variables to factors.\nlibrary(dplyr)\ndiatoms &lt;- diatoms %&gt;%\n    mutate(\n        Zinc = as.factor(Zinc),\n        Stream = as.factor(Stream)\n    )\n\n\nWe can then check if the conversion was successful by using the str() function again.\n\nstr(diatoms)\n\ntibble [34 × 4] (S3: tbl_df/tbl/data.frame)\n $ Stream   : Factor w/ 6 levels \"Arkan\",\"Blue\",..: 4 2 2 2 5 6 6 6 1 1 ...\n $ Zinc     : Factor w/ 4 levels \"BACK\",\"HIGH\",..: 1 1 1 1 1 1 1 1 3 3 ...\n $ Diversity: num [1:34] 2.27 1.7 2.05 1.98 2.2 1.53 0.76 1.89 1.4 2.18 ...\n $ Group    : num [1:34] 1 1 1 1 1 1 1 1 2 2 ...\n\n\n\n\nSummary statistics\nUse summary() for a quick overview of common statistical measures for each variable in the data frame. However it is not informative when we are interested in differences between groups or factors as it only gives the summary statistics for the entire data set.\n\nsummary(diatoms)\n\n   Stream    Zinc     Diversity         Group      \n Arkan:7   BACK:8   Min.   :0.630   Min.   :1.000  \n Blue :7   HIGH:9   1st Qu.:1.377   1st Qu.:2.000  \n Chalk:5   LOW :8   Median :1.855   Median :3.000  \n Eagle:4   MED :9   Mean   :1.694   Mean   :2.559  \n Snake:5            3rd Qu.:2.058   3rd Qu.:3.750  \n Splat:6            Max.   :2.830   Max.   :4.000  \n\n\nIt is more useful to calculate summary statistics for each level of Zn contamination, as we are interested in differences between the mean of each group.\nWe can use the tapply() function to calculate the mean and standard deviation for each level of Zinc. This has to be done separately for each summary statistic (mean and standard deviation).\nThe general structure of the tapply() function is 3 arguments which are described below based on the code above:\n\nthe response variable on which we wish to apply the function, diatoms$Diversity;\nthe categorical variable which indicates the groups we wish to separately apply the function to, diatoms$Zinc;\nthe function we are using, mean().\n\n\ntapply(\n    X = diatoms$Diversity,\n    INDEX = diatoms$Zinc,\n    FUN = mean\n)\n\n    BACK     HIGH      LOW      MED \n1.797500 1.277778 2.032500 1.717778 \n\ntapply(\n    X = diatoms$Diversity,\n    INDEX = diatoms$Zinc,\n    FUN = sd\n)\n\n     BACK      HIGH       LOW       MED \n0.4852613 0.4268717 0.4449960 0.5030104 \n\n\n\n\n\n\n\n\nTip\n\n\n\nThe tidyverse approach – Use the group_by() and summarise() functions to calculate the mean and standard deviation for each level of Zinc. The code is longer, but readable as you can see the sequence of operations.\ndiatoms %&gt;%\n    group_by(Zinc) %&gt;%\n    summarise(\n        mean = mean(Diversity),\n        sd = sd(Diversity)\n    )\n\n\n\n\nGraphical summaries\nNumerical summaries are nice, but visual summaries are often more informative and are the summary of choice for publication. They can also help us to check the assumptions of the statistical test (although sometimes this is not possible until we have fitted the model).\nIn the case of ANOVA model we are interested in the distribution of the response variable for each level of the factor. The table below gives heuristic rules about which graphical summary to use based on the number of observations.\n\n\n\n\n\nobservations\ngraphics\ncommand\n\n\n\n\n1-5\nplot raw data\nstripchart() or geom_jitter()\n\n\n6-20\nboxplot\nboxplot() or geom_boxplot()\n\n\n20 or more\nhistogram\nhist() or geom_histogram()\n\n\n\n\n\nWe will show examples of both the histogram and boxplot for the diatom data. We recommend that you use ggplot2 for your graphical summaries as it is more flexible and has a more consistent syntax than base R graphics. However, we will show both approaches here.\n\nHistogram\nAs there are less than 10 observations per group the histogram may not be very informative. However, it may still be useful for checking the normality of the data. You may be able to see the limitations of the histogram for small sample sizes.\n\n\n\n\n\n\nTip\n\n\n\nTo calculate the number of observations per group:\ntapply(\n    X = diatoms$Diversity,\n    INDEX = diatoms$Zinc,\n    FUN = length\n)\n\n\n\nggplot2base R (no loops)base R (loops)\n\n\n\nlibrary(ggplot2)\nggplot(diatoms, aes(x = Diversity)) +\n    geom_histogram(binwidth = .3) +\n    facet_wrap(~Zinc)\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nhist(\n    x = diatoms$Diversity[diatoms$Zinc == \"BACK\"],\n    main = \"Zinc: BACK\",\n    xlab = \"Diversity\"\n)\nhist(\n    x = diatoms$Diversity[diatoms$Zinc == \"HIGH\"],\n    main = \"Zinc: HIGH\",\n    xlab = \"Diversity\"\n)\nhist(\n    x = diatoms$Diversity[diatoms$Zinc == \"LOW\"],\n    main = \"Zinc: LOW\",\n    xlab = \"Diversity\"\n)\nhist(\n    x = diatoms$Diversity[diatoms$Zinc == \"MED\"],\n    main = \"Zinc: MED\",\n    xlab = \"Diversity\"\n)\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nfor (i in levels(diatoms$Zinc)) {\n    hist(\n        x = diatoms$Diversity[diatoms$Zinc == i],\n        main = paste(\"Zinc:\", i),\n        xlab = \"Diversity\"\n    )\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\nA more appropriate plot for this data is the boxplot. If we want to format the plot for publication we should make the plot clear by adding axis labels and a figure caption.\n\nbase Rggplot2\n\n\n\nboxplot(\n    Diversity ~ Zinc,\n    data = diatoms,\n    ylab = \"Diversity\",\n    xlab = \"Zinc concentration\"\n)\n\n\n\n\n\n\n\nFigure 1: Boxplot of diatom diversity by Zinc concentration.\n\n\n\n\n\n\n\n\nggplot(diatoms, aes(x = Zinc, y = Diversity)) +\n    geom_boxplot() +\n    ylab(\"Diversity\") +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\nFigure 2: Boxplot of diatom diversity by Zinc concentration.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\nWhat can you say about the the different levels of Zinc from looking at the mean and standard deviation values when running the following?\ntapply(\n    X = diatoms$Diversity,\n    INDEX = diatoms$Zinc,\n    FUN = mean\n)\n\ntapply(\n    X = diatoms$Diversity,\n    INDEX = diatoms$Zinc,\n    FUN = sd\n)\n\n\nQuestion 2\nCan you interpret and describe the boxplot above?\n\n\nModel fitting\nA 1-way ANOVA involves one treatment (or grouping) factor. The model we are fitting is:\ny_{i,j} =\\mu_i+\\epsilon_{i,j}\nwhere:\n\ny_{i,j} is the response for observation j in treatment (or group) i,\n\\mu_j is the mean of treatment (or group) i,\n\\epsilon_{i,j} is the residual term which is an independent random variable that has a mean of 0, constant variance and is normally distributed. This can be expressed shorthand as \\sim N(0,\\sigma_2). The residual MS estimates \\sigma_2.\n\nThe statistical hypotheses we are testing are:\nH_{0}: \\mu_1 = \\mu_2 = ... \\mu_t H_{1}: \\text{not all } \\mu_j \\text{ are equal}\nwhere \\mu_j is the mean diatom diversity for each level of Zn concentration.\nThe code below fits the ANOVA model using the aov() function and saves the it to an object called anova.diatoms. We can then extract the ANOVA table using the summary function.\n\nanova.diatoms &lt;- aov(formula = Diversity ~ Zinc, data = diatoms)\nsummary(anova.diatoms)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nZinc         3  2.567  0.8555   3.939 0.0176 *\nResiduals   30  6.516  0.2172                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nAssumptions and interpretation of results\nThe assumptions of the ANOVA model are:\n\nThe residuals are independent,\nThe residuals are normally distributed,\nThe residuals have constant variance.\n\nBased on the boxplots and histograms during data exploration, the assumptions of normality and equal variances are met. We will discuss assumptions again in greater detail next week when we start to look at residuals.\nWe can only interpret the results of the ANOVA model if the assumptions are met. We can report the results the following way:\n\nReporting: The results indicate that there are significant differences between the levels of Zn concentration in terms of diatom diversity (F = 3.9, df = 3, 30, P = 0.02).\n\n\n\nPost-hoc testing\nThe ANOVA test only tells us that there are differences between the groups, but it does not tell us which groups are different. We can use the emmeans package to perform post-hoc testing.\n\nlibrary(emmeans)\n\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\n\nposthoc &lt;- emmeans(anova.diatoms, \"Zinc\")\nposthoc\n\n Zinc emmean    SE df lower.CL upper.CL\n BACK   1.80 0.165 30    1.461     2.13\n HIGH   1.28 0.155 30    0.961     1.60\n LOW    2.03 0.165 30    1.696     2.37\n MED    1.72 0.155 30    1.401     2.04\n\nConfidence level used: 0.95 \n\n\nThe output of the emmeans function gives us the estimated marginal means for each level of Zinc and the 95% confidence intervals. The confidence intervals for the HIGH level of Zn concentration do not overlap with the other levels of Zn concentration, indicating that the HIGH level of Zn concentration is significantly different from the other levels.\n\nReporting: The post-hoc test indicates that the HIGH level of Zn concentration is significantly different from the other levels of Zn concentration. There are no other significant differences.\n\nWe can visualise the results of the post-hoc testing using the plot function, if you prefer a visual representation.\n\nplot(posthoc)"
  },
  {
    "objectID": "labs/Lab03/Lab03-oneway-anova.html#exercise-2-chicks",
    "href": "labs/Lab03/Lab03-oneway-anova.html#exercise-2-chicks",
    "title": "Lab 03",
    "section": "Exercise 2 – chicks",
    "text": "Exercise 2 – chicks\nAn experiment was designed to compare 15-day mean comb weights (g) of two lots of male chicks, one receiving sex hormone A (testosterone), the other C (dehydroandrosterone). While we usually analyse these data by using a (pooled) two-sample t-test, a single factor analysis of variance approach could be used (with two levels of the treatment factor). We will compare the results from both analyses.\nThe data is found in the Comb worksheet of the chick_marigold.xlsx file. Download it below:\nRead the data and save it as comb. It should look like below:\n\ncomb\n\n# A tibble: 22 × 2\n   CombWt Hormone\n    &lt;dbl&gt; &lt;chr&gt;  \n 1     57 A      \n 2    120 A      \n 3    101 A      \n 4    137 A      \n 5    119 A      \n 6    117 A      \n 7    104 A      \n 8     73 A      \n 9     53 A      \n10     68 A      \n# ℹ 12 more rows\n\n\n\nQuestion 3\nPerform some checks to verify the two-sample t-test (or one-way ANOVA) is appropriate, i.e. investigate the shape of the distributions, and the standard deviations.\n\n\nQuestion 4\nIn R, perform a 2-sample t-test using the t.test() function, and use the var.equal = TRUE argument to ensure a standard 2-sample t-test is performed. Interpret the output.\n\n\nQuestion 5\nNext, perform a one-way ANOVA using the aov function, followed by summary, and interpret the results.\n\n\nQuestion 6\nCompare the t-test and ANOVA outputs. What do you notice about:\n\nthe degrees of freedom;\nthe P-value?\n\n\n\nQuestion 7\nWhen there are only two treatment groups, the observed F and t values are related by F = t^2. Demonstrate this for the observed values in this exercise."
  },
  {
    "objectID": "labs/Lab03/Lab03-oneway-anova.html#exercise-3-lambs",
    "href": "labs/Lab03/Lab03-oneway-anova.html#exercise-3-lambs",
    "title": "Lab 03",
    "section": "Exercise 3 – lambs",
    "text": "Exercise 3 – lambs\nWork on this exercise in your own time.\nThe levels of immunoglobulin (Ig) in blood serum (g/100 ml) in 3 breeds of newborn lambs have been investigated. A total of 44 lambs were sampled, with approximately equal numbers per breed. The researcher wants to know whether or not there are significant differences in immunoglobulin levels between the breeds.\nThe data is found in lambs.csv. Download the data if you have not done so:\nRead the data into R and save it as lambs. Use the read_csv() function which should be available when you load the tidyverse package.\nIt should look like below:\n\nlambs\n\n# A tibble: 44 × 2\n      Ig Breed\n   &lt;dbl&gt; &lt;dbl&gt;\n 1   1.1     1\n 2   2.2     1\n 3   1.7     1\n 4   1.4     1\n 5   1.6     1\n 6   2.3     1\n 7   1.4     1\n 8   1.9     1\n 9   0.8     1\n10   1.6     1\n# ℹ 34 more rows\n\n\nBased on the demonstrator walkthrough, analyse the lambs dataset and test the hypothesis that:\nH_{0}: \\mu_1 = \\mu_2 = \\mu_3 H_{1}: \\text{not all } \\mu_j \\text{ are equal}\nwhere \\mu_j is the mean Ig level for breed j.\nRemember to:\n\nState your null and alternate hypotheses.\nPlot an appropriate summary graph for the data.\nDemonstrate the model fit using the aov() function.\nTest assumptions, by checking sd values, or by looking at your exploratory plots (checking residuals is not necessary at this point).\nReport your test statistic, degrees of freedom and p-value.\nReport the statistical conclusion by addressing the null hypothesis.\nExplain the results within a biological context to the data."
  },
  {
    "objectID": "labs/Lab03/Lab03-oneway-anova.html#thanks",
    "href": "labs/Lab03/Lab03-oneway-anova.html#thanks",
    "title": "Lab 03",
    "section": "Thanks!",
    "text": "Thanks!\nDid you know you can also knit to PDF? Check the documentation for R Markdown or Quarto for more information.\n\nAttribution\nThis lab was developed using resources that are available under a Creative Commons Attribution 4.0 International license, made available on the SOLES Open Educational Resources repository.\n\nClick here for session information\n\n\n\n\nsessionInfo()\n\nR version 4.5.2 (2025-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Tahoe 26.2\n\nMatrix products: default\nBLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] C.UTF-8/C.UTF-8/C.UTF-8/C/C.UTF-8/C.UTF-8\n\ntime zone: Australia/Sydney\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] emmeans_2.0.0 ggplot2_4.0.2 readxl_1.4.5 \n\nloaded via a namespace (and not attached):\n [1] bit_4.6.0          rematch_2.0.0      gtable_0.3.6       jsonlite_2.0.0    \n [5] crayon_1.5.3       dplyr_1.2.0        compiler_4.5.2     tidyselect_1.2.1  \n [9] parallel_4.5.2     scales_1.4.0       yaml_2.3.12        fastmap_1.2.0     \n[13] lattice_0.22-7     coda_0.19-4.1      readr_2.1.6        R6_2.6.1          \n[17] labeling_0.4.3     generics_0.1.4     knitr_1.51         htmlwidgets_1.6.4 \n[21] tibble_3.3.1       tzdb_0.5.0         pillar_1.11.1      RColorBrewer_1.1-3\n[25] rlang_1.1.7        utf8_1.2.6         xfun_0.56          S7_0.2.1          \n[29] bit64_4.6.0-1      otel_0.2.0         estimability_1.5.1 cli_3.6.5         \n[33] withr_3.0.2        magrittr_2.0.4     digest_0.6.39      grid_4.5.2        \n[37] vroom_1.6.7        xtable_1.8-4       mvtnorm_1.3-3      hms_1.1.4         \n[41] lifecycle_1.0.5    vctrs_0.7.1        evaluate_1.0.5     glue_1.8.0        \n[45] farver_2.1.2       cellranger_1.1.0   rmarkdown_2.30     tools_4.5.2       \n[49] pkgconfig_2.0.3    htmltools_0.5.9"
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html",
    "href": "labs/Lab01/Lab01-introduction.html",
    "title": "Lab 01",
    "section": "",
    "text": "In this lab, we will learn how to:\n\nExplain the differences between (i) samples and populations (ii) standard error and standard deviation;\nUse R to perform basic data analysis tasks related to exploratory data analysis\nPresent their code and results using RMarkdown."
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#learning-outcomes",
    "href": "labs/Lab01/Lab01-introduction.html#learning-outcomes",
    "title": "Lab 01",
    "section": "",
    "text": "In this lab, we will learn how to:\n\nExplain the differences between (i) samples and populations (ii) standard error and standard deviation;\nUse R to perform basic data analysis tasks related to exploratory data analysis\nPresent their code and results using RMarkdown."
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#specific-goals",
    "href": "labs/Lab01/Lab01-introduction.html#specific-goals",
    "title": "Lab 01",
    "section": "Specific goals",
    "text": "Specific goals\nBy the end of this lab, you should be able to:\n\nCalculate means, medians, and standard deviations\nCreate graphs using ggplot2\nSubset and organise data\nUnderstand why different types of summary statistics exist, and when to use each one"
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#hi",
    "href": "labs/Lab01/Lab01-introduction.html#hi",
    "title": "Lab 01",
    "section": "Hi!",
    "text": "Hi!\nWelcome to the first lab for ENVX2001! Before we start, make sure you have access to the latest versions of R and RStudio."
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#preparation",
    "href": "labs/Lab01/Lab01-introduction.html#preparation",
    "title": "Lab 01",
    "section": "Preparation",
    "text": "Preparation\nIf you are attending this lab in person, your demonstrators will treat you to a short presentation. If you are completing this lab remotely, please do the following:\n\nDownload these files: water.xlsx, browsing_data_2003_2020_2.csv\nRead the abstract of this article: MacNulty et al., 2025\n\n\nA Note on Generative AI (GenAI)\nGenAI is a powerful tool that can help you learn and understand the concepts we cover in ENVX2001. However, for the first six weeks of this course, we ask that you please refrain from asking GenAI for help.\nThere are two crucial skills we want to help you develop this semester:\n\nProblem solving tenacity\nStatistical intuition\n\nIn our experience, these skills are best learned without the help of AI. In weeks 7-12, when we introduce more complex statistical concepts, is where GenAI can really help.\n\n\n\n\n\n\n\n\n\nAspen grove in Yellowstone National Park, from Wikimedia Commons (2008), by John Fowler.\n\n\nFirst Thoughts\nTo recap the story from our presentation (or article, for remote students), a study by Ripple et al. (2025) found new evidence to support the already popular idea that wolves are a keystone species in Yellowstone National Park. By modelling the rate of willow regrowth before and after wolves were reintroduced to the park, Ripple and his team found that the wolves had an enormous, positive effect on the park’s ecosystem (Ripple et al. 2025).\n\nRipple, William J., Robert L. Beschta, Christopher Wolf, Luke E. Painter, and Aaron J. Wirsing. 2025. “The Strength of the Yellowstone Trophic Cascade After Wolf Reintroduction.” Global Ecology and Conservation 58. https://doi.org/10.1016/j.gecco.2025.e03428.\n\nMacNulty, Daniel R., David Cooper, Mike Procko, and Timothy J. Clark-Wolf. 2025. “Flawed Analysis Invalidates Claim of a Strong Yellowstone Trophic Cascade After Wolf Reintroduction: A Comment on Ripple Et Al. (2025).” Global Ecology and Conservation 63. https://doi.org/10.1016/j.gecco.2025.e03899.\nHowever, Ripple’s study was criticized by another group of scientists; Daniel MacNulty and his team argued that Ripple’s methods were flawed, and that while the wolves of Yellowstone National Park may have caused a weak trophic cascade in some areas of the park, this effect was not nearly as strong or as universal as Ripple had claimed (MacNulty et al. 2025).\n\n\n\n\n\n\n\n\n\nGrey wolf in Yellowstone National Park, from Wikimedia Commons (2013), by Mike van Dalen.\n\n\n\n\n\n\nNoteFor you to consider\n\n\n\nBased on what you saw in the presentation, or read from the article, what are your first thoughts on the situation? Do you think MacNulty was right to criticize Ripple’s methods?"
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#part-1-organising-data",
    "href": "labs/Lab01/Lab01-introduction.html#part-1-organising-data",
    "title": "Lab 01",
    "section": "Part 1: Organising data",
    "text": "Part 1: Organising data\n\nExercise: Blue Sea Stars\nThe following exercise involves a fabricated story and simulated data\n\n\n\n\n\n\n\n\n\nBlue sea star (Linckia laevigata), from Wikimedia Commons (2017), by João D’Andretta.\nA marine scientist (we’ll call her Stella) is studying benthic invertebrates on Lady Elliot Island, and notices that the blue sea stars (Linckia laevigata) from this island seem to be smaller than those in other parts of the Great Barrier Reef. She wonders if her eyes are deceiving her.\nTo get to the bottom of this, she collects 16 sea stars from Lady Elliot Island and measures one random arm from each of them to the nearest 0.1 cm. She knows that the typical length of a blue sea star’s arm is around 11.5 cm (Thomson and Thompson 1982).\n\nThomson, G., and C. Thompson. 1982. “Movement and Size Structure in a Population of the Blue Starfish Linckia Laevigata (L.) at Lizard Island, Great Barrier Reef.” Marine and Freshwater Research 33 (3): 561. https://doi.org/10.1071/mf9820561.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nFind out where Lady Elliot Island is located on a map. Why might the sea stars there be different in size to sea stars from other parts of the Great Barrier Reef? It is always a good idea to think about the context behind our data before we analyse it.\n\n\nHere is the data Stella gathers:\n\n\n\n\n\n\nNoteStella’s data\n\n\n\n\n\n\nstars &lt;- c(\n  10.3, 11.0, 10.5, 10.0, 11.3, 14.5, 13.0, 12.1, 12.1,\n  9.4, 11.3, 12.0, 11.5, 9.3, 10.1, 7.6\n)\n\n\n\n\nNotice that we use the c() function to specify a data vector. A vector is a collection of similar objects; in this case, numbers.\nTo make it easier to recall this vector, we can give it the name ‘stars’ using the &lt;- symbol. Now, if we ever want to recall this list of numbers again, we can do so easily:\n\n\n\n\n\n\nNoteRecall Stella’s data\n\n\n\n\n\n\nstars # recalls Stella's data\n\n [1] 10.3 11.0 10.5 10.0 11.3 14.5 13.0 12.1 12.1  9.4 11.3 12.0 11.5  9.3 10.1\n[16]  7.6\n\n\n\n\n\nThe # symbol is used to make a comment. Comments are very useful, and you should get into the habit of including them in your code.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nMake a code chunk. You can do this using the +C button on the top of your screen (ask a demonstrator for help if you are confused).\nType in the following lines of code:\nmean(c(1,2,3,4,5))\nmedian(c(0,0,1,45,459,2,49,1))\nAnd describe what each of them do using comments.\n\n\nTo find the average arm length of Stella’s sea stars, we can use the mean() function:\n\nmean(stars) # Notice that instead of re-typing our data, we can recall it using the name we gave it earlier: 'stars'.\n\n[1] 11\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe average is not the only summary statistic we can calculate; here are some others:\n\nmedian(stars) # Median - the middle number in Stella's data\n\n[1] 11.15\n\nsd(stars) # Standard deviation - the spread of Stella's data\n\n[1] 1.626038\n\n\n\n\n\nThe summary() function can give you many different summary statistics at once:\n\nsummary(stars) # Also gives you the 1st and 3rd quartiles, minimum value, and maximum value\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   7.60   10.07   11.15   11.00   12.03   14.50 \n\n\nThat’s a lot of functions to remember! Don’t worry, you can always ask R for help. Use the ? symbol. For example, to find out more about the mean() function, you can type ?mean into your console (your console is located at the bottom of your screen).\n\n\n\n\n\n\n\n\n\nBlack noddy (Anous minutus), a seabird found on the palm trees of Lady Elliot Island. From Wikimedia Commons (2007), by Dalgo UK\n\n\nPractice: Even More Sea Stars\nStella tells her friends about her study, and they all decide to visit Lady Elliot Island to help her collect more samples. Here are the datasets that each of Stella’s friends collects; we will name them stars_1, stars_2, etc. :\n\n\n\n\n\n\nNoteData collected by Stella’s friends\n\n\n\n\n\n\nstars_1 &lt;- c(\n  11.3, 15.0, 9.5, 10.0, 11.0, 11.2, 12.2, 8.5, 9.1, 9.5,\n  11.4, 12.4, 13.0, 8.3, 11.0, 12.5\n)\nstars_2 &lt;- c(\n  14.0, 11.5, 6.5, 9.1, 9.3, 15.0, 11.0, 9.2, 12.7, 8.5,\n  11.8, 8.8, 8.3, 9.1, 11.6, 14.0\n)\nstars_3 &lt;- c(\n  9.5, 12.3, 13.6, 8.2, 15.8, 7.7, 10.1, 11.3, 11.5, 12.9,\n  10.1, 8.3, 7.5, 8.9, 9.1, 10.0\n)\nstars_4 &lt;- c(\n  10.0, 12.1, 16.0, 8.0, 11.3, 14.0, 12.0, 13.5, 10.1,\n  10.5, 10.8, 9.1, 14.3, 9.0, 15.5, 8.5\n)\nstars_5 &lt;- c(\n  7.0, 8.5, 10.5, 7.1, 11.3, 9.0, 9.5, 12.1, 8.0, 9.3,\n  10.9, 7.3, 8.5, 9.0, 8.1, 12.4\n)\n\n\n\n\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nFind the mean and standard deviation for each of these datasets.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe can apply the mean() and sd() functions to each dataset individually:\n\nmean(stars_1) # mean of stars_1\n\n[1] 10.99375\n\nsd(stars_1) # standard deviation of stars_1\n\n[1] 1.799803\n\nmean(stars_2) # mean of stars_2\n\n[1] 10.65\n\nsd(stars_2) # standard deviation of stars_2\n\n[1] 2.424321\n\nmean(stars_3) # mean of stars_3\n\n[1] 10.425\n\nsd(stars_3) # standard deviation of stars_3\n\n[1] 2.328233\n\nmean(stars_4) # mean of stars_4\n\n[1] 11.54375\n\nsd(stars_4) # standard deviation of stars_4\n\n[1] 2.502257\n\nmean(stars_5) # mean of stars_5\n\n[1] 9.28125\n\nsd(stars_5) # standard deviation of stars_5\n\n[1] 1.716671\n\n\nWhen you have many datasets, repeating this process can become tedious. We will show you shortcuts and workarounds in the coming weeks to make these types of tasks easier.\n\n\n\nWhat do we have here? Another collection of numbers? Let’s make a vector out of them!\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nMake a vector that contains the means of each of Stella and her friends’ datasets.\nName this vector stars_means.\n\n\n\n\n\n\nTip\n\n\n\nYour vector should have 6 entries - one for the mean of Stella’s own dataset stars, and one for the means of each of her friends’ datasets stars_1, stars_2, etc.\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe can use the c() function to create our vector:\n\nstars_means &lt;- c(\n  mean(stars_1), mean(stars_2), mean(stars_3),\n  mean(stars_4), mean(stars_5), mean(stars)\n)\n# The entries of this vector might look strange, but they are really just individual numbers; mean(stars_1) is a number, and so is mean(stars_2), etc.\n\nWhenever you want to store a collection of numbers, you can make them into a vector. These vectors will stay under R’s ‘environment’ tab (at the top right of your screen).\n\n\n\nLet’s see what our new vector looks like:\n\nstars_means # A vector with 6 entries.\n\n[1] 10.99375 10.65000 10.42500 11.54375  9.28125 11.00000\n\n\nWe have just created a brand new dataset out of six pre-existing ones. Let’s find out more about this new dataset - what is its mean and standard deviation?\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nCalculate the mean and standard deviation of stars_means. How do these values compare to the mean and standard deviation of Stella’s original dataset, stars?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nBecause stars_means is a vector, we can apply the mean() and sd() functions to it:\n\nmean(stars_means) # The average value of stars_means\n\n[1] 10.64896\n\nsd(stars_means) # The standard deviation of stars_means\n\n[1] 0.7698764\n\n\nNotice that stars_means has a very similar average to stars (10.6 vs 11), but a much smaller standard deviation (0.77 vs 1.62).\n\n\n\nThe more friends Stella invites, and the more samples they gather, the smaller the standard deviation of stars_means will become.\nThe mean of Stella’s original dataset, stars, is called the sample mean, and the standard deviation of stars is called the sample standard deviation.\nThe mean of our new dataset, stars_means, is called the average of the sample means, and the standard deviation of stars_means is an estimate of the standard error in Stella’s data.\nDepending on how many friends Stella has, and how many sea stars each of them measures, the average of the sample means may serve as a good estimate of the population mean (i.e. the true average arm length of all the blue sea stars on Lady Elliot Island, if we could measure each and every one of them). If Stella had hundreds of friends, stars_means would have hundreds of entries, and its mean would approach the true population mean while its standard deviation approaches the true standard error.\nStella was very lucky. In reality, we won’t always have hundreds of friends (or even five) to help us collect additional data. Because of this, we often need to approximate the population mean and standard error based on a limited number of samples. We can do this using the following equations:\n \\bar{X} \\approx \\mu  SE \\approx \\frac{s}{\\sqrt{n}}\nWhere \\bar{X} is the sample mean, \\mu is the population mean, SE is the true standard error, s is the sample standard deviation, and n is the sample size (how many sea stars Stella measured).\n\n\nSection Summary\nSo, what did Stella find? Were the blue sea stars of Lady Elliot Island really smaller than blue sea stars elsewhere? To answer that question, we need to carry out a statistical test.\nIf you already know how to perform a one-sample t-test, or a one-way ANOVA, give it a try. Otherwise, we will go through how to run both of these tests next week. Then, you can revisit this lab and apply one of them to Stella’s data.\nBefore we move onto the next section, now is a good time to take a 5-minute break."
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#part-2-making-graphs",
    "href": "labs/Lab01/Lab01-introduction.html#part-2-making-graphs",
    "title": "Lab 01",
    "section": "Part 2: Making Graphs",
    "text": "Part 2: Making Graphs\n\nExercise: Water Chemistry\nThe following exercise involves real data from Lovett, Weathers, and Sobczak (2000)\nLovett, Gary M., Kathleen C. Weathers, and William V. Sobczak. 2000. “Nitrogen Saturation and Retention in Forested Watersheds of the Catskill Mountains, New York.” Ecological Applications 10: 73–84.\n\n\n\n\n\n\n\n\n\n\nDiamond Notch Falls in Westkill Mountain, one of the many mountains in New York’s Catskill Park. From Wikimedia Commons (2021), by Daniel Case.\nIn the year 2000, Gary Lovett and his research team measured water chemistry in the streams of the Catskill Mountains. They were concerned that growing levels of industrial activity in the area may affect surrounding forests, and they needed a way to keep track of pollutant levels in the environment.\nFor this exercise, we will focus on sulphates. It is worth noting that Lovett’s original study focused on nitrates instead.\n\nReading data\nUnlike Stella’s data, which we could type directly into R, Lovett’s data is stored in a separate Excel file. We need to load this file into R before we can analyse the data inside.\nTo do this, we need to install a package. Packages are add-ons you can download from the internet, kind of like expansion packs in a video game.\nThe package we want to install is called “readxl”. We can do this using the install.packages() function.\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe code we need to execute is: install.packages(\"readxl\"). However, we do not want to put this line into our coding script. Instead, we want to put it into our console. The console is the window at the bottom of the screen, where you may see lines of blue code that you previously executed.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways execute one-time operations, such as installing packages and inspecting file paths, inside your console. If you include these tasks in your coding script, they will execute every time you render the document. Not only will this slow down the rendering process, it will also result in a messy html file with irrelevant outputs.\n\n\nNow, we can use this brand new package to read our excel file:\n\nlibrary(readxl) # Activates the package 'readxl'\nwater &lt;- read_excel(\"data/water.xlsx\") # Reads the file 'water.xlsx' into R as a table\n\n# Note that we renamed this table 'water' using the &lt;- operator.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIn the function read_excel(\"data/water.xlsx\"), the data/ part tells R which folder to search. If you do not have a folder called “data” on your computer, then you should run read_excel(\"water.xlsx\") instead. To manually reset R’s search location (also called its directory), go to the very top of your screen and find “Session -&gt; Set Working Directory -&gt; Choose Directory…”.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA common error you may encounter is “no such file or directory”. This either means you have misspelled your file name (remember to include .xlsx and use ““), or that R is searching for your file in the wrong folder. To check which folder R is searching, run the command getwd() in your console.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nA good way to organise your data is to keep it close to your script. Wherever you save your qmd. file, make sure your data is also saved in the same folder.\n\n\n\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nAfter reading the data, it is good practice to verify that everything has been imported correctly. Use the str() function to check the structure of your new dataset, water.\n\n\n\n\nSubsetting data\nSubsetting means keeping some parts of your data while excluding others. For example, we may want to keep a specific column, or remove a specific row.\nThe easiest way to subset data is to use the [,] operator. You can use the space in front of the comma [*,] to select rows, and the space after the comma [,*] to select columns.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\n\nUse the [,] operator to select the third row and first column of the water dataset.\nUse the [,] operator to select the ninth row of the water dataset.\nUse the [,] operator to select the first column of the water dataset.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can use the operator $ instead of [,] to select specific columns by name. For example:\n\nwater$SO4 # Selects the column named 'SO4'\n\n [1] 50.6 55.4 56.5 57.5 58.3 63.0 66.5 64.5 63.4 58.4 70.6 56.9 56.7 56.0 60.4\n[16] 67.8 70.8 58.6 59.5 55.5 63.4 57.8 55.1 65.5 62.7 72.1 63.4 68.5 65.8 69.2\n[31] 66.7 59.3 61.1 62.1 70.4 62.1 64.6 61.4 56.9\n\n\nNotice that we went from a table to a collection of numbers (recall that a collection of numbers is called a numerical vector).\nYou can apply functions to these numbers, as you would to any other numerical vector. For example, let’s find out their median value:\n\nmedian(water$SO4) # The median value of column SO4\n\n[1] 62.1\n\n\n\n\n\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nSummary statistics are a great way to quickly make sense of your data.\nUse the summary() function on column SO4. Do you think the values in this column are symmetrically distributed?\n\n\n\n\n\n\n\n\n\n\n\nCatskill Mountains in the fall. From Wikimedia Commons (2016), by Daniel Case.\n\n\n\nPractice: Now You See Me\nGraphs are a great way to visualise data. The ggplot2 package is a popular package made for this very purpose. It is based on the grammar of graphics (as if the grammar of English was not enough), which you can look into in your own time.\nLet’s activate this package:\n\nlibrary(ggplot2)\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf R cannot find ggplot2 in your library, you may not have installed ggplot2 before. In that case, run install.packages(\"ggplot2\") in your console to install it.\nEven better, install the package “tidyverse”. This package includes ggplot2, as well as many other useful packages such as dplyr.\n\n\n\nOf course, there are other ways to create graphs in R; but we recommend using ggplot2 because it is flexible and intuitive.\n\nBasically Yo-Chi\nggplot2 is very similar to Yo-Chi. Really, it is. What is the first thing you do at a Yo-Chi? You grab a cup. Let’s grab a cup:\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nThe basic template for ggplot2 is:\nggplot(data = _, aes(x = _, y = _)) + geom_().\nThink of this as an empty cup. You can put things into it.\nThe argument data = tells R which dataset to reference (in this case, we will use water).\nThe x = and y = arguments specify which column we want to use as our x values, and which column we want to use as our y values. Play around with different columns here, and see what you find.\nThe argument geom_() tells R the type of graph you want. Here are some options you can try: geom_boxplot(), geom_line(), geom_point(), geom_smooth(), geom_jitter().\nMake a few different graphs from the water dataset, and pick your favourite one!\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThese are the graphs we made; just plain yoghurt (signature tart) for now:\n\nggplot(data = water, \n       aes(x = Creek_Formally_Named, y = SO4)) +\n  geom_boxplot() +\n  theme_classic() # Box plot (we added a 'classic' theme to erase the grey background)\n\n\n\n\n\n\n\nggplot(data = water, \n       aes(x = NO3, y = SO4)) +\n  geom_line() +\n  theme_classic() # Line plot (cool, but hard to interpret!)\n\n\n\n\n\n\n\nggplot(data = water, \n       aes(x = NO3, y = SO4)) +\n  geom_point() +\n  theme_classic() # Scatter plot (good for plotting two continuous variables against each other)\n\n\n\n\n\n\n\n\nDid you come up with something different?\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo keep your code neat, hit ‘enter’ after each + sign. This starts a new, indented line and prevents overcrowding.\n\n\n\n\n\nChoose your flavour\nWe can customise our Yo- I mean our graphs by colour-coding them. To do this, we take our basic template:\nggplot(data = _, aes(x = _, y = _)) + geom_()\nAnd add the arguments colour = and fill = into the geom_() bracket, like this:\nggplot(data = _, aes(x = _, y = _)) + geom_(colour = _, fill = _)\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nTake your favourite graph from before, and turn it into a different colour. Simple options you can try include: colour = 'red',colour = 'lightblue', fill = 'grey', etc. Name a colour, and it probably exists. For any other colours, look up their HEX codes.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nOur box plot, turned blue.\n\nggplot(data = water, \n       aes(x = Creek_Formally_Named, y = SO4)) +\n  geom_boxplot(colour = 'black', fill = 'lightblue') +\n  theme_classic() \n\n\n\n\n\n\n\n\n\n\n\nOut of all the graphs that ggplot2 offers, histograms and bar plots are kind of special. This is because they do not take a y-argument; in fact, the y-argument for both of these graphs is by default ‘count’. (If you are confused about why this is the case, reach out to one of your demonstrators.)\nLet’s practice making a histogram:\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nMake a histogram of the SO4 column in water. Use geom_histogram() to create a histogram, and use the argument geom_histogram(binwidth = _) to adjust its appearance.\nEarlier, we guessed from our summary statistics that the values in column SO4 are symmetrically distributed. Is that the case?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nHere is our histogram:\n\nggplot(data = water, aes(x = SO4)) +\n  geom_histogram(colour = 'black', fill = 'lightblue',\n                 binwidth = 3) +\n  theme_classic() \n\n\n\n\n\n\n\n\nThe distribution does look reasonably symmetrical.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe table below contains heuristic guidelines on which graphical summary to use based on the number of observations. Commands refer to arguments in ggplot2, not base R.\n\n\n\n\n\nobservations\ngraphics\ncommand\n\n\n\n\n1-5\nplot raw data\ngeom_point()\n\n\n6-20\nboxplot\ngeom_boxplot()\n\n\n20 or more\nhistogram\ngeom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you prefer a more formal way to detect skewness, you may be tempted to use the skewness() function from the moments package. This function calculates the skewness coefficient of a dataset or vector.\n\nlibrary(moments) # Install this package if you haven't already by running: `install.packages(\"moments\")` in your console\nskewness(water$SO4) # Calculates the skewness coefficient of the SO4 column\n\n[1] 0.1571807\n\n\nA low skewness coefficient (anything &lt; 0.5 is usually considered low) means our distribution should be reasonably symmetrical.\nHowever, skewness coefficients become hard to interpret in the case of multi-modal distributions. In general, we recommend sticking to histograms.\n\n\n\n\nAdd toppings\nWe have our cup, we’ve chosen our flavours, now it’s time to add our toppings.\nYou can add a theme to your graph using the theme_() argument, like this:\nggplot(...) + geom_(...) + theme_()\nYou can also change the axis labels using the labs() argument, like this:\nggplot(...) + .geom_(...) + theme_() + labs(title = _, x = _, y = _)\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nTake your histogram from earlier and re-label its x-axis. Lovett’s team measured sulphate concentration in micromoles per litre.\nChoose a theme for your graph as well. Some cool themes to try out are: theme_classic(), theme_minimal(), theme_bw(), theme_dark().\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nTo change our x-axis label, we should use the argument labs(x = _). We can leave y_ and title_ arguments out, since we are not interested in changing the y-axis label or the title.\n\nggplot(data = water, aes(x = SO4)) +\n  geom_histogram(colour = 'black', fill = 'lightblue',\n                 binwidth = 3) +\n  theme_classic() +\n  labs(x = 'sulphate concentration (micromoles per litre)')\n\n\n\n\n\n\n\n\nFor theme, I stuck with theme_classic() - a personal favourite.\n\n\n\n\n\n\n\n\n\n\n\n\nA freshwater snail (Lymnaea stagnalis) in algae. From Wikimedia Commons (2009), by Peter Pfeiffer.\n\n\n\nSection Summary\nAccording to a study in Finland, it takes upwards of 500 micromoles/litre of sulphate to cause noticeable harm to aquatic crustaceans and molluscs (Karjalainen et al. 2023). The values from Lovett’s study were far below this (check our histograms from earlier). So, it might seem like the freshwater ecosystems of Catskill Park are safe… for now.\n\nKarjalainen, Juha, Xiaoyun Hu, Mikko Mäkinen, Anna K. Karjalainen, Johannes Järvistö, Kaisa Järvenpää, Mika Sepponen, and Matti T. Leppänen. 2023. “Sulfate Sensitivity of Aquatic Organisms in Soft Freshwaters Explored by Toxicity Tests and Species Sensitivity Distribution.” Ecotoxicology and Environmental Safety 258: 114984. https://doi.org/10.1016/j.ecoenv.2023.114984.\nHowever, we must take note of two important thing: Firstly, harmful pollutant levels can be very ecosystem-specific. It is entirely possible for one ecosystems to be more sensitive than another to the same level of sulphate pollution. Secondly, sulphate was not the only pollutant Lovett’s team measured. In fact, their main concern was nitrate saturation (NO3).\nWe will leave it to you to figure out whether nitrate concentrations in the creeks of Catskill Park were above or below environmentally accepted levels at the time of Lovett’s study.\nFor now, let’s take a 5-minute break, and then we will revisit the Yellowstone controversy one last time."
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#part-3-handling-complexity",
    "href": "labs/Lab01/Lab01-introduction.html#part-3-handling-complexity",
    "title": "Lab 01",
    "section": "Part 3: Handling Complexity",
    "text": "Part 3: Handling Complexity\n\nCase Study: Back to Yellowstone\nThe following case study involves real data from Hobbs et al. (2024), which was used by Ripple et al. (2025) in their analysis\nHobbs, N. Thompson, Daniel B. Johnston, Kristin N. Marshall, Erik C. Wolf, and David J. Cooper. 2024. “Does Restoring Apex Predators to Food Webs Restore Ecosystems? Large Carnivores in Yellowstone as a Model System.” Ecological Monographs 94 (2). https://doi.org/10.1002/ecm.1598.\n\n\n\n\n\n\n\n\n\n\nLion geyser and Heart Spring in Yellowstone National Park. From Wikimedia Commons (2008), by Brocken Inaglory.\n\nRead in data\nThe data from Hobbs et al. (2024) comes as a csv file - which means read_xlsx will not work this time, so we need to install the readr package instead. Remember to do this in your console, not your coding script.\nOnce you have installed readr, we can summon it from the R library.\n\nlibrary(readr) # activates the readr package\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nJust like ggplot2,readr is also included in the tidyverse package.\n\n\n\nNow, we can read in our data using the function read.csv():\n\nYellowstone &lt;- read.csv('data/browsing_data_2003_2020_2.csv') # we will name this dataset 'Yellowstone'\n\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nCheck the structure of the Yellowstone dataset.\n\n\nThis dataset is the original one produced by Hobbs and his research team in 2024 from 21 control sites and 16 experimental sites.\nHowever, when Ripple’s team re-analysed the same dataset one year later, they did not include all 37 sites. Instead, for unknown reasons, they only chose 4 out of 16 experimental sites to study.\nOne of the major criticisms leveled at Ripple by MacNulty et al. (2025) was that such an odd choice of study sites jeopardised the validity of the rest of the study.\nTo see why MacNulty thought this, let’s try to replicate Ripple’s study design with our own dataset.\nFirst, we have to remove all the sites in our dataset that Ripple excluded from his study. The code for this is a little bit tricky, so we will go through it step-by-step.\nHere is a list of all the sites that Ripple excluded:\n\n\n\n\n\n\nNoteSites that Ripple excluded\n\n\n\n\n\n\nsites_excluded &lt;- c(\n  'wb-dx','wb-dc','wb-cx',\n  'elk-dx','elk-dc','elk-cx',\n  'eb2-dx','eb2-dc','eb2-cx',\n  'eb1-dx','eb1-dc','eb1-cx'\n) # Notice that this is a vector. We are used to seeing vectors with numbers by now, but we are also allowed to make vectors with characters.\n\n\n\n\nThe challenge is to remove all of these sites from our dataset.\nWe can turn to the [,] function for this. Remember that to remove rows from our dataset, we need to specify them in front of the comma [*,].\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\n\nSelect the first row of the Yellowstone dataset.\nSelect the first five rows of the Yellowstone dataset.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo select rows by name, first use $ to specify the column that lists all the site names, then use == to match a specific name. For example, to select all the rows from the site “crescent-obs”:\n\nhead(Yellowstone[Yellowstone$site_full ==\"crescent-obs\",])\n\n     site_full willid year     willid_full  site_id treat exp plantht N_shoots\n1 crescent-obs     86 2010 crescent-obs-86 crescent   obs   0      49       95\n2 crescent-obs     86 2014 crescent-obs-86 crescent   obs   0      65       84\n3 crescent-obs     86 2013 crescent-obs-86 crescent   obs   0      66      134\n4 crescent-obs     86 2011 crescent-obs-86 crescent   obs   0      45      170\n5 crescent-obs     86 2009 crescent-obs-86 crescent   obs   0      42       54\n6 crescent-obs     86 2017 crescent-obs-86 crescent   obs   0      56       75\n  N_browsed N_unbrowsed N_deep_browsed  p_browsed p_deep_browsed fence dam\n1        60          33              2 0.63157895     0.02105263     0   0\n2         3          76              5 0.03571429     0.05952381     0   0\n3        74          55              5 0.55223881     0.03731343     0   0\n4        43         120              7 0.25294118     0.04117647     0   0\n5        29          25              0 0.53703704     0.00000000     0   0\n6        14          54              7 0.18666667     0.09333333     0   0\n  browse n.plants n.years min_yr max_yr\n1      1        9      10   2009   2018\n2      1        9      10   2009   2018\n3      1        9      10   2009   2018\n4      1        9      10   2009   2018\n5      1        9      10   2009   2018\n6      1        9      10   2009   2018\n\n\nWe first have to specify Yellowstone$site_full, because site_full is the column that lists all the site names. Then, we use == to match the name crescent-obs.\nThe head argument at the beginning is just to prevent R from printing a long table. You can omit it if you want to see the full list of results.\n\n\n\nNow, we do a little bit of coding magic and invoke the %in% function to pick out multiple row names at once.\n\n\n\n\n\n\nNoteSelecting all excluded sites\n\n\n\n\n\n\nhead(Yellowstone[Yellowstone$site_full %in% sites_excluded,])\n\n    site_full willid year willid_full site_id treat exp plantht N_shoots\n191    eb1-cx    637 2011  eb1-cx-637     eb1    cx   1     146      139\n192    eb1-cx    637 2013  eb1-cx-637     eb1    cx   1     162      166\n193    eb1-cx    637 2017  eb1-cx-637     eb1    cx   1     167       63\n194    eb1-cx    637 2010  eb1-cx-637     eb1    cx   1     165       84\n195    eb1-cx    637 2012  eb1-cx-637     eb1    cx   1     120      238\n196    eb1-cx    637 2018  eb1-cx-637     eb1    cx   1     195       41\n    N_browsed N_unbrowsed N_deep_browsed p_browsed p_deep_browsed fence dam\n191         0         139              0         0              0     1   0\n192         0         166              0         0              0     1   0\n193         0          63              0         0              0     1   0\n194         0          84              0         0              0     1   0\n195         0         238              0         0              0     1   0\n196         0          41              0         0              0     1   0\n    browse n.plants n.years min_yr max_yr\n191      0       12      16   2003   2018\n192      0       12      16   2003   2018\n193      0       12      16   2003   2018\n194      0       12      16   2003   2018\n195      0       12      16   2003   2018\n196      0       12      16   2003   2018\n\n\nThe %in% sites_excluded part picks out every site whose name matches the sites_excluded vector we made earlier.\nAgain, head is just to limit the number of rows R displays.\n\n\n\nPhew! That’s the hard part done. All that is left is to use the ! operator to tell R that we want to exclude these sites, not include them, and then give our new dataset a name.\n\nYellowstone_1 &lt;- Yellowstone[!Yellowstone$site_full %in% sites_excluded,] # remove rows and rename as 'Yellowstone_1'\n\nRipple claims that his study occurred across 25 sites from 2001 to 2020. Let’s make a line graph to see how often each of these 25 sites were actually surveyed:\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nUse geom_line() to make a line graph with year on the x-axis, and sites_full on the y-axis.\nWhat do you notice about the times each of these sites were surveyed?\n\n\nRipple describes how willows in 2020 were, on average, twice as tall as they were in 2001; but the willows from 2020 were not the same as the ones from 2001, because new sites were added in between! While Ripple’s study spanned 20 years in principle, the bulk of his evidence really only spanned 13 years in practice (from 2008 to 2020)."
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#conclusion",
    "href": "labs/Lab01/Lab01-introduction.html#conclusion",
    "title": "Lab 01",
    "section": "Conclusion",
    "text": "Conclusion\n\nClosing Thoughts\nIs this all to say that wolves had no effect on the ecology of Yellowstone National Park? No. In fact, MacNulty himself believes that wolves did in fact cause a trophic cascade, but a much weaker one than what Ripple proposed.\nIs it fair to say that Ripple was a charlatan? Certainly not. Dr William Ripple is a distinguished professor at Oregon State University, and has published many groundbreaking papers on complex ecological processes, including trophic cascades.\nWhat this case study really shows is that even experienced researchers working on high-profile experiments can make mistakes. That’s got to take some pressure off the rest of us, right?\nThe most important thing is to listen to the criticisms of others without taking it too personally. That way, we can help each other avoid costly pitfalls through collaboration.\n\n\n\n\n\n\nNoteFor you to consider\n\n\n\nLet’s revisit the debate between MacNulty and Ripple with a better grasp of the situation. What are your thoughts now? Was MacNulty fair in his criticism of Ripple’s study?"
  },
  {
    "objectID": "labs/Lab01/Lab01-introduction.html#thanks",
    "href": "labs/Lab01/Lab01-introduction.html#thanks",
    "title": "Lab 01",
    "section": "Thanks!",
    "text": "Thanks!\nThat’s all for today. If you have any questions, please approach your demonstrators. Don’t forget to save your Quarto document for future reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX2001 Resources",
    "section": "",
    "text": "This site hosts lectures, tutorials, and labs for ENVX2001. Some content is still being developed and may be incomplete. Students can find all course material on Canvas.\nModule 1: Designed studies\n\n\n\n\n\n\n\n\n\n\nWeek\nLecture\nTutorial\nLab\n\n\n\n\n1\nIntroduction\nTutorial 01\nLab 01 \n\n\n2\nSampling designs\nTutorial 02\nLab 02 \n\n\n3\n1-way ANOVA\nTutorial 03\nLab 03 \n\n\n4\nResidual diagnostics & post hoc tests\nTutorial 04\nLab 04 \n\n\n5\nExperimental design\nTutorial 05\nLab 05 \n\n\n6\nANOVA with blocking\nTutorial 06\nLab 06 \n\n\n\n\nModule 2: Finding patterns in data\n\n\n\n\n\n\n\n\n\n\nWeek\nLecture\nTutorial\nLab\n\n\n\n\n7\nRegression modelling\nTutorial 07\nLab 07 \n\n\n8\nRegression model development\nTutorial 08\nLab 08 \n\n\n9\nRegression model assessment\n–\nLab 09 \n\n\n10\nPrinciple component analysis\nTutorial 10\nLab 10 \n\n\n11\nClustering\nTutorial 11\nLab 11 \n\n\n12\nMultidimensional scaling\nTutorial 12\nLab 12"
  },
  {
    "objectID": "labs/Lab02/Lab02-sample-designs.html#exercise-1-soil-carbon-walk-through",
    "href": "labs/Lab02/Lab02-sample-designs.html#exercise-1-soil-carbon-walk-through",
    "title": "Lab 02",
    "section": "Exercise 1 – Soil carbon (walk-through)",
    "text": "Exercise 1 – Soil carbon (walk-through)\nKarunaratne et al. (2012)1 measured soil carbon (%) in the Cox’s creek catchment in northern NSW using a stratified random sampling scheme. The results are summarised below.\n\nUSE is the land use type;\nN is the number of samples collected;\nMEAN is the mean soil carbon content;\nVARIANCE is the variance of the mean;\nPERCENT_AREA is the percentage of the catchment area represented by each land use type.\n\n\n\n1Karunaratne S. B., Bishop T. F. A., Odeh I. O. A., Baldock J. A., Marchant B. P. (2014) Estimating change in soil organic carbon using legacy data as the baseline: issues, approaches and lessons to learn. Soil Research 52, 349-365.\n\n\n\n\n\nuse\nn\nmean\nvariance\npercent_area\n\n\n\n\nForest\n9\n0.65\n1.4\n20\n\n\nDryland Cropping\n14\n0.96\n0.4\n35\n\n\nPasture-Vertosol\n14\n1.06\n0.8\n35\n\n\nPasture-Other\n2\n1.31\n0.4\n6\n\n\nIrrigated\n5\n0.78\n0.8\n4\n\n\nOverall\n44\n0.92\n1.0\n100\n\n\n\n\n\nRecall that the confidence interval for the mean is an effective way to communicate the precision of the estimates (how well we know the true mean), and is derived from the standard error of the mean.\n\nCalculating the 95% confidence interval of the mean\nThe weighted mean (y_s) and variance (Var(\\bar y_s)) of the mean for a stratified random sample are given by:\n\n\nWeighted mean:\n\nCalculate mean value per stratum\nMultiply each mean value by the weight of the stratum\nSum the weighted mean values to get the overall mean\n\n\\bar y_s = \\sum_{i=1}^L \\bar y_i \\times w_i\nwhere L is the number of strata, \\bar y_i is the mean of stratum i, and w_i is the weight of stratum i, and\n\n\nWeighted variance of the mean:\n\nCalculate the variance of the mean for each stratum\nMultiply the variance of the mean by the square of the weight of the stratum\nSum all the weighted variances to get the overall variance of the mean\n\nVar(\\bar y_s) = \\sum_{i=1}^L w_i^2 \\times Var(\\bar y_i)\nwhere Var(\\bar y_i) is the variance of the mean for stratum i.\nWe can then calculate the 95% CI for the mean:\n\n\n95% CI for the mean:\n\nWeighted mean plus (t-critical value \\times standard error of the mean)\nWeighted mean minus (t-critical value \\times standard error of the mean)\n\n95\\% CI = \\bar y_s \\pm t^{0.025}_{n-L} \\times \\sqrt{Var(\\bar y_s)}\nwhere L = number of strata, n = total number of samples, t^{0.025}_{n-L} is the t-critical value for the 95% CI and \\sqrt{Var(\\bar y_s)} is the variance of the mean for the stratified random sample.\nIs stratified random sampling a good idea for this study? Let’s find out.\n\n\n\n\n\n\nImportant\n\n\n\nUnderstanding how to calculate the 95% CI using summary statistics is examinable. If you find it difficult to understand the mathematical notation, it might be easier to rewrite them in your own words – for example, see the side notes.\n\n\n\n\nDoing the calculations in R\nThe data is provided below. The first is the stratified dataset, which contains the mean and variance of the soil carbon content for each land use type.\n\nstratified &lt;- data.frame(\n    use = c(\n        \"Forest\", \"Dryland Cropping\", \"Pasture-Vertosol\", \"Pasture-Other\",\n        \"Irrigated\"\n    ),\n    n = c(9, 14, 14, 2, 5),\n    mean = c(0.65, 0.96, 1.06, 1.31, 0.78),\n    variance = c(1.4, 0.4, 0.8, 0.4, 0.8),\n    percent_area = c(20, 35, 35, 6, 4)\n)\n\nYou should copy and paste this data into your own document and run stratified to view the data.\nLook at the code and see if you can understand how a data frame is created in R. The data.frame function is used to create a data frame, and the c function is used to create vectors of data. Each vector forms a column in the data frame.\n\n\nQuestion 1\nEstimate the mean and associated 95% CI assuming stratified random sampling using the carbon dataset. We can use the weighted.mean function to calculate the weighted mean, and the qt function to calculate the t-critical value for a 95% CI. You should find out what these functions do by seeking their help documentation using the ? operator.\n?weighted.mean\n?qt\n\n\nTip: you can manipulate the object and subset specific rows, columns or cells using $ and [] respectively. For example, to access the mean column of the carbon dataset, you can use carbon$mean. To access the mean value for the Forest stratum, you can use carbon$mean[1]."
  },
  {
    "objectID": "labs/Lab02/Lab02-sample-designs.html#exercise-2---soil-carbon-what-if",
    "href": "labs/Lab02/Lab02-sample-designs.html#exercise-2---soil-carbon-what-if",
    "title": "Lab 02",
    "section": "Exercise 2 - Soil carbon: what if…?",
    "text": "Exercise 2 - Soil carbon: what if…?\nWhat if the authors had used simple random sampling? Below is the data object overall which contains the overall mean and variance of the soil carbon content for the entire catchment. It is also the last row of the table above.\noverall &lt;- data.frame(\n    use = \"Overall\",\n    n = 44,\n    mean = 0.92,\n    variance = 1,\n    percent_area = 100\n)\nCalculating the 95% CI for the mean using simple random sampling is similar to the stratified random sampling. The only difference is that the variance of the mean is calculated using the overall variance and sample size.\nVar(\\bar y) = \\frac{Var(y)}{n}\nwhere Var(y) is the variance of the entire dataset and n is the total number of samples.\nThus the 95% CI for the mean is:\n95\\% CI = \\bar y \\pm t^{0.025}_{n-1} \\times \\sqrt{Var(\\bar y)}\nwhere n is the total number of samples, and t^{0.025}_{n-1} is the t-critical value for the 95% CI.\n\nQuestion 2\nEstimate the mean and associated 95% CI assuming simple random sampling using the overall dataset.\n\n\nQuestion 3\nWould you recommend stratified random sampling for future surveys? Provide evidence from the results of questions 2 and 3."
  },
  {
    "objectID": "labs/Lab02/Lab02-sample-designs.html#exercise-3---kangaroos",
    "href": "labs/Lab02/Lab02-sample-designs.html#exercise-3---kangaroos",
    "title": "Lab 02",
    "section": "Exercise 3 - Kangaroos",
    "text": "Exercise 3 - Kangaroos\n\nShould we return to the same locations?\nIn this exercise we will explore the impact that resampling the locations has on the precision (width of the 95% CI) with which we estimate the change in mean between 2 surveys. We will also show the equivalence of this to 2-sample t-tests.\nThe key equation for estimating the variance of the change in mean is:\nVar (\\Delta\\bar{y}) =\n  Var (\\Delta\\bar{y}_2) +\n  Var (\\Delta\\bar{y}_1) -\n  2 \\times Cov(y_1, y_2) \nwhere:\n\nVar (\\Delta\\bar{y}) is the variance of the change in mean between 2 surveys;\nVar (\\Delta\\bar{y}_2) is the variance of the mean for the 1st survey (baseline);\nVar (\\Delta\\bar{y}_1) is the variance of the mean for the 2nd survey (repeat);\nCov(y_1, y_2) is the covariance between the means of the 2 surveys.\n\nWhen we resample the same locations, we include the covariance term which describes the relationship between the observations in the 2 surveys. If resample at different locations we assume the covariance is equal to 0, that is:\n Var (\\Delta\\bar{y}) = Var (\\Delta\\bar{y}_2) + Var (\\Delta\\bar{y}_1) \nIn this exercise we will consider a study where kangaroos (number/km2) were counted in a woodland in an initial survey, and then 2 years later. The data is below.\n\n\n'data.frame':   5 obs. of  2 variables:\n $ baseline: num  4 2 6 1 3\n $ rerun   : num  12 11 13 14 9\n\n\nCan you generate a data.frame object from the output above? Try it yourself first, and then click on show the code below to see the answer (Note: PDF output will not hide the answer).\n\n\nShow the code\nbaseline &lt;- c(4, 2, 6, 1, 3)\nrerun &lt;- c(12, 11, 13, 14, 9)\nkangaroos &lt;- data.frame(baseline, rerun)\n\n\n\n\nQuestion 4\nIntuitively, what would be the impact on the precison with which we estimate the mean when:\n\nwe resample the same locations in the repeat survey?\nwe sample at different locations in the repeat survey?\n\n\n\nDifferent locations\nLet’s assume that the resampled locations are completely different from the baseline survey. This means that the covariance term is equal to 0.\n\n\nQuestion 5\nUse R to estimate the 95% CI for the change in mean between the 2 surveys. The equation is:\n95\\%\\ CI=\\Delta\\left(\\bar{y}\\right)\\pm t_{df}^{0.025}\\times\\sqrt{Var\\left(\\Delta\\bar{y}\\right)}\nYou will need to calculate {Var}\\left(\\Delta\\left(\\bar{y}_1\\right)\\right), Var\\left(\\Delta\\left(\\bar{y}_2\\right)\\right), and Cov\\left(\\bar{y}_1,\\bar{y}_2\\right).\nAs a starting point R script for calculating the covariance and variance for the observations is below, as is the code to calculate the t critical value (t_{df}^{0.025}). When we sample at different locations between surveys: df = n_{1}+ n_{2} - 2 = 5 + 5 - 2 = 8\n\n# If you want to isolate data from the two sampling periods:\nt0 &lt;- kangaroos$baseline\nt2 &lt;- kangaroos$rerun\n\n# NOTE: functions below are not assigned to objects.\n# To calculate covariance:\ncov(t0, t2)\n\n# To calculate variances:\nvar(t0)\nvar(t2)\n\n# t-critical value for a 95% CI with 8 degrees of freedom:\nqt(0.975, 8)\n\n\n\nSame locations\nNow let’s assume that the resampled locations are the same as the baseline survey. This means that the covariance term is not equal to 0.\n\n\nQuestion 6\nAssuming we resampled the same locations as the baseline survey, use R to estimate the 95% CI for the change in mean between the 2 surveys.\n\n\nNote: when we sample at same locations between surveys the df = number of paired sites - 1 = 5 - 1 = 4.\nHint: the following formulae are used:\n\\Delta \\bar y = \\bar y_2 - \\bar y_1 Var(\\Delta \\bar y) = Var( \\bar y_2) + Var( \\bar y_1) - 2 \\times Cov(\\bar y_1, \\bar y_2)\n\n\nQuestion 7\nWas there a significant change in the mean number of kangaroos between the study period, if we assume the same locations were resampled, or if different locations were sampled?\n\n\nPutting it together\nAs you should realise by now, the same data may produce different results depending on how it was analysed. This is the danger of statistics: not clearly understanding the assumptions and implications of an experimental design can lead to incorrect analysis and conclusions.\nAny statistical software, including R, will not tell you if you have made a mistake in your analysis. It is up to you to understand the implications of your design and to ensure that your work is appropriate.\nIn th case of this kangaroo study, the results are the same for both scenarios, which means that the implications lie in the context of future studies.\n\n\nQuestion 8\nIn the future, would you re-sample the same sites, or visit new sites in a kangaroo monitoring program of the same woodland?"
  },
  {
    "objectID": "labs/Lab02/Lab02-sample-designs.html#exercise-4-equivalence-to-t-tests",
    "href": "labs/Lab02/Lab02-sample-designs.html#exercise-4-equivalence-to-t-tests",
    "title": "Lab 02",
    "section": "Exercise 4 – Equivalence to t-tests",
    "text": "Exercise 4 – Equivalence to t-tests\nThe scenario when we resample at different locations between surveys is actually the equivalent to a two-sample t-test, and the scenario when we resample the same locations is equivalent to a paired t-test.\nWithout manually calculating and comparing the 95% CI, we can use the t.test() function in R to perform both of these tests.\nCheck your answers in R by trying to perform both of these using the t.test() function. Assume the variances are equal for the two-sample t-test (use the argument var.equal = TRUEfor this).\n\nQuestion 9\nCompare the 95% CI for each scenario between the t.test() results and the ones you calculated manually. Are they same?"
  },
  {
    "objectID": "labs/Lab02/Lab02-sample-designs.html#thanks",
    "href": "labs/Lab02/Lab02-sample-designs.html#thanks",
    "title": "Lab 02",
    "section": "Thanks!",
    "text": "Thanks!\nDid you know you can also knit to PDF? Check the documentation for R Markdown or Quarto for more information.\n\nAttribution\nThis lab was developed using resources that are available under a Creative Commons Attribution 4.0 International license, made available on the SOLES Open Educational Resources repository."
  },
  {
    "objectID": "labs/Lab04/Lab04-residuals-posthoc.html",
    "href": "labs/Lab04/Lab04-residuals-posthoc.html",
    "title": "Lab 4 - Assumptions & Multiple Comparisons",
    "section": "",
    "text": "TipLearning outcomes\n\n\n\nAt the end of this Lab students should be able to:\n\ntest the assumptions of ANOVA using residual diagnostics;\nuse plotting and Tukey’s tests to determine which pairs of groups are significantly different;\nuse R to perform the analyses.\n\nAll of the data for this practical is in the Data4.xlsx file."
  },
  {
    "objectID": "labs/Lab04/Lab04-residuals-posthoc.html#exercise-1---diatoms-in-streams-walk-through",
    "href": "labs/Lab04/Lab04-residuals-posthoc.html#exercise-1---diatoms-in-streams-walk-through",
    "title": "Lab 4 - Assumptions & Multiple Comparisons",
    "section": "Exercise 1 - Diatoms in streams (Walk-through)",
    "text": "Exercise 1 - Diatoms in streams (Walk-through)\nHere we will test the assumptions using residual diagnostics and finding significant differences using plots and Tukey’s test. The data is found in the Diatoms worksheet.\n\nQuestion 1.1\n(i) Importing and processing data, then fitting an ANOVA model\n\n# write your code here\n\n\n\nQuestion 1.2\n(ii) Statistical test of the assumption of constant variance\nStatistics is made up of different tribes and some tribes use hypothesis testing to see if a dataset meets the assumptions of normality and constant variance. One option is the Bartlett’s test for constant variances. The mechanics are not important but the function and syntax are shown below. The hypotheses are:\n\nH_0:\\ \\sigma_{BACK}^2=\\sigma_{LOW}^2=\\sigma_{MED}^2=\\sigma_{HIGH}^2\nH_1:\\ not\\ all\\ \\sigma_i^2\\ are\\ equal\\ \\left(i\\ =\\ BACK,\\ LOW,\\ MED,\\ HIGH\\right)\n\nWe prefer to use numerical and graphical diagnostics, e.g. residuals plots, but this is more to show you other possibilities. You can use this as a different line of evidence for testing assumptions if you wish.\n\n\n\n\n\n\nWarning\n\n\n\nIt won’t work if the data is non-normal and only use it if the data has one treatment factor with a completely randomised design!\n\n\n\n#\n\n\n\nQuestion 1.3\n(iii) Identify significant differences\nIn Topic 3 we used the emmeans package to extract means for each group and their associated 95% CI. The emmeans is useful to produce a plot showing the mean and 95% CI which is a nice way to present the results.\n\n#\n\n\n\nQuestion 1.4\n(iv)The another approach is to use a Tukey’s test which we can extract using the emmeans(model-goes-here, pairwise ~ your-treatment) function from the emmeans package. Note there are many other post-hoc tests that have come and gone, but we will just focus on one of them.\n\n\nQuestion 1.5\n(v) Another way to present the results is to use the plot() function to show the confidence intervals and the comparisons among them.\n\n\nQuestion 1.6\nAn alternative base R function for Tukey tests is TukeyHSD(). It creates a set of confidence intervals on the differences between the means of the levels of a factor. Also plot the results from this function.\n\n\n\n\n\n\nTip\n\n\n\nIf the confidence interval does not cross over 0, then that pair significantly differs from each other.\n\n\n\n#"
  },
  {
    "objectID": "labs/Lab04/Lab04-residuals-posthoc.html#exercise-2---mean-comparisons-residual-diagnostics-and-back-transformations",
    "href": "labs/Lab04/Lab04-residuals-posthoc.html#exercise-2---mean-comparisons-residual-diagnostics-and-back-transformations",
    "title": "Lab 4 - Assumptions & Multiple Comparisons",
    "section": "Exercise 2 - Mean comparisons, residual diagnostics and back-transformations",
    "text": "Exercise 2 - Mean comparisons, residual diagnostics and back-transformations\nIn this exercise will add a layer of complexity by considering a transformation. If our data does not meet the assumptions we need to transform the data, possible transformations are the square root (weak) and log (high). When we transform the data we need to be careful about how we interpret the results.\nConcentration of prolactin (units g/L) in the pituitary glands of nine-spined stickleback fish was assessed. The fish were kept in either saltwater or freshwater prior to assay and were different batches were examined on three successive occasions. Cysts tend to develop in fish when kept in saltwater and sometimes develop in freshwater populations. The four different groups of fish were used in a preliminary experiment to examine the effects of cysts, whether induced by saltwater or normally present, on the prolactin production of the pituitary gland.\nThe four groups of fish were codes as follows, with 10 fish per group:\n\nA = saltwater cysts, day 1;\nB = freshwater, no cysts, day 2;\nC = freshwater, no cysts, day 2;\nD = freshwater, cysts, day 3.\n\nThe data is found in the Prolactin worksheet.\n\nQuestion 2.1\n(i) Import the data into R, perform some exploratory data analysis to make tentative suggestions about differences between means and the likelihood of the data meeting the assumptions.\n\n#\n\n\n\nQuestion 2.2\n(ii) Fit an ANOVA model and test the assumption of normality using a QQ plot and a histogram - both based on standardised residuals.\n\n#\n\n\n\nQuestion 2.3\n(iii) Assess the assumption of constant variance by:\n\nexamine the plot of the standardised residuals against fitted values;\n\n\n#\n\n\nFrom the performance package, use check_model() to assess the assumptions of the model. This function will check the assumptions of normality and constant variance;\n\n\n#\n\n\nusing the Bartlett’s test;\n\n\n#\n\n\nusing check_homogeneity() from the performance package;\n\n\n\n\n\n\n\nTip\n\n\n\nNote there are many tests: method = c(\"bartlett\", \"fligner\", \"levene\", \"auto\")\n\n\n\n#\n\n\ncalculating the ratio of the larges SD:smallest SD to see if it is below 2:1;\n\n\n#\n\n\n\nQuestion 2.4\n(iv) The data does not meet the assumptions so log transform (‘log’ function) the response and repeat (ii) and (iii) to test the assumptions;\n\n#\n\n\n\nIn R you can transform data in the model formula see below or you could create a new column in your data frame, for example fish$logProlactin&lt;-log(fish$Prolactin).\n\n\nQuestion 2.5\n(v) If the assumptions are met and there is significant F-test perform Tukey tests and identify which pairs are significantly different.\n\n#\n\n\n\nQuestion 2.6\n(vi) One issue is that we have performed our hypothesis testing on the log scale. This means there are some steps to be made if we wish to interpret the data on the original scale; e.g. provide a 95% CI on the original scale. We will step through these.\nSuppose the biologist was primarily interested in comparing the prolactin concentrations for A (saltwater cysts, day 1) vs B (freshwater, no cysts, day 1).\n\nFind the means and CI from the output of the TukeyHSD() or emmeans() function.\nThe CI and mean are on the log scale, so back-transform the difference in the means (exp() function), the lower and upper end-point 95% CI. Note that the upper and lower tail are not of equal length on the original scale.\n\n\n#\n\n\n\nQuestion 2.7\n(vii) Now have an estimate of the difference in the means on the original scale. It actually corresponds to a ratio on the original scale. The reason is based on log laws, we can write the difference between 2 logged numbers (A and B) as a log of their ratio (A/B);\n\\log\\left(A\\right)-\\log\\left(B\\right)=\\log\\left(\\frac{A}{B}\\right).\nIf we back-transform the log of their ratio we get the ratio on the original scale;\ne^{\\log\\left(\\frac{A}{B}\\right)}=\\frac{A}{B}.\nSo the back-transformed difference between the pairs of the means is a ratio.\nNote: if we were to back-transform the group means on the log scale we would get the geometric mean on the original scale.\nProvide a biological interpretation for this estimate and confidence interval. Use the CI to decided if there is a significant difference between Treatment A and Treatment B."
  },
  {
    "objectID": "labs/Lab04/Lab04-residuals-posthoc.html#exercise-3---broiler-chickens",
    "href": "labs/Lab04/Lab04-residuals-posthoc.html#exercise-3---broiler-chickens",
    "title": "Lab 4 - Assumptions & Multiple Comparisons",
    "section": "Exercise 3 - Broiler Chickens",
    "text": "Exercise 3 - Broiler Chickens\nThis exercise is an analysis of a set of growth data. It is an open question for you to gain more practice.\nThe effect of weight gain in dressed broiler chickens was determined after five generations of selection. Group A was bred by using only the heaviest 10% in each generation; groups B and C were bred using respectively the heaviest 30% and 50%; group D was obtained by crossing groups A and C of the previous generation. The dressed weights (kg) of 25 birds from each group have been recorded.\nThe data is found in the Broilers worksheet.\n\nQuestion 3.1\n(i) Write down the null and alternate hypothesis. What is the treatment factor, and how many levels does it have? What are the sample sizes for each group (n_i)?\n\n\nQuestion 3.2\n(ii) Import the data into R, and then obtain some numerical and graphical summaries of the data, by each group. How would you interpret these data? From these summaries, is the assumption of homogeneity of variances met? What about normality? Try a formal Bartlett’s test using the bartlett.test or check_homogeneity() function. Use residual diagnostics to assess the assumptions.\n\n#\n\n\n\nQuestion 3.3\n(iii) Note that the results of the analysis can only be used when the assumptions of the analysis have been met. If you believe that the assumptions are met, then what would your conclusions of the analysis of variance be? You should use the summary() function applied to your aov() object to obtain the ANOVA table.\n\n#\n\n\n\nQuestion 3.4\n(iv) Without any formal analysis, consider the result of the group means in relation to the group treatment - i.e. type of selection. Would this pattern be expected? If appropriate perform a Tukey test.\n\n#"
  },
  {
    "objectID": "labs/Lab06/Lab06-factorial-anova.html",
    "href": "labs/Lab06/Lab06-factorial-anova.html",
    "title": "Lab 6 - Factorial Designs",
    "section": "",
    "text": "TipLearning outcomes\n\n\n\nAt the end of this practical students should be able to:\n\nuse R to analyse experiments with a factorial treatment structure where the experimental design is a CRD or RCBD;\n\nAll of the data for this practical is in the Data6.xlsx file."
  },
  {
    "objectID": "labs/Lab06/Lab06-factorial-anova.html#exercise-1---more-treatment-structures-walk-through",
    "href": "labs/Lab06/Lab06-factorial-anova.html#exercise-1---more-treatment-structures-walk-through",
    "title": "Lab 6 - Factorial Designs",
    "section": "Exercise 1 - More Treatment Structures (Walk-through)",
    "text": "Exercise 1 - More Treatment Structures (Walk-through)\nA microbiologist conducted an experiment to assess the survival of Salmonella typhinerium when subjected to various treatments. A factorial treatment design was used, the treatments being various combinations of sorbic acid, pH, and water activity. The density of Salmonella seven days after the treatment began was recorded. The Salmonella data is in the form:\nlog_{e}(density/ml), to 2 decimal places.\nThe data is found in the Salmonella sheet in the Data6.xlsx file.\nThere is a slight trick to analysing this experiment which will improve your ability to analyse factorial experiments in general.\nThe table below shows you how to specify the treatment structure and individual interaction term in a factorial design involving 2 factors, A and B.\n\n\n\n\n\nterm\ncommand\n\n\n\n\n2-way factorial\nA*B\n\n\n2-way interaction\nA:B\n\n\n\n\n\n\nQuestion 1.1\n(i) Import the data into R and describe the data using numerical and graphical summaries. In particular does the exploratory data analysis (EDA) show any difference in S.typhinerium density between the levels of each of the treatment factors (a) sorbic acid (b) pH (c) water activity? Is there evidence of an interaction using interaction plots. No need to look at marginal means as the interaction plots summarise these.\n\n\n\n\n\n\n\nTip\n\n\n\nSome hints for the EDA are:\n\nin many experiments we will have treatment factors which have a numerical value (as is the case here) but for ANOVA-type problems we are comparing the mean response between each value so we need to tell R the data type is a factor. If we don’t, it will fit a regression model which is Topics 7-9. This will result in the wrong type of analysis.\nuse the tapply function in conjunction with the summary function;\nboxplots for individual levels of a factor are likely to be the most informative as compared to numerical summaries they visually show the spread of the observations as well. Example R code is below.\n\n\nboxplot(Density~pH,data=salm)\n\n\nto create interaction plots use the interaction.plot or emmip function. You will need one of these for each 2-way interaction in your model.\n\n\n\n\n#\n\n\nQuestion 1.2\n(ii) Write out the model you are fitting in terms of main effects and interactions, words are fine. Note, that each of these will have an associated statistical hypotheses. Obtain an ANOVA for these data, including appropriate interaction terms in the model.\nWhat do you notice in the ANOVA table when your run the full 3-way factorial ANOVA? Use the summary function to extract the ANOVA table.\nRun the model again but only include the interactions involving two factors: do not include the three factor interaction term. Why are we not including the three factor interaction?\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the data in Excel and manually calculate the mean Density for when pH = 5.0, Activity = 0.78 and Sorbic.Acid = 0. How many observations were used to calculate the mean?\n\n\nThe model in words is:\n\nQuestion 1.3\n(iii) Test that the model assumptions have been met.\n\n\n#\n\n\nQuestion 1.4\n(iv) What are the significant effects in the model? For the significant effects we conduct post-hoc tests the Tukey’s Test to determine which pairs are significantly different. Write overall conclusions, in terms of the statistical hypothesis testing and in terms of the biological description of the experiment.\n\n\n#"
  },
  {
    "objectID": "labs/Lab06/Lab06-factorial-anova.html#exercise-2---more-practice",
    "href": "labs/Lab06/Lab06-factorial-anova.html#exercise-2---more-practice",
    "title": "Lab 6 - Factorial Designs",
    "section": "Exercise 2 - More practice",
    "text": "Exercise 2 - More practice\nAn experiment was performed to study the control of potato blight on potatoes. A factorial treatment structure was employed with 3 Varieties in combination with 5 Chemical treatments. The experiment was conducted using a randomised complete block design (RCBD) with three blocks. The response is yield of potatoes (lbs).\nProvide graphical summaries of the data, analyse the experiment, conduct any post-hoc tests if the results are significant. The data is found in the Potato sheet in the Data6.xlsx file.\n\n#"
  },
  {
    "objectID": "labs/Lab08/Lab08-model-selection.html",
    "href": "labs/Lab08/Lab08-model-selection.html",
    "title": "ENVX2001 - Model selection",
    "section": "",
    "text": "In this lab, you will work towards achieving learning outcomes"
  },
  {
    "objectID": "labs/Lab08/Lab08-model-selection.html#learning-outcomes",
    "href": "labs/Lab08/Lab08-model-selection.html#learning-outcomes",
    "title": "ENVX2001 - Model selection",
    "section": "",
    "text": "In this lab, you will work towards achieving learning outcomes"
  },
  {
    "objectID": "labs/Lab08/Lab08-model-selection.html#lab-objectives",
    "href": "labs/Lab08/Lab08-model-selection.html#lab-objectives",
    "title": "ENVX2001 - Model selection",
    "section": "Lab Objectives",
    "text": "Lab Objectives\nIn this lab, we will:\n\n[]\n[]\n\n\n\n\n\n\n\nTip\n\n\n\nPlease work on this exercise by creating your own R Markdown file."
  },
  {
    "objectID": "labs/Lab08/Lab08-model-selection.html#exercise-1-model-quality---multiple-vs-adjusted-r2",
    "href": "labs/Lab08/Lab08-model-selection.html#exercise-1-model-quality---multiple-vs-adjusted-r2",
    "title": "ENVX2001 - Model selection",
    "section": "Exercise 1: Model quality - multiple vs adjusted r2",
    "text": "Exercise 1: Model quality - multiple vs adjusted r2\nData: California_streamflow spreadsheet\nImport the “California_streamflow” sheet into R.\n\n# Load library if needed\nlibrary(readxl)\n# Load data\nstream_data &lt;- read_xlsx(\"data/california_streamflow.xlsx\", \"streamflow\")\n\nin this exercise we will use the same data as last week. To jog your memory, the dataset contains 43 years of annual precipitation measurements (in mm) taken at (originally) 6 sites in the Owens Valley in California. Through model selection via partial F-test we have the final model as below:\n\nfit &lt;- lm(runoff_volume ~ rock_creek + pine_creek, data = stream_data)\n\nWe will now add a totally useless variable to the dataset. This variable is a random number generated from a normal distribution with mean 3 and standard deviation 2. We use the set.seed() function to make sure that everybody gets the same random values.\n\nset.seed(100) # to make sure everybody gets the same results\n\n# this generates the random number into the dataset\nstream_data$random_no &lt;- rnorm(n = nrow(stream_data), mean = 3, sd = 2) \n\nWe will see the impact of including a totally useless variable, such as this random variable, has on measures of model quality, r2 and adjusted r2 values.\nTask: create two regression models:\n\nrunoff_volume ~ rock_creek + pine_creek\nrunoff_volume ~ rock_creek + pine_creek + random_no\n\n\nQuestion 2\nCompare each in terms of their multiple r2 and adjusted r2 values. Which performance measure (multiple r2 or adj r2) would you use to identify which predictors to use in your model?"
  },
  {
    "objectID": "labs/Lab08/Lab08-model-selection.html#exercise-2-fish-productivity",
    "href": "labs/Lab08/Lab08-model-selection.html#exercise-2-fish-productivity",
    "title": "ENVX2001 - Model selection",
    "section": "Exercise 2: Fish productivity",
    "text": "Exercise 2: Fish productivity\nFish communities were surveyed in lakes across a eutrophication gradient to investigate the relatioship between productivity and fish diversity.\nThe datasheet has thefollowing variables:\n\nlake_id Unique identifier for each lake\nchla Log-transformed Chlorophyll a\nrichness Log-transformed species richness\nevenness Log-transformed Pielou’s evenness\n\nabundance Log-transformed abundance per unit effort (NPUE)\n\nbiomass Log-transformed biomass per unit effort (BPUE)\nproductivity Log-transformed productivity proxy\n\n\n#Load library if necessary\nlibrary(tidyverse)\n\n#Read in data\nfish_data &lt;- read_csv(\"data/fish_communities.csv\")\n\nRows: 39 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): lake_id\ndbl (6): chla, richness, evenness, abundance, biomass, productivity\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Have a look at the structure of the data\nstr(fish_data)\n\nspc_tbl_ [39 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ lake_id     : chr [1:39] \"FTH\" \"XLH\" \"HGH\" \"LH\" ...\n $ chla        : num [1:39] 4.61 4.53 3.84 3.71 2.43 ...\n $ richness    : num [1:39] 2.64 2.89 1.99 2.94 2.37 ...\n $ evenness    : num [1:39] -0.356 -0.67 -0.423 -0.405 -0.589 ...\n $ abundance   : num [1:39] 2.24 3.34 1.24 2.67 1.12 ...\n $ biomass     : num [1:39] 4.96 6.03 4.67 6.52 5.32 ...\n $ productivity: num [1:39] 3.47 4.47 2.71 4.47 3.24 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   lake_id = col_character(),\n  ..   chla = col_double(),\n  ..   richness = col_double(),\n  ..   evenness = col_double(),\n  ..   abundance = col_double(),\n  ..   biomass = col_double(),\n  ..   productivity = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nExplore the data on your own before making any models. (Hint: remember that we are only interested in the numeric variables for our linear models)\n\nQuestion 1\nAre there any obvious relationships between the response variable (productivity) and the other variables?\n\n\nQuestion 2\nAre there any other patterns or potential issues you can see from the exploratory plots?"
  },
  {
    "objectID": "labs/Lab08/Lab08-model-selection.html#take-home-exercise",
    "href": "labs/Lab08/Lab08-model-selection.html#take-home-exercise",
    "title": "ENVX2001 - Model selection",
    "section": "Take home exercise:",
    "text": "Take home exercise:"
  },
  {
    "objectID": "labs/Lab08/Lab08-model-selection.html#review",
    "href": "labs/Lab08/Lab08-model-selection.html#review",
    "title": "ENVX2001 - Model selection",
    "section": "Review",
    "text": "Review\n\nAttribution"
  },
  {
    "objectID": "labs/Lab10/Lab10-pca.html",
    "href": "labs/Lab10/Lab10-pca.html",
    "title": "Lab 10",
    "section": "",
    "text": "In this lab, we will learn how to:\n\nPerform Principal Component Analysis (PCA) on various datasets\nInterpret the output of PCA and Factor Analysis (FA) – Loadings, biplot, screeplot, rotations"
  },
  {
    "objectID": "labs/Lab10/Lab10-pca.html#learning-outcomes",
    "href": "labs/Lab10/Lab10-pca.html#learning-outcomes",
    "title": "Lab 10",
    "section": "",
    "text": "In this lab, we will learn how to:\n\nPerform Principal Component Analysis (PCA) on various datasets\nInterpret the output of PCA and Factor Analysis (FA) – Loadings, biplot, screeplot, rotations"
  },
  {
    "objectID": "labs/Lab10/Lab10-pca.html#specific-goals",
    "href": "labs/Lab10/Lab10-pca.html#specific-goals",
    "title": "Lab 10",
    "section": "Specific goals",
    "text": "Specific goals\nBy the end of this lab, you should be able to:\n\nDecide if a dataset is suitable for multivariate analysis\nPerform a PCA in R\nMake a scree plot\nInterpret loadings"
  },
  {
    "objectID": "labs/Lab10/Lab10-pca.html#buckle-up",
    "href": "labs/Lab10/Lab10-pca.html#buckle-up",
    "title": "Lab 10",
    "section": "Buckle Up!",
    "text": "Buckle Up!\nWelcome to week 10. We hope you’ve had fun so far in ENVX2001 :) From now until the end of the course, we will dive into the world of multivariate statistics.\nWe’ve saved the best for last. Out of all the topics in this unit, multivariate statistics might just be the coolest one. In the beginning, you may find some aspects of this topic a little confusing; if that’s the case, please approach one of your demonstrators – they will happily help you out.\nBy the end of this lab, you will know how to explore multivariate datasets in a simple and intuitive way.\nIn particular, we will learn about a very common multivariate analysis technique: Principle Component Analysis, or PCA."
  },
  {
    "objectID": "labs/Lab10/Lab10-pca.html#preparation",
    "href": "labs/Lab10/Lab10-pca.html#preparation",
    "title": "Lab 10",
    "section": "Preparation",
    "text": "Preparation\nTo prepare for this lab, please activate the following packages:\n\nlibrary(readr)\nlibrary(ggplot2)\n\nAnd download these files: bumpus_sparrows_clean.csv, FactBeer.csv, kangaroo.csv\nIf you are a fan of learning from textbooks, here are some we recommend:\n\nQuinn, G. P. and Keough, M. J. (2002, 2024) Experimental Design and Data Analysis for Biologists. Cambridge University Press.\nHan, S. Y., Filippi, P., Román Dobarco, M., Harianto, J., Crowther M. S., and Bishop, T. F. A. (2023). Multivariate analysis for soil science. In ‘Encyclopedia of Soils in the Environment (Second Edition)’. (Ed. M. J. Goss and M. Oliver) pp. 499-508. Academic Press: Oxford.\n\nIt is also a good idea to have pen and paper (or stylus and ipad) in hand while you go through this lab. One of the best ways to make sense of multivariate statistics is to draw lots of pictures.\nReady? Let’s get started!"
  },
  {
    "objectID": "labs/Lab10/Lab10-pca.html#part-1-why-pca-when-pca-who-pca",
    "href": "labs/Lab10/Lab10-pca.html#part-1-why-pca-when-pca-who-pca",
    "title": "Lab 10",
    "section": "Part 1: Why PCA? When PCA? Who PCA?",
    "text": "Part 1: Why PCA? When PCA? Who PCA?\nIn the last 9 weeks of ENVX2001, we learned about t-tests, ANOVAs, simple linear regressions, multiple linear regressions, interactions, blocking terms …\nThat’s a lot of statistical tools already. Do we really need more?\nWell, let’s see.\n\nExercise: Irises\n\n\n\n\n\n\n\n\n\nThe beautiful Iris versicolor , also called the harlequin blueflag, a species of wildflower native to the marshes of North America. From Wikimedia Commons (2016), by R. A. Nonenmacher.\nOne of my personal favourite datasets to explore, for almost any kind of analysis, is the iris dataset, which contains floral measurements from three different species of irises: Iris versicolour, Iris setosa, and Iris virginica.\nThis dataset actually has a rather shady past; it was originally published in the ‘Annals of Eugenics’ by Ronald Fisher, one of the most influential figures in 20th Century Statistics, and a pretty controversial person. 1\n1 FISHER, R. A. (1936). THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS. Annals of Eugenics, 7(2), 179–188. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x2 FISHER, R. A. (1958). Cancer and Smoking. Nature, 182(4635), 596–596. https://doi.org/10.1038/182596a0Fisher invented brilliant statisical concepts such as the F-Distribution (F for Fisher, if you were wondering) and the Analysis of Variance (ANOVA), but he also endorsed eugenics, feuded with his fellow academics, and refused to believe that smoking could increase the risk of lung cancer. 2\nA good example of how complex people can be, I suppose.\nFortunately for us, the ‘Annals of Eugenics’ was brought down a long time ago, but the iris dataset survived as a training tool for statisticians.\nThe iris dataset is built into R itself, and conveniently named iris. We can summon it like this:\n\n\n\n\n\n\nNoteThe iris dataset\n\n\n\n\n\n\niris # It's LONG\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.7         4.4          1.5         0.4     setosa\n17           5.4         3.9          1.3         0.4     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n22           5.1         3.7          1.5         0.4     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n28           5.2         3.5          1.5         0.2     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n32           5.4         3.4          1.5         0.4     setosa\n33           5.2         4.1          1.5         0.1     setosa\n34           5.5         4.2          1.4         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n36           5.0         3.2          1.2         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n41           5.0         3.5          1.3         0.3     setosa\n42           4.5         2.3          1.3         0.3     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n47           5.1         3.8          1.6         0.2     setosa\n48           4.6         3.2          1.4         0.2     setosa\n49           5.3         3.7          1.5         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n55           6.5         2.8          4.6         1.5 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n58           4.9         2.4          3.3         1.0 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n60           5.2         2.7          3.9         1.4 versicolor\n61           5.0         2.0          3.5         1.0 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n65           5.6         2.9          3.6         1.3 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n69           6.2         2.2          4.5         1.5 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n72           6.1         2.8          4.0         1.3 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n79           6.0         2.9          4.5         1.5 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n83           5.8         2.7          3.9         1.2 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n98           6.2         2.9          4.3         1.3 versicolor\n99           5.1         2.5          3.0         1.1 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n101          6.3         3.3          6.0         2.5  virginica\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n106          7.6         3.0          6.6         2.1  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n112          6.4         2.7          5.3         1.9  virginica\n113          6.8         3.0          5.5         2.1  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n116          6.4         3.2          5.3         2.3  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n125          6.7         3.3          5.7         2.1  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n133          6.4         2.8          5.6         2.2  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n141          6.7         3.1          5.6         2.4  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n144          6.8         3.2          5.9         2.3  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\n\n\n\n\n\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nCheck the structure of the iris dataset using the str() function.\n\n\nIt looks like the iris dataset contains quite a few variables. In particular, we have four response variables to test: sepal width, sepal length, petal width, and petal length (check out this page to learn more about flower anatomy). All of these are measured in centimeters.\nOur first instinct might be to analyse them one at a time. Let’s see how that works out:\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nChoose one of the four response variables in iris, and use ggplot2 to make a box plot. What do you think? Is there a difference between the three iris species?\n\n\nDepending on which response variable you chose, you may or may not have seen much of a difference between the three species.\nFor example, Iris setosa certainly seems to have shorter petals than the other two species:\n\nggplot(iris, aes(x = Species, y = Petal.Length)) +\n  geom_boxplot(colour = 'black', fill = 'lightblue')+\n  theme_classic()\n\n\n\n\n\n\n\n\nLet’s run a one-way ANOVA to check.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nUse a one-way ANOVA to test whether petal length differs between the three iris species.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo run a one-way ANOVA in R, use the function aov(). Remember that petal length is the response variable in this case, and iris species is the explanatory variable.\nStructure your data as: aov(response_variable ~ explanatory_variable, data = iris).\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nHere’s what we did:\n\nANOVA_petal_length &lt;- aov(Petal.Length ~ Species, data =iris) # Runs the test\n\nsummary(ANOVA_petal_length) # Prints the output\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  437.1  218.55    1180 &lt;2e-16 ***\nResiduals   147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe found a p-value of &lt; 0.001, which means that at least one of the three iris species has a different average petal length to the others.\n\n\n\nSo… case closed? The irises are different, time to move on?\nNot so fast. Petal length is just one out of four response variables we could have analysed. What about the other three? Should we run 3 more ANOVAs?\nUnfortunately, performing multiple ANOVAs will run us into a series of problems.\nFirst, we have the multiple comparisons problem. We won’t discuss this topic in detail; but if you want to read up on it, I have linked an article here. 3\n3 Ranganathan, P., Pramesh, C., & Buyse, M. (2016). Common pitfalls in statistical analysis: The perils of multiple testing. Perspectives in Clinical Research, 7(2), 106. https://doi.org/10.4103/2229-3485.179436Second, we have the problem of wasting our time. Really. You should be hanging out with your friends instead of running ANOVAs over and over.\nRemember the plot for petal length from earlier? Check out this plot for petal width:\n\nggplot(iris, aes(x = Species, y = Petal.Width)) +\n  geom_boxplot(colour = 'black', fill = 'lightblue')+\n  theme_classic()\n\n\n\n\n\n\n\n\nLooks… almost the same, right?\nThat is because in our dataset, petal length and petal width are positively correlated – the iris species with long petals also tend to have wide petals. In fact, we could really combine petal width and length into a single characteristic: petal size.\nBy now, we are used to thinking about correlations between explanatory and response variables – these are often the “effects” we look for in our experiments. We have also learned how to handle correlations between multiple explanatory variables – recall our labs in weeks 7-9, where we learned about linear regression.\nHowever, this time we need to deal with correlations between multiple response variables, a situation we have never encountered before.\nHow do we do that? Enter multivariate statistics.\n\n\n\n\n\n\n\n\n\nCity view from the Kuala Lumpar Tower, Malaysia. The magic of photographs lie in their ability to project 3D objects onto 2D planes without losing a sense of reality. From Wikimedia Commons (2018), by itsfatxn.\n\n\nPractice: Take a Picture\nAll forms of multivariate statistics share a common goal: to distill a large collection of response variables into a small set of useful information.\nThis process is called dimension reduction. I like to think of it as using mathematics to create a simple, low-dimensional (usually 2D) picture out of a complex, higher-dimensional (usually 4D+) situation.\nTo push this analogy further, think of all the different tools and techniques artists use to depict the world around them. As science students, we also have a rich collection of tools with which to handle multivariate data.\nPrinciple Component Analysis (PCA) is very much like an artist’s camera. Next week, we will learn about Cluster Analysis, which works more like a paintbrush. In week 12, we will encounter Non-metric Multidimensional Scaling (nMDS), which I like to think of as the pen-and-ink doodles of the statistical world.\n\n\n\n\n\n\n\n\n\nA set of vintage cameras. Photography is one of many ways to compress the 3D world into a 2D image. From Wikimedia Commons (2014), by Jeff Sheldon.\nI say PCA is like a camera, because it reduces the dimension of our sample space by directly projecting each sample point onto a lower dimensional plane. In technical terms, this is known as creating a linear map.\nNot all dimension reduction techniques work this way. Some preserve the relative distance or rank distance between points instead. We will talk more about these ordination techniques in week 12.\n\n\n\n\n\n\nTipHow come my sample points live in multi-dimensional space?\n\n\n\n\n\nEach time we take a sample of anything (plants, soil, fur, etc.) we can measure many different characteristics.\nIn univariate analysis, we only measure one characteristic; for example, soil pH. This gives us a single value per sample, which we can then plot on a number-line. The corresponding analysis will then be an ANOVA.\nIn multivariate analysis, however, we can measure multiple characteristics; for example, soil pH along with soil texture, soil moisture, and soil nutrients. We can’t plot these measurements on a single number line anymore – we need one axis for each measurement.\nThis is how your sample points can end up in multi-dimensional space. If you measure 8 response variables from each of your samples, then your data must occupy an 8-dimensional mathematical space.\nIn the case of the iris dataset, we have 4 response variables. This means our sample points live in 4-dimensional space. We have to find a way to bring them down to 3 or 2-dimensions if we want to plot them nicely.\n\n\n\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nImport the kangaroo.csv dataset using read.csv(), and name it kangaroo.\nCheck the structure of this dataset using str(). Compare it to the structure of the iris dataset.\n\nHow many response variables does kangaroo have? Is this more or fewer than iris? What dimensional space do the each of these datasets occupy?\nRecall that petal width and petal length were positively correlated in iris. Which response variables in kangaroo are positively correlated with each other?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo check which variables are correlated in a dataset, you can use the pairs() function. Try it with the iris dataset first: pairs(iris).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe column names in kangaroo are as follows:\n\nBLEN = basillar length\nPLEN = palatilar length\nOLEN = occipitonasal length\nNLEN = nasal length\nPWID = palate width\nSPECIES = 1: Eastern Grey Kangaroo (M. giganteus), 2: Western Grey Kangaroo (M. fuliginosus)\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFirst, we read in the dataset and check its structure:\n\nkangaroo &lt;- read.csv('data/kangaroo.csv') \nstr(kangaroo)\n\n'data.frame':   27 obs. of  6 variables:\n $ BLEN   : int  1439 1315 1294 1377 1470 1612 1388 1575 1717 1587 ...\n $ PLEN   : int  985 895 872 954 987 1119 936 1100 1184 1115 ...\n $ OLEN   : int  1503 1367 1421 1504 1563 1699 1500 1655 1821 1711 ...\n $ NLEN   : int  629 564 606 660 672 778 616 727 810 778 ...\n $ PWID   : int  230 230 239 248 236 281 227 295 307 293 ...\n $ SPECIES: int  1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nThere are 5 response variables here (BLEN, PLEN, OLEN, NLEN, and PWID), and one explanatory variable (SPECIES). In the iris dataset, we only had 4 response variables.\n\nThis means the sample points in kangaroo occupy 5 dimensional space, while the sample points in iris occupy 4 dimensional space.\nNow, we can look for correlations between our variables. But we have to be a little bit clever here – we don’t want to include our explanatory variable SPECIES in our pairs plot, so we use [,] to select only the first 5 columns before applying pairs():\n\npairs(kangaroo[,1:5])\n\n\n\n\n\n\n\n\nIt looks like just about all of our response variables are positively correlated with each other. This means that kangaroos with longer noses also had longer and wider palattes, etc.\n\n\n\nEarlier, we mentioned that a PCA works by taking a 2D photograph of a multi-dimensional situation. This is only partly true. In reality, the real power of a PCA lies in what it does before it takes this photograph.\n\nThought Experiment\nPicture a bicycle – a real, solid bicycle that you can push around and ride. One day, you are commissioned to take a photograph of this bicycle for an information brochure. This brochure is meant to help people understand how a bicycle works by pointing out its various parts. Which angle would you take your photograph from?\nThis angle?\n\n\n\n\n\n\n\n\n\nA Roadster (Dutch) bicycle, side view. From Wikimedia Commons (2018), by Petar Milošević.\nOr this angle?\n\n\n\n\n\n\n\n\n\nTwo bicycles on 23 Foster St, Boston, United States, front and back views. From Wikimedia Commons (2016), by Myles Tan.\nWhen we reduce the number of dimensions in our data, we inevitably lose information. Even the best photographs cannot show you all sides of an object. Because of this, we need to be very careful which angle we take our photograph from, so as to lose the least amount of information.\nIn the bicycle example, the first angle is the one I would prefer. This is because it shows me more of the bicycle than the second. The second picture is lovely, but it would make a poor addition to any information brochure, because most of the bicycle parts are hidden.\nThe real power of a PCA is in its ability to rotate a multidimensional sample space until it finds the best angle from which to take a photograph of your data points. It does this through either one of two mathematical procedures: eigenvalue decomposition, or singular value decomposition.\nIn our case, we will use the built-in R function prcomp() to perform our PCAs, which is based on singular value decomposition. Do not worry if you are not familiar with singular value decomposition; R will handle that step for you automatically."
  },
  {
    "objectID": "labs/Lab10/Lab10-pca.html#part-2-how-to-perform-a-pca-without-panicking",
    "href": "labs/Lab10/Lab10-pca.html#part-2-how-to-perform-a-pca-without-panicking",
    "title": "Lab 10",
    "section": "Part 2: How to Perform a PCA Without Panicking",
    "text": "Part 2: How to Perform a PCA Without Panicking\n\n\n\n\n\n\n\n\n\nBeer in Berlin. From Wikimedia Commons (2004), by k.ivoutin; Flickr: https://www.flickr.com/photos/ivoutin/1403816845/.\n\nExercise: Go Home, Eigenvalues. You’re Drunk.\nWhat makes a good beer? That was not my attempt to distract you – in fact, it was questions like these that first led psychologists and mathematicians to join forces and develop multivariate techniques. 4\n4 Groenen, P. J. F., Borg, I., Blasius, J., & Greenacre, M. J. (2014). The Past, Present, and Future of Multidimensional Scaling. Visualization and verbalization of data (pp. 96–116). Crc Press, Taylor & Francis Group.5 Ikasari, D. M., & Lestari, E. R. (2019). Analysis of fast food restaurant competition based on consumer perception using multidimensional scaling (MDS) (case study in Malang City, East Java, Indonesia). IOP Conference Series: Earth and Environmental Science, 230. https://doi.org/10.1088/1755-1315/230/1/012060Psychologists realised that people made even very simple decisions, such as which restaurants to visit or what clothes to buy, based on a huge number of interacting variables where the weight of any one variable is unclear. 5\nFor example, if you could walk out of this lab right now (don’t do it) and have a bite to eat, what would you go for? Rice paper rolls? Ramen? Pizza? How many variables might contribute to that decision? Price would be one, for sure. Taste might be another. What about the distance to the nearest Italian restaurant vs the nearest Japanese restaurant?\nSome of these variables might turn out to correlate with one another. For example, you may find that people who prefer restaurants that are nearby also tend to prefer restaurants that are affordable. We can combine these two variables together into one: “accessibility”.\nPCA deals with this by creating new set of response variables called principle components, or PCs – but more on that later.\nI’ll stop tempting you with food for now. Let’s get back to beer instead. Here are seven variables that people normally use to judge the quality of a beer:\n\n\n\n\n\n\nNoteSeven influential variables in a beer\n\n\n\n\n\n\nCost\nBottle size\nAlcohol content\nBrand reputation\nColour\nAroma\nTaste\n\n\n\n\n220 customers were asked to rate each of these variables from 0 to 100 based on how important they judge that variable to be. For example, a customer who scores “Cost” as 0 and “Brand Reputation” as 100 cares more about beer clout than financial security; whereas a customer who scores “Cost” as 100 and “Brand Reputation” as 0 just wants some free booze.\nLet’s take a look at this dataset.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nRead the dataset FactBeer.csv into R and check its structure. Rename the dataset beer.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe can use the read.csv() function to read in our dataset, and the str() function to check its structure.\n\nbeer &lt;- read.csv(\"data/FactBeer.csv\")\nstr(beer)\n\n'data.frame':   220 obs. of  7 variables:\n $ cost   : int  90 75 10 100 20 50 5 65 95 85 ...\n $ size   : int  80 95 15 70 10 100 15 30 95 80 ...\n $ alcohol: int  70 100 20 50 25 100 15 35 100 70 ...\n $ reputat: int  20 50 85 30 35 30 75 80 0 40 ...\n $ color  : int  50 55 40 75 30 90 20 80 80 60 ...\n $ aroma  : int  70 40 30 60 35 75 10 60 70 50 ...\n $ taste  : int  60 65 50 80 45 100 25 90 95 65 ...\n\n\nAll the variables in this dataset are numeric.\n\n\n\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nHow many variables does the beer dataset have? Can we treat all of them as response variables?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis might seem like a trick question at first glance, because how can every variable be a response variable? We need an explanatory variable somewhere, don’t we?\nAs strange as it might seem, we can in fact treat all 7 variables in FactBeer as response variables, and this is exactly what we will do for this exercise. What this means, however, is that our subsequent analyses will be descriptive rather than deductive/inferential.\nResearchers often use descriptive studies to refine their research questions. Then, they follow up on these questions with more rigorous, inferential tests. Think of descriptive statistics as a special case of Exploratory Data Analysis (EDA), if you will.\n\n\n\n\n\n\n\n\n\nWarningHow do I know if my study is multivariate?\n\n\n\nNo strict rule dictates whether your study should be univariate or multivariate. As a researcher, you are free to choose your response and explanatory variables depending on your research question. Whether you decide to choose multiple response variables or not is up to you.\nAs a rule of thumb, whenever you are given a large dataset with more than three numeric variables, be aware that multivariate analysis is an option available to you.\n\n\nOur beer dataset definitely fits the criteria for multivariate analysis. We will treat all 7 variables in this dataset as response variables, and see if we can spot any patterns in them using a PCA.\nTo run the PCA, we use the prcomp() function in R:\n\npca_beer &lt;- prcomp(beer, scale = TRUE)\n\nPCA done – it’s that easy! Everything is over in a flash, just like taking a photograph.\nR stores the results of our PCA as a dataset, from which we need to pull out the very important ‘rotation’ column:\n\n\n\n\n\n\nNoteThe ‘rotation’ column\n\n\n\n\n\n\nround(pca_beer$rotation,6) \n\n              PC1       PC2      PC3       PC4       PC5       PC6       PC7\ncost    -0.302309 -0.454046 0.084258  0.784602 -0.031082 -0.255689 -0.115793\nsize    -0.366651 -0.417510 0.310547 -0.174129  0.213427  0.469571  0.546301\nalcohol -0.347204 -0.432492 0.086555 -0.578538 -0.275153 -0.364266 -0.376567\nreputat  0.404031  0.043831 0.884035  0.018842 -0.101951 -0.204916  0.024101\ncolor   -0.417815  0.356163 0.307245  0.085150  0.056377  0.487470 -0.596761\naroma   -0.404393  0.379468 0.106137 -0.075887  0.595065 -0.541660  0.167180\ntaste   -0.390233  0.399636 0.042278  0.077508 -0.714212 -0.091785  0.402108\n\n# I used the `round()` function to round everything to 6dcp. You don't have to do this.\n\n\n\n\nWe can then see our results by running summary():\n\n\n\n\n\n\nNoteA summary of our PCA results\n\n\n\n\n\n\nsummary(pca_beer) # Tells us how much variance each PC explains.\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8201 1.6173 0.75804 0.48978 0.36668 0.29231 0.19206\nProportion of Variance 0.4733 0.3737 0.08209 0.03427 0.01921 0.01221 0.00527\nCumulative Proportion  0.4733 0.8470 0.92905 0.96332 0.98252 0.99473 1.00000\n\n\n\n\n\nWhat do these numbers mean? We will talk about that in the next section. For now, let’s practice running a few more PCAs using the prcomp() function to get you comfortable with the process.\n\n\n\n\n\n\n\n\n\nA young eastern grey kangaroo (Macropus giganteus) in Majura Nature Reserve, ACT. From Wikimedia Commons (2016), by Thennicke.\n\n\nPractice: You’re Multidimensional (I Think That’s a Compliment)\nYour turn! Let’s bring back the iris and kangaroo datasets.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nRe-open the iris and kangaroo datasets, and check their structure.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nEasy. We named them earlier, so now we can check their structure using the str() function:\n\nstr(iris) # the iris dataset\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nstr(kangaroo) # the kangaroo dataset\n\n'data.frame':   27 obs. of  6 variables:\n $ BLEN   : int  1439 1315 1294 1377 1470 1612 1388 1575 1717 1587 ...\n $ PLEN   : int  985 895 872 954 987 1119 936 1100 1184 1115 ...\n $ OLEN   : int  1503 1367 1421 1504 1563 1699 1500 1655 1821 1711 ...\n $ NLEN   : int  629 564 606 660 672 778 616 727 810 778 ...\n $ PWID   : int  230 230 239 248 236 281 227 295 307 293 ...\n $ SPECIES: int  1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\nNotice that both iris and kangaroo have categorical variables (“Species” for iris and “SPECIES” for kangaroo) as their last columns. Let’s remove these columns before we proceed with our PCA.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nUse the [,] function to remove the last columns of iris and kangaroo respectively. Then, use the &lt;- operator to give these newly-cropped datasets their own names.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nRemember that to filter for columns, we use the space behind the comma [,*]:\n\niris_cropped &lt;- iris[,1:4] # columns 1 to 4 of the iris dataset\nkangaroo_cropped &lt;- kangaroo[,1:5] # columns 1 to 5 of the kangaroo dataset\n\n\n\n\nNow go for it! Run a PCA on the iris dataset.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nUse the prcomp() function to perform a PCA on the iris dataset.\n\n\n\n\n\n\nTip\n\n\n\n\n\nInstead of scale = TRUE, you can use scale = FALSE for this PCA. You don’t have to scale your variables if they are all measured in the same units (in this case, centimeters).\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nOne line of code should do the trick. Remember to use your cropped dataset, not the original iris:\n\npca_iris &lt;- prcomp(iris_cropped, scale = FALSE) # I used \"&lt;-\" to name it \"pca_iris\".\n\n\n\n\n\n\nEasy? Summarise your results.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nSelect the “rotation” column from your PCA, and run summary() on your results.\n\n\n\n\n\n\nTip\n\n\n\n\n\nRemember that you can select columns by name using the $ operator.\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe “rotation” column:\n\npca_iris$rotation\n\n                     PC1         PC2         PC3        PC4\nSepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872\nSepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231\nPetal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390\nPetal.Width   0.35828920  0.07548102 -0.54583143  0.7536574\n\n\nResults summary:\n\nsummary(pca_iris)\n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000\n\n\n\n\n\n\n\nThat’s it! You have just performed your first PCA. Do you think you can do the whole thing by yourself without any hints? Give it a try:\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nPerform a PCA on the kangaroo_cropped dataset, extracting both the “rotation” column and the results summary.\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nCheck the structure of kangaroo_cropped:\n\nstr(kangaroo_cropped) \n\n'data.frame':   27 obs. of  5 variables:\n $ BLEN: int  1439 1315 1294 1377 1470 1612 1388 1575 1717 1587 ...\n $ PLEN: int  985 895 872 954 987 1119 936 1100 1184 1115 ...\n $ OLEN: int  1503 1367 1421 1504 1563 1699 1500 1655 1821 1711 ...\n $ NLEN: int  629 564 606 660 672 778 616 727 810 778 ...\n $ PWID: int  230 230 239 248 236 281 227 295 307 293 ...\n\n\nWe have 5 response variables and they are all numeric, so we can proceed with multivariate analysis.\nPerform the PCA:\n\npca_kangaroo &lt;- prcomp(kangaroo_cropped)\n\nExtract the “rotation” column:\n\npca_kangaroo$rotation\n\n            PC1        PC2         PC3         PC4        PC5\nBLEN 0.58361875 -0.6506053 -0.16256188 -0.03837344 -0.4562927\nPLEN 0.44909213 -0.1860983  0.55797993  0.26346878  0.6188104\nOLEN 0.58327608  0.4672692 -0.34309615 -0.51343366  0.2451927\nNLEN 0.32970748  0.5027396 -0.07404816  0.72415472 -0.3296403\nPWID 0.09374119  0.2664444  0.73418931 -0.37562947 -0.4899883\n\n\nSummarise the results:\n\nsummary(pca_kangaroo)\n\nImportance of components:\n                            PC1     PC2      PC3      PC4      PC5\nStandard deviation     295.5783 34.6311 21.40951 16.50136 13.25427\nProportion of Variance   0.9765  0.0134  0.00512  0.00304  0.00196\nCumulative Proportion    0.9765  0.9899  0.99499  0.99804  1.00000\n\n\n\n\n\nHow did you go?"
  },
  {
    "objectID": "labs/Lab10/Lab10-pca.html#part-3-admiring-your-handiwork",
    "href": "labs/Lab10/Lab10-pca.html#part-3-admiring-your-handiwork",
    "title": "Lab 10",
    "section": "Part 3: Admiring Your Handiwork",
    "text": "Part 3: Admiring Your Handiwork\n\n\n\n\n\n\n\n\n\nBoule, a classic French bread. Bakers take a long time to prepare their loaves beforehand, even though the baking process itself is taken care of by their ovens. Bakers must take their loaves out of their ovens at the right time to slice, decorate, and package each loaf nicely for their customers. We should do the same with our PCA results. From Wikimedia Commons (2014), by Lehightrails.\nNow that the technical part is done, let’s take some time to admire what we have created.\nWe will revisit the results of our beer dataset and see what they tell us.\n\nExercise: New Year, New Me, New Variables\nAs a reminder, here are the results from our beer PCA:\n\n\n\n\n\n\nNoteThe “rotation” column, rounded to 6dcp\n\n\n\n\n\n\nround(pca_beer$rotation,6) \n\n              PC1       PC2      PC3       PC4       PC5       PC6       PC7\ncost    -0.302309 -0.454046 0.084258  0.784602 -0.031082 -0.255689 -0.115793\nsize    -0.366651 -0.417510 0.310547 -0.174129  0.213427  0.469571  0.546301\nalcohol -0.347204 -0.432492 0.086555 -0.578538 -0.275153 -0.364266 -0.376567\nreputat  0.404031  0.043831 0.884035  0.018842 -0.101951 -0.204916  0.024101\ncolor   -0.417815  0.356163 0.307245  0.085150  0.056377  0.487470 -0.596761\naroma   -0.404393  0.379468 0.106137 -0.075887  0.595065 -0.541660  0.167180\ntaste   -0.390233  0.399636 0.042278  0.077508 -0.714212 -0.091785  0.402108\n\n\n\n\n\n\n\n\n\n\n\nNoteResults Summary\n\n\n\n\n\n\nsummary(pca_beer) \n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8201 1.6173 0.75804 0.48978 0.36668 0.29231 0.19206\nProportion of Variance 0.4733 0.3737 0.08209 0.03427 0.01921 0.01221 0.00527\nCumulative Proportion  0.4733 0.8470 0.92905 0.96332 0.98252 0.99473 1.00000\n\n\n\n\n\nLet’s interpret them one at a time.\n\nThe Loadings Table\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nTake a look at pca_beer$rotation. What do you think the numbers stand for?\n\n\nThe “rotation” column of pca_beer is also called its loadings table. We will refer to it as such going forward.\nThe easiest way to read a loadings table is to consider one column at a time. Each column in this table is a principle component, and the numbers in that column tells you which combination of variables went into creating that component.\nYou can think of these principle components as a new set of axes.\nRemember that PCAs are good for finding the best angle from which to view multidimensional datasets. The 7 original variables in FactBeer (cost, reputation, etc.) represented our original point of view, while the 7 PCs (PC1, PC2, etc.) represent our new point of view.\nHowever, these new axes did not come out of nothing. The PCA used combinations from our 7 original variables to create them.\nWhich combinations? The loadings table will tell us.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nLook at the fourth column in our loadings table. Which of our 7 original variables went into creating PC4?\nAre some number in the column larger than others? What do you think this means?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAll 7 original variables went into creating PC4. However, some variables contributed more than others.\nFor instance, cost and alcohol contributed the most (with loadings of +0.78 and -0.58 respectively), while reputation contributed the least (with a loading of only 0.02).\nThis means that PC4 represents a preference for using cost as a way to judge beer quality (+ve loading for cost), and a preference against using alcohol levels to do the same (-ve loading for alcohol), but does not have much to do with reputation either way (~0 loading for reputation).\n\n\n\n\nGiving names to our PCs\nFor the moment, our principle components are named: “PC1, PC2, PC3, etc.”; but we can give them new names based on our loadings table.\nYou can be creative with these names – as long as they make sense. For example, let’s take a look at PC1.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nLook at the first column in our loadings table. Which of our original variables went into creating PC1? Which ones contributed the most? Are their loadings positive or negative?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nReputation contributed positively to PC1 (+0.40), but every other variable contributed negatively. The strength of the contributions are spread quite evenly between the variables.\n\n\n\nHere, we see that PC1 is made up of a positive contribution from ‘reputation’ combined with negative contributions from the other 6 variables.\nThis means that customers who scored high on the PC1 axis thought reputation was an important signal of beer quality (+ve loading), but every other variable was unimportant (-ve loading). On the other hand, customers who scored low on the PC1 axis thought precisely the opposite.\nIn other words, PC1 separated customers who valued reputation from customers who valued other factors instead.\nTherefore, we can name PC1 something like “pretentiousness”.\n\n\n\n\n\n\nNoteBuilding Habits\n\n\n\nWhat would you name the other six PCs?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nHere’s what I went with for PCs 1 to 4:\nPC2 = “drunk for buck”. This axis separates customers who prefer to judge a beer on its taste, colour, and aroma from those who prefer to get as much alcohol as possible for the least amount of money.\nPC3 = “aesthetic appeal”. This axis separates customers who value the reputation, size, and colour of beer from those who are indifferent to these characteristics.\nPC4 = “financial and moral consciousness”. This axis separates customers who value money from those who value alcohol.\nI will leave the rest up to you!\n\n\n\nAt this point, you may have noticed a problem. If the point of a PCA is to reduce the number of variables in our study, why do we still have 7 PCs?\nIt seems that while we did successfully rotate our dataset to a new perspective, we are still working in 7D space.\nThe good news is that going from 7D space to 2D space is actually very easy now that we have our PCs. The photograph has already been taken – all that is left is to print it out.\n\n\n\n\n\n\n\n\n\nStony deposits, or scree, at the base of a mountain. There is a sudden change in slope as solid rock transitions into unconsolidated material. From Wikimedia Commons (2013), by Sergey Pashko.\n\n\n\nThe Scree Plot\nOne very important thing to realise about our PCs is that they are ranked by the amount of variance they explain. In other words, sample points are more spread out along the PC1 axis than along the PC2 axis, and more spread out along the PC2 axis than along the PC3 axis, etc.\nThink back to the bicycle pictures from earlier. A bike is more ‘spread out’ in its side view, with all its features visible. On the other hand, these same features would be squished together in a front view.\nPC1 is like the side view of our data, while PC7 is like the front view. Along PC1, we can see more of our data because it is more spread out; along PC7, the same data looks like a congested cloud.\n\n\n\n\n\n\n\n\n\nOur dataset with principle components ‘PC1’ and ‘PC2’ as the two viewing axes. Notice how the data is nicely spread out.\n\n\n\n\n\n\n\n\n\nThe same dataset with principle components ‘PC6’ and ‘PC7’ as the two viewing axes. Notice how congested the data becomes.\nIf we remove the PC1 axis from our analysis, we will lose a lot of information. If we remove PC7, however, we will not lose much information at all.\nThis is the key to dimension reduction – getting rid of all the bad photography angles, and keeping only the good ones.\nLet’s use a scree plot to decide which PCs we should keep, and which ones we should remove.\n\n\n\n\n\n\nNoteSharpen your skills\n\n\n\nTake a look at the results summary of pca_beer. You should see a row in this table called “Proportion of Variance”.\nHow much variance do each of the PCs explain? Which ones do you think we can remove without losing too much information?\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nTo remind you, here is the summary for pca_beer we extracted earlier:\n\nsummary(pca_beer)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8201 1.6173 0.75804 0.48978 0.36668 0.29231 0.19206\nProportion of Variance 0.4733 0.3737 0.08209 0.03427 0.01921 0.01221 0.00527\nCumulative Proportion  0.4733 0.8470 0.92905 0.96332 0.98252 0.99473 1.00000\n\n\nPC1 explains around 47% of the variance, while PC2 explains 37%. The two of them explain almost 85% of the variance together. This means that we can remove PCs 3 to 7 and still retain a lot of information about our dataset.\nA scree plot will show us the same information in visual form.\n\n\n\nTo make a scree plot, we will use base R instead of ggplot (one of the few instances where I recommend base R over ggplot). The function we will use is called screeplot().\n\nscreeplot(pca_beer, type = \"lines\")\n\n\n\n\n\n\n\n\nSee the big drop-off after PC2? This is called the elbow of the scree plot. We can remove every PC after the elbow from our analysis; in this case, PCs 3 to 7.\nAnother common protocol for removing PCs is Kaiser’s criterion, which says to remove every PC lower than 1 on the y-axis of your screeplot. In our case, that still means removing PCs 3 to 7.\nIn most situations, I prefer the elbow protocol anyway.\nAt this point, we can consider our exploratory PCA more or less complete. Out of 7 original variables, we created 2 new ones. These 2 new variables explain almost as much variation as the original 7, but take up far fewer dimensions.\nPretty neat, right?\n\n\n\n\n\n\nWarningOther forms of Factor Analysis\n\n\n\nPCA is one of many techniques that fall under the broad category of factor analysis. Factor analysis, in turn, is a subtopic under multivariate statistics.\nFor your interest, here is a different function in R (called factanal) that executes a slightly different type of factor analysis using a type of rotation called “varimax rotation”. (Note: varimax rotation can be applied to PCAs as well, but we chose not to do so.)\nThe outputs of this factor analysis are similar to those of a PCA; but instead of creating PCs, factanal creates latent variables (also called factors).\nHere is factanal applied to our beer dataset. See if you can spot some key differences between the loadings tables of this analysis and the PCA we performed earlier.\n\n\n\n\n\n\nNoteFactor analysis with Varimax rotation, applied to our beer dataset\n\n\n\n\n\n\nfa1 &lt;- factanal(beer, 3, rotation=\"varimax\")\nfa1$loadings\n\n\nLoadings:\n        Factor1 Factor2 Factor3\ncost             0.839   0.141 \nsize             0.992         \nalcohol          0.904   0.255 \nreputat -0.342  -0.348  -0.629 \ncolor    0.994                 \naroma    0.913           0.173 \ntaste    0.913           0.261 \n\n               Factor1 Factor2 Factor3\nSS loadings      2.783   2.640   0.578\nProportion Var   0.398   0.377   0.083\nCumulative Var   0.398   0.775   0.857\n\n\n\n\n\n\n\n\n\n\n\nTipWhat you might notice\n\n\n\n\n\nIt seems that the factors from our factor analysis are a little bit easier to interpret than the PCs from our PCA. This is because the varimax rotation constructed each factor so that they are influenced by only a few of the original variables.\nRecall that in our PCA, all 7 original variables contributed to PC1 in roughly equal proportions. In our factor analysis, however, Factor 1 is made up mostly of colour, aroma, and taste, with very little contribution from the other four variables.\nSometimes this is a good thing, sometimes not. For more on this topic, please see the week 10 lectures.\n\n\n\n\n\n\n\n\nPractice: Sparrows\nWe have come to the end of the lesson, so now it is time to explore a multivariate dataset on your own from start to finish.\nYour demonstrators will be there to help you, and so will your classmates. We strongly encourage you to collaborate with your peers on this exercise.\n\n\n\n\n\n\n\n\n\nHouse Sparrow (Passer domesticus), a small bird native to the Northern Hemisphere. House sparrows are so named for their ability to live in urban settings. From Wikimedia Commons (2004), by Mathias Appel.\n\nThe sparrow dataset\nIn 1898, after a heavy storm, Professor Hermon Bumpus of Brown University came across a collection of house sparrows that had been injured in the storm. 6 Some of these sparrows were dead, but others had just about survived.\n6 Johnson, R. F., Niles, D. M., & Rohwer, S. A. (1972). Hermon Bumpus and Natural Selection in the House Sparrow Passer domesticus. Evolution, 26(1), 20. https://doi.org/10.2307/2406980Professor Bumpus wondered whether he could learn something about natural selection by studying these sparrows. In his study, he used a series of univariate analyses.\nYour task is to examine the same dataset using multivariate techniques.\n\n\n\n\n\n\nNoteCase study\n\n\n\nThe name of the dataset is “bumpus_sparrows_clean.csv”. Load this dataset into R, and analyse it using multivariate techniques.\nWe will leave it up to you to decide which variables to analyse and how to analyse them. Share your ideas with your classmates, and see what they think.\n\n\nThat’s all for this week! We hope you had fun :)\nThanks!"
  },
  {
    "objectID": "labs/Lab12/Lab12-nmds.html",
    "href": "labs/Lab12/Lab12-nmds.html",
    "title": "nMDS and PERMANOVA",
    "section": "",
    "text": "Quinn, G. P. and Keough, M. J. (2002, 2023) Experimental Design and Data Analysis for Biologists. Cambridge University Press (1st  and 2nd edition).\nHan, S. Y., Filippi, P., Román Dobarco, M., Harianto, J., Crowther M. S., and Bishop, T. F. A. (2023). Multivariate analysis for soil science. In ‘Encyclopedia of Soils in the Environment (Second Edition)’. (Ed. M. J. Goss and M. Oliver) pp. 499-508. Academic Press: Oxford."
  },
  {
    "objectID": "labs/Lab12/Lab12-nmds.html#readings",
    "href": "labs/Lab12/Lab12-nmds.html#readings",
    "title": "nMDS and PERMANOVA",
    "section": "",
    "text": "Quinn, G. P. and Keough, M. J. (2002, 2023) Experimental Design and Data Analysis for Biologists. Cambridge University Press (1st  and 2nd edition).\nHan, S. Y., Filippi, P., Román Dobarco, M., Harianto, J., Crowther M. S., and Bishop, T. F. A. (2023). Multivariate analysis for soil science. In ‘Encyclopedia of Soils in the Environment (Second Edition)’. (Ed. M. J. Goss and M. Oliver) pp. 499-508. Academic Press: Oxford."
  },
  {
    "objectID": "labs/Lab12/Lab12-nmds.html#learning-outcomes",
    "href": "labs/Lab12/Lab12-nmds.html#learning-outcomes",
    "title": "nMDS and PERMANOVA",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nAt the end of this practical students should be able to:\n1.          Calculate a similarity matrix or transformed data\n2.          Plot an ordination using Non-metric Multidimensional Scaling (nMDS)\n3.          Statistically test differences between factors using a ANOSIM and PERMANOVA\n4.          Determine what contributes most to the differences using a SIMPER analysis\nWorking directory Set the working directory for this tutorial and ensure you copy all data into this directory and save your code into the directory. setwd(“C:/Users/.”)"
  },
  {
    "objectID": "labs/Lab12/Lab12-nmds.html#exercise-1-copepod-communities-in-norway",
    "href": "labs/Lab12/Lab12-nmds.html#exercise-1-copepod-communities-in-norway",
    "title": "nMDS and PERMANOVA",
    "section": "Exercise 1: Copepod Communities in Norway",
    "text": "Exercise 1: Copepod Communities in Norway\nIt is thought that copepod (a marine invertebrate) assemblages in Solbergstrand, Norway are affected by nutrient levels. There were 4 spatially independent sites for each treatment in;\na)         Control sites (C),\nb)         Low nutrient sites(L) and\nc)          High nutrient sites (H).\nWe want to know if there is a pattern in species assemblages using nMDS, whether there is a statistically significant difference between treatments using ANOSIM and what species contribute to these differences via SIMPER.\nFirst you need to load the packages vegan and MASS and file CopepodData.csv\n\nlibrary(vegan)\n\nLoading required package: permute\n\nlibrary(MASS)\n\nCopepodData &lt;- read.csv(\"data/CopepodData.csv\", header = TRUE)\n\nNow the data is entered, let’s apply a 4th root transformation. This will take the emphasis from the more common species. Other transformations include the square root transformation and the log transformation. Other transformations included square root, presence/absence and log. You can also standardise the data if there are differences in the amount of sampling (which there isn’t here).\n\nTransCopepodData &lt;- (CopepodData[,2:20])^(1/4)\n\nNow let’s generate a Bray-Curtis dissimilarity matrix on this transformed data and perform a nMDS. Bray-Curtis similarities are useful for this kind of data, as they did not group sites by shared absences. For environmental data, a Euclidean distance would be more useful.\n\ncopepod.dis &lt;- vegdist(TransCopepodData, method=\"bray\")\n\nNow let’s look at the nMDS with labels for Community and take a note of the stress value (how well the ordination represents the data). A great stress value is &lt;0.1, an OK stress value is between 0.1 and 0.2, and &gt;0.2 is not so good. A stress greater than 0.3 is essentially arbitrary.\nNext we plot the nMDS, labelling the sites with treatment.\n\npchs&lt;- c(0:2)\nCopepod_Factor &lt;- factor(CopepodData$Treatment)\nnMDS_copepod &lt;- metaMDS(TransCopepodData, distance=\"bray\", k=2) \n\nRun 0 stress 0.1003895 \nRun 1 stress 0.09313703 \n... New best solution\n... Procrustes: rmse 0.06920897  max resid 0.1880611 \nRun 2 stress 0.1145506 \nRun 3 stress 0.1145514 \nRun 4 stress 0.09313704 \n... Procrustes: rmse 4.172137e-05  max resid 6.123411e-05 \n... Similar to previous best\nRun 5 stress 0.09313703 \n... Procrustes: rmse 5.264303e-06  max resid 8.51057e-06 \n... Similar to previous best\nRun 6 stress 0.1102998 \nRun 7 stress 0.1003895 \nRun 8 stress 0.1003895 \nRun 9 stress 0.3359232 \nRun 10 stress 0.110277 \nRun 11 stress 0.09313703 \n... Procrustes: rmse 2.20052e-05  max resid 3.183964e-05 \n... Similar to previous best\nRun 12 stress 0.09313703 \n... Procrustes: rmse 3.655204e-06  max resid 5.320949e-06 \n... Similar to previous best\nRun 13 stress 0.1003895 \nRun 14 stress 0.09313703 \n... Procrustes: rmse 6.41951e-06  max resid 1.051411e-05 \n... Similar to previous best\nRun 15 stress 0.09313703 \n... Procrustes: rmse 5.993992e-06  max resid 1.227241e-05 \n... Similar to previous best\nRun 16 stress 0.09313703 \n... Procrustes: rmse 1.73614e-05  max resid 2.691538e-05 \n... Similar to previous best\nRun 17 stress 0.2236026 \nRun 18 stress 0.2260812 \nRun 19 stress 0.110277 \nRun 20 stress 0.1003895 \n*** Best solution repeated 7 times\n\nplot_copepod &lt;- ordiplot(nMDS_copepod, display = \"sites\", type=\"n\") \npoints(nMDS_copepod, col=\"black\", pch = pchs[Copepod_Factor])\nlegend(\"topright\", bty = \"n\", legend = levels(Copepod_Factor), pch = pchs)\n\n\n\n\n\n\n\nnMDS_copepod\n\n\nCall:\nmetaMDS(comm = TransCopepodData, distance = \"bray\", k = 2) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     TransCopepodData \nDistance: bray \n\nDimensions: 2 \nStress:     0.09313703 \nStress type 1, weak ties\nBest solution was repeated 7 times in 20 tries\nThe best solution was from try 1 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'TransCopepodData' \n\n\nExercise: Can you see a separation between treatments for copepod assemblages?\nExercise: What is the stress and is it good?\nLet’s test if the groups are significantly different using ANOSIM (Analysis of Similarities), a non-parametric permutation test. We will use 999 permutations to calculate the significance level.\nSo, let’s test for Treatment\n\ncopepod.anosim &lt;- with(CopepodData, anosim(copepod.dis, CopepodData$Treatment))\nsummary(copepod.anosim)\n\n\nCall:\nanosim(x = copepod.dis, grouping = CopepodData$Treatment) \nDissimilarity: bray \n\nANOSIM statistic R: 0.8495 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.206 0.269 0.341 0.412 \n\nDissimilarity ranks between and within classes:\n        0%   25%  50%   75% 100%  N\nBetween 12 28.50 42.5 54.25   66 48\nC        1  6.50  9.5 16.25   23  6\nH        7 18.25 25.0 29.50   32  6\nL        2  3.25  4.5  8.00   10  6\n\nplot(copepod.anosim)\n\nWarning in (function (z, notch = FALSE, width = NULL, varwidth = FALSE, : some\nnotches went outside hinges ('box'): maybe set notch=FALSE\n\n\n\n\n\n\n\n\n\nIs there a significant difference between the different treatments? We can see that using a brand-new ANOSIM code.\n\n# Extract a grouping factor (e.g., Management type)\ngroup &lt;- CopepodData$Treatment\n\n# Perform ANOSIM for all groups combined\nanosim_all &lt;- anosim(copepod.dis, group)\nprint(anosim_all)\n\n\nCall:\nanosim(x = copepod.dis, grouping = group) \nDissimilarity: bray \n\nANOSIM statistic R: 0.8495 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\npairwise.anosim &lt;- function(copepod.dis, group, p.adjust.method = \"holm\", ...) {\n  # Ensure grouping is a factor\n  group &lt;- factor(group)\n  groups &lt;- levels(group)\n  \n  # Generate all unique pairs of group levels\n  pairs &lt;- t(combn(groups, 2))\n  \n  # Prepare storage for results\n  results &lt;- data.frame(\n    group1  = character(),\n    group2  = character(),\n    R       = numeric(),\n    p.value = numeric(),\n    stringsAsFactors = FALSE\n  )\n  \n  # Loop through each pair\n  for (i in seq_len(nrow(pairs))) {\n    g1 &lt;- pairs[i, 1]\n    g2 &lt;- pairs[i, 2]\n    \n    # Subset the data to just these two groups\n    keep &lt;- group %in% c(g1, g2)\n    dist_sub &lt;- as.dist(as.matrix(copepod.dis)[keep, keep])\n    group_sub &lt;- droplevels(group[keep])\n    \n    # Run ANOSIM\n    anosim_res &lt;- anosim(dist_sub, group_sub, ...)\n    \n    # Store results\n    results &lt;- rbind(\n      results,\n      data.frame(\n        group1  = g1,\n        group2  = g2,\n        R       = anosim_res$statistic,\n        p.value = anosim_res$signif,\n        stringsAsFactors = FALSE\n      )\n    )\n  }\n  \n  # Adjust p-values for multiple comparisons\n  results$p.adjusted &lt;- p.adjust(results$p.value, method = p.adjust.method)\n  \n  return(results)\n}\n\npairwise_results &lt;- pairwise.anosim(copepod.dis, group, p.adjust.method = \"holm\")\npairwise_results\n\n  group1 group2       R p.value p.adjusted\n1      C      H 0.96875   0.024      0.072\n2      C      L 1.00000   0.028      0.072\n3      H      L 0.59375   0.030      0.072\n\n\nExercise: Was there a significant difference between treatments for copepod assemblages? What was the Global R value? Were there significant differences between sites?\nTo see what species of copepods are contributing to the differences between treatments, we can use a SIMPER analysis (Similarity Percentages). The average is the amount that species contributes to the dissimilarity between the groups. The sd is the standard devation (i.e the variation) of the average dissimilarity contribution. The ratio is the average dissimilarity/sd. The ava and avb are the average abundances of that species to the group. The cumsum is the ordered cumulative contribution to the dissimilarity between groups. Note we can ignore the cumulative 70%.\nNote we use the ratio as the rank for the species that mostly separate the groups.\nIf R approaches 1 the dissimilarity between groups is greater than the dissimilarity within groups, if R is close to 0 the dissimilarity within groups is about the same as the dissimilarity between groups, and if R approaches -1, the variation within groups is greater than the dissimilarity between groups.\n\ncopepod.simper &lt;- simper(TransCopepodData, CopepodData$Treatment)\nsummary(copepod.simper)\n\n\nContrast: C_L \n\n                average       sd    ratio      ava      avb cumsum     p   \nTisbe.sp.4      0.12602  0.01742  7.23200  0.00000  3.64100  0.189 0.003 **\nTisbe.sp.2      0.08194  0.00696 11.77800  0.00000  2.37300  0.311 0.008 **\nTisbe.sp.3      0.07820  0.04708  1.66100  0.00000  2.28200  0.428 0.046 * \nTisbe.sp.5      0.05630  0.05839  0.96400  0.00000  1.66100  0.513 0.313   \nHalect.gothic   0.05107  0.02030  2.51600  0.50000  1.96800  0.589 0.043 * \nAmeira.parvula  0.04409  0.00342 12.89100  0.00000  1.27700  0.655 0.009 **\nCyclopoida      0.02920  0.01798  1.62400  0.84460  0.00000  0.699 0.180   \nCopepodit.ind   0.02868  0.01653  1.73500  0.25000  1.07900  0.742 0.149   \nStenhelia.refl  0.02527  0.01845  1.37000  0.82900  0.29700  0.780 0.420   \nAmphiascus.ten  0.02428  0.02342  1.03700  0.25000  0.68900  0.816 0.209   \nProameira.simp  0.02367  0.02489  0.95100  0.00000  0.67100  0.852 0.007 **\nEnhydros.long   0.02296  0.01856  1.23700  1.14190  0.57900  0.886 0.841   \nAncorab.mirab   0.02015  0.02351  0.85700  1.03610  1.47000  0.916 0.880   \nBulbamph.imus   0.01802  0.01877  0.96000  0.54730  0.00000  0.943 0.398   \nDaniel.fusifo   0.01324  0.00931  1.42200  1.00000  1.37300  0.963 0.906   \nLaophontidae    0.00891  0.01597  0.55800  0.00000  0.25000  0.976 0.013 * \nLeptopsy.para   0.00873  0.01563  0.55800  0.25000  0.00000  0.990 0.704   \nTyphlam.typhl   0.00703  0.00550  1.27800  1.30170  1.47700  1.000 0.999   \nTisbe.sp.1      0.00000  0.00000      NaN  0.00000  0.00000  1.000    NA   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: C_H \n\n               average      sd   ratio     ava     avb cumsum     p    \nTisbe.sp.1     0.12838 0.05504 2.33200 0.00000 2.28280  0.155 0.001 ***\nTisbe.sp.3     0.07987 0.05554 1.43800 0.00000 1.48270  0.251 0.041 *  \nTyphlam.typhl  0.07134 0.01048 6.80800 1.30170 0.00000  0.337 0.001 ***\nTisbe.sp.2     0.06684 0.04205 1.59000 0.00000 1.31490  0.418 0.032 *  \nEnhydros.long  0.06297 0.01168 5.39100 1.14190 0.00000  0.494 0.001 ***\nTisbe.sp.5     0.06165 0.04244 1.45300 0.00000 1.23230  0.568 0.118    \nTisbe.sp.4     0.04869 0.05131 0.94900 0.00000 1.02410  0.627 0.991    \nStenhelia.refl 0.04526 0.02867 1.57900 0.82900 0.00000  0.681 0.008 ** \nAmeira.parvula 0.04380 0.02667 1.64200 0.00000 0.84460  0.734 0.009 ** \nAncorab.mirab  0.04305 0.03671 1.17300 1.03610 1.14890  0.786 0.106    \nCyclopoida     0.03908 0.03005 1.30000 0.84460 0.25000  0.833 0.031 *  \nDaniel.fusifo  0.03472 0.02934 1.18300 1.00000 0.57900  0.875 0.023 *  \nBulbamph.imus  0.02783 0.02938 0.94700 0.54730 0.00000  0.909 0.045 *  \nHalect.gothic  0.02726 0.02902 0.93900 0.50000 0.25000  0.942 0.999    \nCopepodit.ind  0.02182 0.02962 0.73700 0.25000 0.25000  0.968 0.661    \nLeptopsy.para  0.01395 0.02527 0.55200 0.25000 0.00000  0.985 0.135    \nAmphiascus.ten 0.01277 0.02308 0.55300 0.25000 0.00000  1.000 0.813    \nProameira.simp 0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA    \nLaophontidae   0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: L_H \n\n               average      sd   ratio     ava     avb cumsum     p  \nTisbe.sp.4     0.08575 0.04324 1.98300 3.64100 1.02410  0.149 0.082 .\nTisbe.sp.1     0.07326 0.02796 2.62000 0.00000 2.28280  0.276 0.091 .\nHalect.gothic  0.05378 0.01410 3.81500 1.96800 0.25000  0.370 0.019 *\nTisbe.sp.5     0.05238 0.03401 1.54000 1.66100 1.23230  0.460 0.476  \nTisbe.sp.3     0.04921 0.03207 1.53500 2.28200 1.48270  0.546 0.892  \nTyphlam.typhl  0.04684 0.00580 8.07600 1.47700 0.00000  0.627 0.030 *\nTisbe.sp.2     0.03665 0.02956 1.24000 2.37300 1.31490  0.691 0.994  \nDaniel.fusifo  0.02801 0.02171 1.29000 1.37300 0.57900  0.740 0.184  \nCopepodit.ind  0.02569 0.01470 1.74700 1.07900 0.25000  0.784 0.297  \nAmphiascus.ten 0.02244 0.02375 0.94500 0.68900 0.00000  0.823 0.369  \nAncorab.mirab  0.02243 0.01715 1.30700 1.47000 1.14890  0.862 0.855  \nProameira.simp 0.02169 0.02288 0.94800 0.67100 0.00000  0.900 0.070 .\nEnhydros.long  0.01780 0.01868 0.95300 0.57900 0.00000  0.930 0.991  \nAmeira.parvula 0.01457 0.01799 0.81000 1.27700 0.84460  0.956 1.000  \nStenhelia.refl 0.00885 0.01589 0.55700 0.29700 0.00000  0.971 0.986  \nCyclopoida     0.00843 0.01509 0.55800 0.00000 0.25000  0.986 0.996  \nLaophontidae   0.00816 0.01466 0.55700 0.25000 0.00000  1.000 0.182  \nBulbamph.imus  0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA  \nLeptopsy.para  0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 999\n\n\nExercise: Which species most contribute to the separation between treatments?"
  },
  {
    "objectID": "labs/Lab12/Lab12-nmds.html#exercise-2-analysis-of-ant-assemblages-from-different-sydney-vegetation-communties",
    "href": "labs/Lab12/Lab12-nmds.html#exercise-2-analysis-of-ant-assemblages-from-different-sydney-vegetation-communties",
    "title": "nMDS and PERMANOVA",
    "section": "Exercise 2: Analysis of ant assemblages from different Sydney Vegetation Communties",
    "text": "Exercise 2: Analysis of ant assemblages from different Sydney Vegetation Communties\nThis is a study of the diversity of ants found in bushland remnants in the Sydney region. The ants were sampled with pitfall traps in the ground, and taken back to the lab to determine their species and relative abundance in each sample.\nIn this example we are looking at a  collection of ants sampled in different vegetation communities (“Community”) and whether they were sampled in the interior or on the edge of the vegetation community (“Sample”). We expected that the abundance and diversity of ants (the assemblage) would be different in different vegetation communities, and that the diversity of ants would be different in the interior to the edge of the vegetation community.\nThis leads to three multivariate null hypotheses.\nH0: There is no difference in ant assemblage between the different vegetation communities.\nH0: There is no difference in ant assemblage between the interior and the edge of the community.\nH0: There is no interaction between the vegetation community and the location for the ant assemblages\nWe want to know if there is a pattern in species assemblages using nMDS, whether there is a statistically significant difference between communities and samples using PERMANOVA, and what species contribute to these differences via SIMPER.\nFirst you need to load the packages vegan and MASS and file AntData.csv,\n\nlibrary(vegan)\nlibrary(MASS)\n\nAntData &lt;- read.csv(\"data/AntDataTotal.csv\", header = TRUE)\n\nNow the data is entered, let’s apply a 4th root transformation, like in the example above,\n\nTransAntData &lt;- (AntData[,4:103])^(1/4)\n\nNow let’s generate a Bray-Curtis dissimilarity matrix on this transformed data.\n\nant.dis &lt;- vegdist(TransAntData, method = \"bray\")\n\nNow let’s look at the nMDS with labels for Community and take a note of the stress value (how well the ordination represents the data). A great stress value is &lt;0.1, an OK stress value is between 0.1 and 0.2, and &gt;0.2 is not so good. A stress greater than 0.3 is essentially arbitrary.\n\npchs&lt;- c(0:5)\n\nAnt_Factor &lt;- factor(AntData$Community)\nnMDS_Ant &lt;- metaMDS(TransAntData, distance=\"bray\", k=2)\n\nRun 0 stress 0.2869097 \nRun 1 stress 0.2895111 \nRun 2 stress 0.2903446 \nRun 3 stress 0.2871656 \n... Procrustes: rmse 0.03721497  max resid 0.162524 \nRun 4 stress 0.299054 \nRun 5 stress 0.3020263 \nRun 6 stress 0.2879711 \nRun 7 stress 0.2889032 \nRun 8 stress 0.2867978 \n... New best solution\n... Procrustes: rmse 0.04581971  max resid 0.1499163 \nRun 9 stress 0.2887396 \nRun 10 stress 0.294087 \nRun 11 stress 0.2987492 \nRun 12 stress 0.2934602 \nRun 13 stress 0.2902736 \nRun 14 stress 0.2856511 \n... New best solution\n... Procrustes: rmse 0.07007276  max resid 0.2494061 \nRun 15 stress 0.2893781 \nRun 16 stress 0.3052214 \nRun 17 stress 0.2880005 \nRun 18 stress 0.2875764 \nRun 19 stress 0.2956403 \nRun 20 stress 0.2943664 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n     3: no. of iterations &gt;= maxit\n    17: stress ratio &gt; sratmax\n\nplot_Ant &lt;- ordiplot(nMDS_Ant, display = \"sites\", type=\"n\") \npoints(nMDS_Ant, col=\"black\", pch = pchs[Ant_Factor])\nlegend(\"topright\", bty = \"n\", legend = levels(Ant_Factor), pch = pchs)\n\n\n\n\n\n\n\nnMDS_Ant\n\n\nCall:\nmetaMDS(comm = TransAntData, distance = \"bray\", k = 2) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     TransAntData \nDistance: bray \n\nDimensions: 2 \nStress:     0.2856511 \nStress type 1, weak ties\nBest solution was not repeated after 20 tries\nThe best solution was from try 14 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'TransAntData' \n\n\nExercise: Can you see a separation between communties?\nExercise: What is the stress and is it good?\nWe can test for differences between Communities, Samples and their Interactions using PERMANOVA (called Adonis in the package vegan). This is better for analyses with at least 2 factors than ANOSIM, as it can test the interactions. We use 999 permutations to calculate the significance (p) value. You interpret the PERMANOVA table like an ANOVA Table, except permutations are used to generate the p value,\n\n# PERMANOVA with interaction using adonis2\nadonis2(vegdist(TransAntData) ~ Community * Sample,\n        data = AntData,\n        permutations = 999,\n        by = \"terms\")\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = vegdist(TransAntData) ~ Community * Sample, data = AntData, permutations = 999, by = \"terms\")\n                 Df SumOfSqs      R2      F Pr(&gt;F)    \nCommunity         4   2.8190 0.13512 2.6985  0.001 ***\nSample            1   0.1918 0.00919 0.7345  0.744    \nCommunity:Sample  4   0.6160 0.02953 0.5897  0.992    \nResidual         66  17.2367 0.82616                  \nTotal            75  20.8636 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nExercise: Is there a significant difference for the Community, Sample and/or their interaction?\nIf we find an overall significant p value, we need to see which level is significant from which other level. Here we can do pairwise tests (a bit like post-hoc tests in ANOVAs), with the p value adjusted with a Bonferroni correction. Here we need to create a function to do this, as it is not standard in the vegan package. So copy and paste this function.\nTo do the pairwise tests, there is a new R command on github and you can copy this text below to do it. This is for the Community factor in the ant data.\nNote the first time you do this, you need to first install Rtools from here https://cran.r-project.org/bin/windows/Rtools/\n\nlibrary(devtools)\n\nLoading required package: usethis\n\n\n\nAttaching package: 'devtools'\n\n\nThe following object is masked from 'package:permute':\n\n    check\n\ninstall_github(\"pmartinezarbizu/pairwiseAdonis/pairwiseAdonis\")\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'pairwiseAdonis' from a github remote, the SHA1 (cb190f76) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(pairwiseAdonis)\n\nLoading required package: cluster\n\npairwise.adonis(TransAntData, AntData$Community)\n\n          pairs Df SumsOfSqs  F.Model         R2 p.value p.adjusted sig\n1  BGHF vs CCIF  1 1.1326581 4.532465 0.13516648   0.001       0.01   *\n2   BGHF vs CPW  1 1.3163257 5.351814 0.15579421   0.001       0.01   *\n3   BGHF vs SRW  1 0.6422157 2.204225 0.07063867   0.016       0.16    \n4  BGHF vs SSTF  1 0.7759586 2.965274 0.10237341   0.001       0.01   *\n5   CCIF vs CPW  1 0.4603595 2.075474 0.06470596   0.013       0.13    \n6   CCIF vs SRW  1 0.5712089 2.149875 0.06687041   0.009       0.09    \n7  CCIF vs SSTF  1 0.3236528 1.381556 0.04867796   0.173       1.00    \n8    CPW vs SRW  1 0.8702832 3.323148 0.09972490   0.001       0.01   *\n9   CPW vs SSTF  1 0.4584616 1.993009 0.06874103   0.009       0.09    \n10  SRW vs SSTF  1 0.4534517 1.626466 0.05681688   0.062       0.62    \n\n\nExercise: Which communities are significantly different from each other?\nTo see what species are contributing to the differences between the vegetation communities, we can again use a SIMPER analysis (Similarity Percentages)\n\nantsim &lt;- simper(TransAntData, AntData$Community)\nsummary(antsim)\n\n\nContrast: BGHF_CCIF \n\n                          average      sd   ratio     ava     avb cumsum     p\nPheidole.5                0.05345 0.04054 1.31860 0.44630 1.22750  0.069 0.001\nTetramorium.3             0.04337 0.03735 1.16130 0.71720 0.00000  0.125 0.001\nAnonychromyrma.1          0.03847 0.03475 1.10700 0.69070 0.25380  0.174 0.019\nPheidole.7                0.03459 0.03500 0.98840 0.50690 0.38970  0.219 0.029\nRhytidiponera..metallica. 0.03448 0.03747 0.92030 0.98380 1.30250  0.263 0.294\nTapinoma.1                0.03402 0.03186 1.06780 0.42520 0.65120  0.307 0.118\nMonomorium.1              0.03233 0.04049 0.79850 0.45680 0.29450  0.349 0.036\nMeranoplus.1              0.02743 0.03296 0.83210 0.34180 0.33620  0.384 0.438\nNotoncus.1                0.02495 0.03250 0.76780 0.20000 0.37000  0.416 0.713\nParatrechina.1            0.02038 0.02882 0.70720 0.21260 0.25000  0.443 0.829\nMachomyrma.1              0.01886 0.03016 0.62530 0.00000 0.33620  0.467 0.645\nParatrechina.4            0.01802 0.02650 0.68000 0.26670 0.14480  0.490 0.116\nPheidole.7.1              0.01604 0.02829 0.56710 0.30690 0.00000  0.511 0.139\nHeteroponera.1            0.01543 0.02693 0.57290 0.13330 0.18750  0.531 0.113\nTetramorium.4             0.01536 0.02604 0.58980 0.30040 0.00000  0.550 0.003\nIridomyrmex.7             0.01409 0.02534 0.55610 0.00000 0.26180  0.569 0.002\nIridomyrmex.purpureus     0.01395 0.02541 0.54900 0.00000 0.26180  0.587 0.847\nParatrechina.2            0.01262 0.02405 0.52460 0.15440 0.12500  0.603 0.953\nSolenopsis.1              0.01200 0.02462 0.48760 0.20000 0.00000  0.618 0.033\nPolyrachis.5              0.01151 0.02474 0.46500 0.00000 0.18750  0.633 0.012\nMachomyrma.3              0.01131 0.02500 0.45240 0.00000 0.18750  0.648 0.116\nDoleromyrma.1             0.01120 0.02414 0.46390 0.00000 0.18750  0.662 0.645\nPristomyrmex.1            0.01119 0.02335 0.47930 0.21260 0.00000  0.676 0.028\nCrematogaster.1           0.01117 0.02270 0.49230 0.20000 0.00000  0.691 0.922\nPristomyrmex.2            0.01080 0.02361 0.45740 0.14590 0.07430  0.705 0.804\nColobostruma.1            0.01019 0.02101 0.48500 0.20000 0.00000  0.718 0.009\nCrematogaster.3           0.00987 0.02758 0.35770 0.00000 0.16760  0.731 0.054\nRhopalomastix.2           0.00920 0.02555 0.36020 0.09430 0.06250  0.743 0.091\nCamponotus.12             0.00920 0.01972 0.46660 0.00000 0.19930  0.754 0.570\nPheidole.6                0.00919 0.01970 0.46670 0.00000 0.19930  0.766 0.787\nRhopalomastix.1           0.00909 0.02516 0.36140 0.00000 0.14480  0.778 0.194\nRhopalomastix.3           0.00792 0.02083 0.38030 0.15860 0.00000  0.788 0.023\nNotoncus.4                0.00767 0.02079 0.36910 0.00000 0.13680  0.798 0.055\nMyrmecia.1                0.00689 0.01809 0.38080 0.13330 0.00000  0.807 0.067\nCamponotus.consobrinus    0.00682 0.01929 0.35360 0.00000 0.12500  0.816 0.896\nIridomyrmex.2             0.00680 0.01856 0.36640 0.00000 0.12500  0.825 0.846\nMyrmecia.3                0.00677 0.01784 0.37920 0.13330 0.00000  0.833 0.511\nMayriella.2               0.00669 0.01851 0.36130 0.06670 0.06250  0.842 0.774\nProlasius.1               0.00666 0.01822 0.36530 0.07930 0.06250  0.850 0.293\nPolyrachis.2              0.00624 0.01709 0.36490 0.06670 0.06250  0.859 0.226\nOchetellus.1              0.00618 0.01701 0.36340 0.00000 0.12500  0.866 0.925\nDoleromyrma.3             0.00608 0.01575 0.38600 0.13330 0.00000  0.874 0.892\nStrumigenys.1             0.00580 0.01599 0.36290 0.06670 0.06250  0.882 0.301\nPapyrius.1                0.00559 0.01535 0.36420 0.06670 0.06250  0.889 0.232\nParatrechina.6            0.00434 0.01644 0.26400 0.07930 0.00000  0.894 0.098\nProlasius.2               0.00434 0.01644 0.26400 0.07930 0.00000  0.900 0.222\nCrematogaster.2           0.00421 0.01598 0.26320 0.06670 0.00000  0.905 0.960\nDoleromyrma.2             0.00401 0.01600 0.25070 0.00000 0.06250  0.911 0.208\nStrumigenys.2             0.00400 0.01517 0.26350 0.06670 0.00000  0.916 0.117\nIridomyrmex.4             0.00388 0.01546 0.25120 0.00000 0.06250  0.921 0.717\nPolyrachis.1              0.00388 0.01546 0.25120 0.00000 0.06250  0.926 0.221\nPlagiolepis.1             0.00365 0.01382 0.26400 0.06670 0.00000  0.931 0.098\nMelophorus.1              0.00365 0.01446 0.25210 0.00000 0.06250  0.935 0.227\nNotoncus.3                0.00354 0.01340 0.26420 0.06670 0.00000  0.940 0.091\nProlasius.6               0.00354 0.01340 0.26420 0.06670 0.00000  0.944 0.914\nProbolomyrmex.1           0.00339 0.01283 0.26440 0.06670 0.00000  0.949 0.084\nTetramorium.5             0.00339 0.01283 0.26440 0.06670 0.00000  0.953 0.084\nPheidole.2                0.00337 0.01274 0.26440 0.06670 0.00000  0.958 0.101\nProlasius.3               0.00337 0.01274 0.26440 0.06670 0.00000  0.962 0.676\nLeptomyrmex.1             0.00335 0.01265 0.26440 0.06670 0.00000  0.966 0.103\nFroggattella.1            0.00328 0.01296 0.25330 0.00000 0.06250  0.970 0.242\nColobostruma.2            0.00301 0.01186 0.25410 0.00000 0.06250  0.974 0.238\nMachomyrma.6              0.00301 0.01183 0.25410 0.00000 0.06250  0.978 0.240\nMelophorus.2              0.00301 0.01183 0.25410 0.00000 0.06250  0.982 0.306\nPolyrachis.3              0.00301 0.01183 0.25410 0.00000 0.06250  0.986 0.718\nMayriella.1               0.00286 0.01078 0.26500 0.06670 0.00000  0.990 0.472\nPapyrius.2                0.00286 0.01078 0.26500 0.06670 0.00000  0.993 0.971\nMesostruma.1              0.00273 0.01031 0.26520 0.06670 0.00000  0.997 0.962\nStigmacros.3              0.00253 0.00993 0.25520 0.00000 0.06250  1.000 0.883\nAnonychromyrma.2          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nAnonychromyrma.3          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotis.26             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDolichonderus.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nEpopostruma.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.6             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.8             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMeranoplus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecorhrynchus.1        0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPachychondyla.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPachycondyla.2            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.5            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.6.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.7              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.8              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.4               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.7               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.5           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTechnomyrmex.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                             \nPheidole.5                ***\nTetramorium.3             ***\nAnonychromyrma.1          *  \nPheidole.7                *  \nRhytidiponera..metallica.    \nTapinoma.1                   \nMonomorium.1              *  \nMeranoplus.1                 \nNotoncus.1                   \nParatrechina.1               \nMachomyrma.1                 \nParatrechina.4               \nPheidole.7.1                 \nHeteroponera.1               \nTetramorium.4             ** \nIridomyrmex.7             ** \nIridomyrmex.purpureus        \nParatrechina.2               \nSolenopsis.1              *  \nPolyrachis.5              *  \nMachomyrma.3                 \nDoleromyrma.1                \nPristomyrmex.1            *  \nCrematogaster.1              \nPristomyrmex.2               \nColobostruma.1            ** \nCrematogaster.3           .  \nRhopalomastix.2           .  \nCamponotus.12                \nPheidole.6                   \nRhopalomastix.1              \nRhopalomastix.3           *  \nNotoncus.4                .  \nMyrmecia.1                .  \nCamponotus.consobrinus       \nIridomyrmex.2                \nMyrmecia.3                   \nMayriella.2                  \nProlasius.1                  \nPolyrachis.2                 \nOchetellus.1                 \nDoleromyrma.3                \nStrumigenys.1                \nPapyrius.1                   \nParatrechina.6            .  \nProlasius.2                  \nCrematogaster.2              \nDoleromyrma.2                \nStrumigenys.2                \nIridomyrmex.4                \nPolyrachis.1                 \nPlagiolepis.1             .  \nMelophorus.1                 \nNotoncus.3                .  \nProlasius.6                  \nProbolomyrmex.1           .  \nTetramorium.5             .  \nPheidole.2                   \nProlasius.3                  \nLeptomyrmex.1                \nFroggattella.1               \nColobostruma.2               \nMachomyrma.6                 \nMelophorus.2                 \nPolyrachis.3                 \nMayriella.1                  \nPapyrius.2                   \nMesostruma.1                 \nStigmacros.3                 \nAnonychromyrma.2             \nAnonychromyrma.3             \nCamponotis.26                \nCamponotus.1                 \nDisturbed.lost               \nDolichonderus.1              \nEpopostruma.1                \nIridomyrmex.5                \nIridomyrmex.6                \nIridomyrmex.8                \nMachomyrma.4                 \nMeranoplus.2                 \nMyrmecia.2                   \nMyrmecia.4                   \nMyrmecorhrynchus.1           \nOligomyrmex.1                \nOligomyrmex.2                \nPachychondyla.1              \nPachycondyla.2               \nParatrechina.5               \nPheidole.1                   \nPheidole.6.1                 \nPolyrachis.7                 \nPolyrachis.8                 \nProlasius.4                  \nProlasius.7                  \nRhopalomastix.5              \nStigmacros.1                 \nStigmacros.2                 \nStigmacros.4                 \nTechnomyrmex.1               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: BGHF_CPW \n\n                          average      sd   ratio     ava     avb cumsum     p\nTetramorium.3             0.03724 0.03269 1.13910 0.71720 0.00000  0.047 0.001\nPheidole.5                0.03715 0.03141 1.18270 0.44630 0.92190  0.094 0.574\nMeranoplus.1              0.03196 0.02890 1.10590 0.34180 0.65660  0.135 0.100\nAnonychromyrma.1          0.03196 0.02791 1.14510 0.69070 0.08230  0.176 0.295\nTapinoma.1                0.03127 0.02931 1.06680 0.42520 0.73330  0.215 0.448\nRhytidiponera..metallica. 0.03040 0.03400 0.89420 0.98380 1.45860  0.254 0.591\nPheidole.7                0.02792 0.03003 0.92990 0.50690 0.34410  0.290 0.495\nParatrechina.1            0.02761 0.02785 0.99110 0.21260 0.57810  0.325 0.141\nIridomyrmex.purpureus     0.02585 0.03202 0.80740 0.00000 0.54280  0.357 0.067\nMonomorium.1              0.02474 0.03321 0.74510 0.45680 0.16760  0.389 0.395\nNotoncus.1                0.02287 0.02804 0.81560 0.20000 0.42060  0.418 0.863\nMachomyrma.1              0.02194 0.02674 0.82050 0.00000 0.43750  0.446 0.375\nPheidole.7.1              0.01935 0.02739 0.70640 0.30690 0.19930  0.470 0.016\nMesostruma.1              0.01824 0.02621 0.69570 0.06670 0.39440  0.494 0.014\nCrematogaster.1           0.01707 0.02408 0.70900 0.20000 0.25000  0.515 0.580\nIridomyrmex.5             0.01685 0.02711 0.62160 0.00000 0.34800  0.537 0.002\nParatrechina.2            0.01512 0.02293 0.65940 0.15440 0.25000  0.556 0.833\nPristomyrmex.2            0.01511 0.02610 0.57890 0.14590 0.22520  0.575 0.459\nProlasius.6               0.01482 0.02421 0.61210 0.06670 0.31540  0.594 0.057\nPapyrius.2                0.01478 0.02913 0.50740 0.06670 0.21120  0.613 0.126\nTetramorium.4             0.01448 0.02315 0.62550 0.30040 0.06250  0.631 0.006\nOchetellus.1              0.01402 0.02158 0.64950 0.00000 0.32430  0.649 0.301\nCamponotus.consobrinus    0.01372 0.02459 0.55770 0.00000 0.25000  0.666 0.404\nMayriella.2               0.01347 0.02244 0.60030 0.06670 0.25000  0.683 0.133\nIridomyrmex.2             0.01314 0.02377 0.55280 0.00000 0.27370  0.700 0.305\nParatrechina.4            0.01304 0.02090 0.62380 0.26670 0.06250  0.717 0.447\nCamponotus.12             0.01088 0.01960 0.55510 0.00000 0.25000  0.730 0.421\nSolenopsis.1              0.01034 0.02144 0.48210 0.20000 0.00000  0.744 0.119\nPristomyrmex.1            0.00978 0.02056 0.47570 0.21260 0.00000  0.756 0.139\nDoleromyrma.3             0.00941 0.01784 0.52750 0.13330 0.12500  0.768 0.736\nStigmacros.3              0.00913 0.01969 0.46340 0.00000 0.21910  0.779 0.112\nColobostruma.1            0.00893 0.01855 0.48140 0.20000 0.00000  0.791 0.038\nDoleromyrma.1             0.00841 0.01800 0.46710 0.00000 0.19930  0.802 0.862\nPheidole.6                0.00817 0.01769 0.46170 0.00000 0.18750  0.812 0.878\nMyrmecia.3                0.00803 0.01780 0.45140 0.13330 0.06250  0.822 0.373\nHeteroponera.1            0.00802 0.01758 0.45610 0.13330 0.07430  0.832 0.742\nMyrmecia.1                0.00757 0.01689 0.44780 0.13330 0.06250  0.842 0.063\nPolyrachis.2              0.00715 0.01591 0.44920 0.06670 0.12500  0.851 0.163\nRhopalomastix.3           0.00695 0.01840 0.37790 0.15860 0.00000  0.860 0.136\nPolyrachis.8              0.00636 0.01715 0.37080 0.00000 0.12500  0.868 0.053\nProlasius.1               0.00549 0.01529 0.35900 0.07930 0.06250  0.875 0.473\nPachycondyla.2            0.00543 0.01485 0.36570 0.00000 0.12500  0.882 0.520\nMayriella.1               0.00541 0.01505 0.35970 0.06670 0.06250  0.889 0.144\nPapyrius.1                0.00540 0.01484 0.36400 0.06670 0.06250  0.895 0.261\nPolyrachis.3              0.00531 0.01445 0.36740 0.00000 0.12500  0.902 0.414\nRhopalomastix.2           0.00510 0.01960 0.26010 0.09430 0.00000  0.909 0.412\nIridomyrmex.4             0.00449 0.01767 0.25410 0.00000 0.09350  0.914 0.627\nParatrechina.6            0.00378 0.01446 0.26150 0.07930 0.00000  0.919 0.405\nProlasius.2               0.00378 0.01446 0.26150 0.07930 0.00000  0.924 0.409\nCrematogaster.2           0.00360 0.01386 0.26010 0.06670 0.00000  0.929 0.982\nStrumigenys.2             0.00345 0.01323 0.26060 0.06670 0.00000  0.933 0.402\nMyrmecia.2                0.00344 0.01360 0.25280 0.00000 0.06250  0.937 0.213\nOligomyrmex.1             0.00319 0.01258 0.25360 0.00000 0.06250  0.942 0.231\nPlagiolepis.1             0.00318 0.01216 0.26150 0.06670 0.00000  0.946 0.405\nNotoncus.3                0.00309 0.01182 0.26170 0.06670 0.00000  0.949 0.386\nStigmacros.4              0.00300 0.01182 0.25410 0.00000 0.06250  0.953 0.581\nProbolomyrmex.1           0.00298 0.01137 0.26200 0.06670 0.00000  0.957 0.402\nTetramorium.5             0.00298 0.01137 0.26200 0.06670 0.00000  0.961 0.402\nIridomyrmex.6             0.00297 0.01167 0.25420 0.00000 0.06250  0.965 0.265\nPheidole.2                0.00296 0.01130 0.26210 0.06670 0.00000  0.968 0.378\nProlasius.3               0.00296 0.01130 0.26210 0.06670 0.00000  0.972 0.849\nLeptomyrmex.1             0.00294 0.01123 0.26210 0.06670 0.00000  0.976 0.392\nMeranoplus.2              0.00278 0.01092 0.25470 0.00000 0.06250  0.979 0.225\nRhopalomastix.1           0.00267 0.01044 0.25580 0.00000 0.07430  0.983 0.841\nStrumigenys.1             0.00255 0.00970 0.26320 0.06670 0.00000  0.986 0.775\nMelophorus.2              0.00244 0.00954 0.25610 0.00000 0.07430  0.989 0.571\nPheidole.6.1              0.00241 0.00942 0.25550 0.00000 0.06250  0.992 0.238\nIridomyrmex.8             0.00205 0.00802 0.25610 0.00000 0.06250  0.995 0.263\nMyrmecia.4                0.00205 0.00802 0.25610 0.00000 0.06250  0.997 0.263\nRhopalomastix.5           0.00205 0.00802 0.25610 0.00000 0.06250  1.000 0.263\nAnonychromyrma.2          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nAnonychromyrma.3          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotis.26             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nColobostruma.2            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCrematogaster.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDoleromyrma.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDolichonderus.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nEpopostruma.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nFroggattella.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.7             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.3              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.6              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMelophorus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecorhrynchus.1        0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPachychondyla.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.5            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.5              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.7              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.4               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.7               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTechnomyrmex.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                             \nTetramorium.3             ***\nPheidole.5                   \nMeranoplus.1              .  \nAnonychromyrma.1             \nTapinoma.1                   \nRhytidiponera..metallica.    \nPheidole.7                   \nParatrechina.1               \nIridomyrmex.purpureus     .  \nMonomorium.1                 \nNotoncus.1                   \nMachomyrma.1                 \nPheidole.7.1              *  \nMesostruma.1              *  \nCrematogaster.1              \nIridomyrmex.5             ** \nParatrechina.2               \nPristomyrmex.2               \nProlasius.6               .  \nPapyrius.2                   \nTetramorium.4             ** \nOchetellus.1                 \nCamponotus.consobrinus       \nMayriella.2                  \nIridomyrmex.2                \nParatrechina.4               \nCamponotus.12                \nSolenopsis.1                 \nPristomyrmex.1               \nDoleromyrma.3                \nStigmacros.3                 \nColobostruma.1            *  \nDoleromyrma.1                \nPheidole.6                   \nMyrmecia.3                   \nHeteroponera.1               \nMyrmecia.1                .  \nPolyrachis.2                 \nRhopalomastix.3              \nPolyrachis.8              .  \nProlasius.1                  \nPachycondyla.2               \nMayriella.1                  \nPapyrius.1                   \nPolyrachis.3                 \nRhopalomastix.2              \nIridomyrmex.4                \nParatrechina.6               \nProlasius.2                  \nCrematogaster.2              \nStrumigenys.2                \nMyrmecia.2                   \nOligomyrmex.1                \nPlagiolepis.1                \nNotoncus.3                   \nStigmacros.4                 \nProbolomyrmex.1              \nTetramorium.5                \nIridomyrmex.6                \nPheidole.2                   \nProlasius.3                  \nLeptomyrmex.1                \nMeranoplus.2                 \nRhopalomastix.1              \nStrumigenys.1                \nMelophorus.2                 \nPheidole.6.1                 \nIridomyrmex.8                \nMyrmecia.4                   \nRhopalomastix.5              \nAnonychromyrma.2             \nAnonychromyrma.3             \nCamponotis.26                \nCamponotus.1                 \nColobostruma.2               \nCrematogaster.3              \nDisturbed.lost               \nDoleromyrma.2                \nDolichonderus.1              \nEpopostruma.1                \nFroggattella.1               \nIridomyrmex.7                \nMachomyrma.3                 \nMachomyrma.4                 \nMachomyrma.6                 \nMelophorus.1                 \nMyrmecorhrynchus.1           \nNotoncus.4                   \nOligomyrmex.2                \nPachychondyla.1              \nParatrechina.5               \nPheidole.1                   \nPolyrachis.1                 \nPolyrachis.5                 \nPolyrachis.7                 \nProlasius.4                  \nProlasius.7                  \nStigmacros.1                 \nStigmacros.2                 \nTechnomyrmex.1               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: BGHF_SRW \n\n                          average      sd   ratio     ava     avb cumsum     p\nTetramorium.3             0.04571 0.04213 1.08490 0.71720 0.29710  0.058 0.001\nPheidole.5                0.04554 0.04637 0.98200 0.44630 0.75730  0.117 0.027\nRhytidiponera..metallica. 0.04178 0.03930 1.06300 0.98380 0.79590  0.170 0.022\nAnonychromyrma.1          0.03930 0.03593 1.09370 0.69070 0.35530  0.220 0.008\nPheidole.7                0.03562 0.04061 0.87700 0.50690 0.26180  0.266 0.019\nMonomorium.1              0.03444 0.04336 0.79440 0.45680 0.27240  0.310 0.017\nNotoncus.1                0.02975 0.03307 0.89970 0.20000 0.48480  0.348 0.250\nCrematogaster.2           0.02843 0.03727 0.76300 0.06670 0.43640  0.384 0.001\nTapinoma.1                0.02781 0.03265 0.85190 0.42520 0.18750  0.420 0.897\nDoleromyrma.3             0.02663 0.04353 0.61190 0.13330 0.33670  0.454 0.004\nMeranoplus.1              0.02641 0.03671 0.71920 0.34180 0.24430  0.487 0.580\nParatrechina.1            0.02429 0.03114 0.78010 0.21260 0.34410  0.518 0.478\nParatrechina.2            0.02210 0.02986 0.74040 0.15440 0.33230  0.547 0.241\nParatrechina.4            0.01956 0.02986 0.65500 0.26670 0.12500  0.572 0.054\nPheidole.7.1              0.01918 0.03155 0.60790 0.30690 0.06250  0.596 0.019\nPristomyrmex.2            0.01825 0.03129 0.58320 0.14590 0.21340  0.619 0.210\nTetramorium.4             0.01672 0.02853 0.58580 0.30040 0.00000  0.641 0.001\nSolenopsis.1              0.01533 0.02859 0.53620 0.20000 0.06250  0.660 0.002\nCrematogaster.1           0.01470 0.02711 0.54230 0.20000 0.06250  0.679 0.776\nMyrmecia.3                0.01374 0.02702 0.50830 0.13330 0.12500  0.697 0.013\nHeteroponera.1            0.01289 0.02495 0.51650 0.13330 0.12500  0.713 0.277\nPristomyrmex.1            0.01222 0.02573 0.47480 0.21260 0.00000  0.729 0.017\nColobostruma.1            0.01110 0.02310 0.48050 0.20000 0.00000  0.743 0.001\nProlasius.3               0.01032 0.02298 0.44900 0.06670 0.13680  0.756 0.050\nIridomyrmex.2             0.00923 0.02626 0.35130 0.00000 0.12500  0.768 0.655\nRhopalomastix.3           0.00862 0.02285 0.37710 0.15860 0.00000  0.779 0.002\nTechnomyrmex.1            0.00828 0.02270 0.36470 0.00000 0.12500  0.789 0.040\nPachycondyla.2            0.00810 0.02202 0.36800 0.00000 0.13680  0.800 0.203\nProlasius.1               0.00767 0.02104 0.36470 0.07930 0.06250  0.810 0.213\nMayriella.2               0.00759 0.02110 0.35990 0.06670 0.06250  0.819 0.691\nProlasius.2               0.00758 0.02102 0.36080 0.07930 0.06250  0.829 0.027\nMyrmecia.1                0.00751 0.01991 0.37740 0.13330 0.00000  0.839 0.051\nDolichonderus.1           0.00722 0.01961 0.36820 0.00000 0.12500  0.848 0.035\nMachomyrma.3              0.00707 0.01917 0.36910 0.00000 0.12500  0.857 0.507\nStigmacros.1              0.00692 0.01872 0.36950 0.00000 0.12500  0.866 0.151\nRhopalomastix.2           0.00661 0.02531 0.26120 0.09430 0.00000  0.874 0.169\nPapyrius.2                0.00623 0.01722 0.36180 0.06670 0.06250  0.882 0.824\nStrumigenys.1             0.00623 0.01722 0.36180 0.06670 0.06250  0.890 0.217\nParatrechina.6            0.00475 0.01806 0.26270 0.07930 0.00000  0.896 0.004\nStrumigenys.2             0.00442 0.01687 0.26180 0.06670 0.00000  0.902 0.002\nIridomyrmex.purpureus     0.00434 0.01743 0.24920 0.00000 0.06250  0.907 0.997\nPlagiolepis.1             0.00399 0.01519 0.26270 0.06670 0.00000  0.912 0.004\nNotoncus.3                0.00386 0.01467 0.26300 0.06670 0.00000  0.917 0.004\nPolyrachis.2              0.00386 0.01467 0.26300 0.06670 0.00000  0.922 0.513\nProlasius.6               0.00386 0.01467 0.26300 0.06670 0.00000  0.927 0.880\nProbolomyrmex.1           0.00368 0.01398 0.26330 0.06670 0.00000  0.932 0.003\nTetramorium.5             0.00368 0.01398 0.26330 0.06670 0.00000  0.937 0.003\nPapyrius.1                0.00365 0.01387 0.26340 0.06670 0.00000  0.941 0.333\nPheidole.2                0.00365 0.01387 0.26340 0.06670 0.00000  0.946 0.004\nLeptomyrmex.1             0.00363 0.01377 0.26340 0.06670 0.00000  0.951 0.006\nMachomyrma.1              0.00357 0.01415 0.25240 0.00000 0.06250  0.955 1.000\nCamponotus.consobrinus    0.00353 0.01397 0.25250 0.00000 0.06250  0.960 0.982\nDoleromyrma.1             0.00353 0.01397 0.25250 0.00000 0.06250  0.964 0.983\nIridomyrmex.4             0.00350 0.01387 0.25260 0.00000 0.06250  0.969 0.772\nPachychondyla.1           0.00345 0.01365 0.25280 0.00000 0.06250  0.973 0.232\nRhopalomastix.1           0.00341 0.01350 0.25290 0.00000 0.06250  0.977 0.753\nStigmacros.4              0.00325 0.01284 0.25340 0.00000 0.06250  0.982 0.326\nMayriella.1               0.00306 0.01156 0.26440 0.06670 0.00000  0.986 0.375\nMesostruma.1              0.00292 0.01102 0.26460 0.06670 0.00000  0.989 0.938\nOchetellus.1              0.00282 0.01107 0.25460 0.00000 0.06250  0.993 0.984\nOligomyrmex.2             0.00282 0.01107 0.25460 0.00000 0.06250  0.996 0.531\nPheidole.6                0.00282 0.01107 0.25460 0.00000 0.06250  1.000 0.996\nAnonychromyrma.2          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nAnonychromyrma.3          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotis.26             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotus.12             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nColobostruma.2            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCrematogaster.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDoleromyrma.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nEpopostruma.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nFroggattella.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.6             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.7             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.8             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.6              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMelophorus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMelophorus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMeranoplus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecorhrynchus.1        0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.5            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.6.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.3              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.5              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.7              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.8              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.4               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.7               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.5           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.3              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                             \nTetramorium.3             ***\nPheidole.5                *  \nRhytidiponera..metallica. *  \nAnonychromyrma.1          ** \nPheidole.7                *  \nMonomorium.1              *  \nNotoncus.1                   \nCrematogaster.2           ***\nTapinoma.1                   \nDoleromyrma.3             ** \nMeranoplus.1                 \nParatrechina.1               \nParatrechina.2               \nParatrechina.4            .  \nPheidole.7.1              *  \nPristomyrmex.2               \nTetramorium.4             ***\nSolenopsis.1              ** \nCrematogaster.1              \nMyrmecia.3                *  \nHeteroponera.1               \nPristomyrmex.1            *  \nColobostruma.1            ***\nProlasius.3               *  \nIridomyrmex.2                \nRhopalomastix.3           ** \nTechnomyrmex.1            *  \nPachycondyla.2               \nProlasius.1                  \nMayriella.2                  \nProlasius.2               *  \nMyrmecia.1                .  \nDolichonderus.1           *  \nMachomyrma.3                 \nStigmacros.1                 \nRhopalomastix.2              \nPapyrius.2                   \nStrumigenys.1                \nParatrechina.6            ** \nStrumigenys.2             ** \nIridomyrmex.purpureus        \nPlagiolepis.1             ** \nNotoncus.3                ** \nPolyrachis.2                 \nProlasius.6                  \nProbolomyrmex.1           ** \nTetramorium.5             ** \nPapyrius.1                   \nPheidole.2                ** \nLeptomyrmex.1             ** \nMachomyrma.1                 \nCamponotus.consobrinus       \nDoleromyrma.1                \nIridomyrmex.4                \nPachychondyla.1              \nRhopalomastix.1              \nStigmacros.4                 \nMayriella.1                  \nMesostruma.1                 \nOchetellus.1                 \nOligomyrmex.2                \nPheidole.6                   \nAnonychromyrma.2             \nAnonychromyrma.3             \nCamponotis.26                \nCamponotus.1                 \nCamponotus.12                \nColobostruma.2               \nCrematogaster.3              \nDisturbed.lost               \nDoleromyrma.2                \nEpopostruma.1                \nFroggattella.1               \nIridomyrmex.5                \nIridomyrmex.6                \nIridomyrmex.7                \nIridomyrmex.8                \nMachomyrma.4                 \nMachomyrma.6                 \nMelophorus.1                 \nMelophorus.2                 \nMeranoplus.2                 \nMyrmecia.2                   \nMyrmecia.4                   \nMyrmecorhrynchus.1           \nNotoncus.4                   \nOligomyrmex.1                \nParatrechina.5               \nPheidole.1                   \nPheidole.6.1                 \nPolyrachis.1                 \nPolyrachis.3                 \nPolyrachis.5                 \nPolyrachis.7                 \nPolyrachis.8                 \nProlasius.4                  \nProlasius.7                  \nRhopalomastix.5              \nStigmacros.2                 \nStigmacros.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: BGHF_SSTF \n\n                          average      sd   ratio     ava     avb cumsum     p\nPheidole.5                0.04728 0.03956 1.19510 0.44630 1.08140  0.062 0.012\nTetramorium.3             0.04012 0.03587 1.11840 0.71720 0.25510  0.114 0.001\nAnonychromyrma.1          0.03661 0.03367 1.08740 0.69070 0.49420  0.162 0.049\nRhytidiponera..metallica. 0.03363 0.03803 0.88450 0.98380 1.22640  0.207 0.364\nPheidole.7                0.03164 0.03434 0.92150 0.50690 0.24530  0.248 0.165\nCrematogaster.1           0.03090 0.03234 0.95540 0.20000 0.56960  0.289 0.003\nMonomorium.1              0.02876 0.03777 0.76140 0.45680 0.20030  0.326 0.198\nTapinoma.1                0.02771 0.02968 0.93340 0.42520 0.38460  0.363 0.867\nMeranoplus.1              0.02179 0.03107 0.70120 0.34180 0.15380  0.391 0.850\nParatrechina.2            0.02106 0.02933 0.71790 0.15440 0.32220  0.419 0.365\nMachomyrma.1              0.01990 0.02632 0.75610 0.00000 0.41370  0.445 0.554\nNotoncus.1                0.01961 0.02981 0.65790 0.20000 0.24530  0.470 0.958\nPheidole.6                0.01656 0.02603 0.63620 0.00000 0.32220  0.492 0.163\nParatrechina.4            0.01614 0.02494 0.64730 0.26670 0.09150  0.513 0.226\nParatrechina.1            0.01610 0.02596 0.62000 0.21260 0.15380  0.534 0.972\nPheidole.7.1              0.01597 0.02834 0.56370 0.30690 0.00000  0.555 0.134\nTetramorium.4             0.01529 0.02607 0.58650 0.30040 0.00000  0.575 0.012\nPristomyrmex.2            0.01409 0.02577 0.54690 0.14590 0.15380  0.594 0.547\nPristomyrmex.1            0.01377 0.02511 0.54850 0.21260 0.07690  0.612 0.011\nDoleromyrma.1             0.01349 0.02543 0.53070 0.00000 0.23080  0.630 0.412\nSolenopsis.1              0.01196 0.02470 0.48430 0.20000 0.00000  0.645 0.048\nCamponotus.consobrinus    0.01148 0.02157 0.53230 0.00000 0.24530  0.660 0.584\nIridomyrmex.purpureus     0.01099 0.02700 0.40730 0.00000 0.20030  0.675 0.938\nPapyrius.2                0.01076 0.02258 0.47680 0.06670 0.15380  0.689 0.465\nMachomyrma.4              0.01054 0.02600 0.40530 0.00000 0.15380  0.703 0.004\nProlasius.6               0.01052 0.02193 0.47970 0.06670 0.16840  0.717 0.343\nOchetellus.1              0.01026 0.02493 0.41170 0.00000 0.16840  0.730 0.641\nColobostruma.1            0.01014 0.02103 0.48230 0.20000 0.00000  0.743 0.015\nCrematogaster.2           0.00923 0.02469 0.37390 0.06670 0.07690  0.755 0.765\nCamponotus.12             0.00800 0.01933 0.41400 0.00000 0.15380  0.766 0.715\nMesostruma.1              0.00789 0.02143 0.36830 0.06670 0.09150  0.776 0.625\nRhopalomastix.3           0.00789 0.02085 0.37840 0.15860 0.00000  0.786 0.093\nMayriella.2               0.00715 0.01883 0.37960 0.06670 0.07690  0.796 0.700\nMyrmecia.1                0.00686 0.01811 0.37880 0.13330 0.00000  0.805 0.130\nMyrmecia.3                0.00674 0.01786 0.37720 0.13330 0.00000  0.814 0.519\nHeteroponera.1            0.00670 0.01775 0.37760 0.13330 0.00000  0.822 0.787\nProlasius.3               0.00648 0.01681 0.38550 0.06670 0.07690  0.831 0.409\nDoleromyrma.3             0.00605 0.01574 0.38450 0.13330 0.00000  0.839 0.861\nRhopalomastix.2           0.00593 0.02266 0.26160 0.09430 0.00000  0.847 0.286\nProlasius.7               0.00505 0.01806 0.27960 0.00000 0.07690  0.853 0.045\nStigmacros.2              0.00493 0.01745 0.28270 0.00000 0.09150  0.860 0.044\nAnonychromyrma.2          0.00472 0.01681 0.28090 0.00000 0.07690  0.866 0.052\nPolyrachis.7              0.00444 0.01577 0.28180 0.00000 0.07690  0.872 0.040\nStigmacros.1              0.00444 0.01577 0.28180 0.00000 0.07690  0.877 0.332\nPheidole.1                0.00441 0.01566 0.28190 0.00000 0.07690  0.883 0.050\nParatrechina.6            0.00432 0.01644 0.26270 0.07930 0.00000  0.889 0.253\nProlasius.2               0.00432 0.01644 0.26270 0.07930 0.00000  0.895 0.284\nProlasius.1               0.00399 0.01514 0.26330 0.07930 0.00000  0.900 0.625\nStrumigenys.2             0.00398 0.01520 0.26200 0.06670 0.00000  0.905 0.260\nCamponotis.26             0.00375 0.01320 0.28380 0.00000 0.07690  0.910 0.051\nCamponotus.1              0.00375 0.01320 0.28380 0.00000 0.07690  0.915 0.051\nEpopostruma.1             0.00375 0.01320 0.28380 0.00000 0.07690  0.920 0.051\nPolyrachis.3              0.00375 0.01320 0.28380 0.00000 0.07690  0.925 0.529\nPlagiolepis.1             0.00363 0.01383 0.26270 0.06670 0.00000  0.929 0.253\nAnonychromyrma.3          0.00356 0.01252 0.28420 0.00000 0.07690  0.934 0.049\nMyrmecorhrynchus.1        0.00356 0.01252 0.28420 0.00000 0.07690  0.939 0.049\nParatrechina.5            0.00356 0.01252 0.28420 0.00000 0.07690  0.944 0.049\nIridomyrmex.4             0.00356 0.01251 0.28420 0.00000 0.07690  0.948 0.767\nOligomyrmex.2             0.00356 0.01251 0.28420 0.00000 0.07690  0.953 0.151\nProlasius.4               0.00356 0.01251 0.28420 0.00000 0.07690  0.957 0.045\nNotoncus.3                0.00352 0.01340 0.26290 0.06670 0.00000  0.962 0.258\nPolyrachis.2              0.00352 0.01340 0.26290 0.06670 0.00000  0.967 0.580\nProbolomyrmex.1           0.00338 0.01283 0.26320 0.06670 0.00000  0.971 0.282\nTetramorium.5             0.00338 0.01283 0.26320 0.06670 0.00000  0.976 0.282\nPapyrius.1                0.00335 0.01273 0.26330 0.06670 0.00000  0.980 0.455\nPheidole.2                0.00335 0.01273 0.26330 0.06670 0.00000  0.984 0.256\nLeptomyrmex.1             0.00333 0.01265 0.26330 0.06670 0.00000  0.989 0.275\nStigmacros.3              0.00296 0.01035 0.28550 0.00000 0.07690  0.993 0.787\nMayriella.1               0.00284 0.01076 0.26410 0.06670 0.00000  0.996 0.554\nStrumigenys.1             0.00284 0.01076 0.26410 0.06670 0.00000  1.000 0.718\nColobostruma.2            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCrematogaster.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDoleromyrma.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDolichonderus.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nFroggattella.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.6             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.7             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.8             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.3              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.6              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMelophorus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMelophorus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMeranoplus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPachychondyla.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPachycondyla.2            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.6.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.5              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.8              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.5           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTechnomyrmex.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                             \nPheidole.5                *  \nTetramorium.3             ***\nAnonychromyrma.1          *  \nRhytidiponera..metallica.    \nPheidole.7                   \nCrematogaster.1           ** \nMonomorium.1                 \nTapinoma.1                   \nMeranoplus.1                 \nParatrechina.2               \nMachomyrma.1                 \nNotoncus.1                   \nPheidole.6                   \nParatrechina.4               \nParatrechina.1               \nPheidole.7.1                 \nTetramorium.4             *  \nPristomyrmex.2               \nPristomyrmex.1            *  \nDoleromyrma.1                \nSolenopsis.1              *  \nCamponotus.consobrinus       \nIridomyrmex.purpureus        \nPapyrius.2                   \nMachomyrma.4              ** \nProlasius.6                  \nOchetellus.1                 \nColobostruma.1            *  \nCrematogaster.2              \nCamponotus.12                \nMesostruma.1                 \nRhopalomastix.3           .  \nMayriella.2                  \nMyrmecia.1                   \nMyrmecia.3                   \nHeteroponera.1               \nProlasius.3                  \nDoleromyrma.3                \nRhopalomastix.2              \nProlasius.7               *  \nStigmacros.2              *  \nAnonychromyrma.2          .  \nPolyrachis.7              *  \nStigmacros.1                 \nPheidole.1                *  \nParatrechina.6               \nProlasius.2                  \nProlasius.1                  \nStrumigenys.2                \nCamponotis.26             .  \nCamponotus.1              .  \nEpopostruma.1             .  \nPolyrachis.3                 \nPlagiolepis.1                \nAnonychromyrma.3          *  \nMyrmecorhrynchus.1        *  \nParatrechina.5            *  \nIridomyrmex.4                \nOligomyrmex.2                \nProlasius.4               *  \nNotoncus.3                   \nPolyrachis.2                 \nProbolomyrmex.1              \nTetramorium.5                \nPapyrius.1                   \nPheidole.2                   \nLeptomyrmex.1                \nStigmacros.3                 \nMayriella.1                  \nStrumigenys.1                \nColobostruma.2               \nCrematogaster.3              \nDisturbed.lost               \nDoleromyrma.2                \nDolichonderus.1              \nFroggattella.1               \nIridomyrmex.2                \nIridomyrmex.5                \nIridomyrmex.6                \nIridomyrmex.7                \nIridomyrmex.8                \nMachomyrma.3                 \nMachomyrma.6                 \nMelophorus.1                 \nMelophorus.2                 \nMeranoplus.2                 \nMyrmecia.2                   \nMyrmecia.4                   \nNotoncus.4                   \nOligomyrmex.1                \nPachychondyla.1              \nPachycondyla.2               \nPheidole.6.1                 \nPolyrachis.1                 \nPolyrachis.5                 \nPolyrachis.8                 \nRhopalomastix.1              \nRhopalomastix.5              \nStigmacros.4                 \nTechnomyrmex.1               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: CCIF_CPW \n\n                          average      sd   ratio     ava     avb cumsum     p\nPheidole.5                0.03206 0.03256 0.98460 1.22750 0.92190  0.047 0.946\nTapinoma.1                0.03099 0.02800 1.10670 0.65120 0.73330  0.093 0.494\nMeranoplus.1              0.02893 0.02737 1.05720 0.33620 0.65660  0.136 0.302\nIridomyrmex.purpureus     0.02750 0.02985 0.92150 0.26180 0.54280  0.176 0.025\nParatrechina.1            0.02696 0.02684 1.00460 0.25000 0.57810  0.216 0.175\nNotoncus.1                0.02613 0.02863 0.91270 0.37000 0.42060  0.255 0.590\nMachomyrma.1              0.02456 0.02646 0.92830 0.33620 0.43750  0.291 0.120\nPheidole.7                0.02400 0.02762 0.86890 0.38970 0.34410  0.327 0.830\nMesostruma.1              0.01697 0.02569 0.66080 0.00000 0.39440  0.352 0.029\nMonomorium.1              0.01693 0.02906 0.58250 0.29450 0.16760  0.377 0.870\nIridomyrmex.5             0.01657 0.02626 0.63080 0.00000 0.34800  0.401 0.001\nOchetellus.1              0.01623 0.02231 0.72760 0.12500 0.32430  0.425 0.102\nCamponotus.consobrinus    0.01607 0.02473 0.64990 0.12500 0.25000  0.449 0.153\nIridomyrmex.2             0.01595 0.02432 0.65580 0.12500 0.27370  0.473 0.118\nRhytidiponera..metallica. 0.01589 0.02204 0.72090 1.30250 1.45860  0.496 0.998\nAnonychromyrma.1          0.01553 0.03085 0.50350 0.25380 0.08230  0.519 0.995\nCamponotus.12             0.01540 0.02177 0.70720 0.19930 0.25000  0.542 0.061\nDoleromyrma.1             0.01473 0.02288 0.64370 0.18750 0.19930  0.564 0.292\nParatrechina.2            0.01424 0.02191 0.64980 0.12500 0.25000  0.585 0.890\nPheidole.6                0.01356 0.02101 0.64540 0.19930 0.18750  0.605 0.394\nPapyrius.2                0.01304 0.02789 0.46770 0.00000 0.21120  0.624 0.228\nProlasius.6               0.01288 0.02309 0.55760 0.00000 0.31540  0.643 0.149\nMayriella.2               0.01279 0.02114 0.60490 0.06250 0.25000  0.662 0.182\nIridomyrmex.7             0.01210 0.02174 0.55680 0.26180 0.00000  0.680 0.022\nCrematogaster.1           0.01193 0.02094 0.56950 0.00000 0.25000  0.697 0.897\nPristomyrmex.2            0.01182 0.02325 0.50820 0.07430 0.22520  0.715 0.718\nHeteroponera.1            0.01130 0.02161 0.52300 0.18750 0.07430  0.731 0.446\nStigmacros.3              0.01053 0.01998 0.52680 0.06250 0.21910  0.747 0.032\nPheidole.7.1              0.00985 0.02127 0.46320 0.00000 0.19930  0.761 0.570\nPolyrachis.5              0.00969 0.02078 0.46630 0.18750 0.00000  0.776 0.077\nRhopalomastix.1           0.00964 0.02216 0.43480 0.14480 0.07430  0.790 0.158\nMachomyrma.3              0.00950 0.02072 0.45850 0.18750 0.00000  0.804 0.284\nCrematogaster.3           0.00837 0.02340 0.35770 0.16760 0.00000  0.817 0.180\nParatrechina.4            0.00795 0.01827 0.43500 0.14480 0.06250  0.828 0.888\nIridomyrmex.4             0.00730 0.02037 0.35830 0.06250 0.09350  0.839 0.336\nPolyrachis.3              0.00729 0.01635 0.44580 0.06250 0.12500  0.850 0.166\nPolyrachis.2              0.00676 0.01508 0.44830 0.06250 0.12500  0.860 0.203\nNotoncus.4                0.00655 0.01775 0.36910 0.13680 0.00000  0.870 0.169\nPolyrachis.8              0.00626 0.01674 0.37380 0.00000 0.12500  0.879 0.037\nPachycondyla.2            0.00536 0.01456 0.36830 0.00000 0.12500  0.887 0.547\nDoleromyrma.3             0.00513 0.01372 0.37380 0.00000 0.12500  0.894 0.934\nMelophorus.2              0.00481 0.01332 0.36160 0.06250 0.07430  0.901 0.181\nPapyrius.1                0.00474 0.01322 0.35850 0.06250 0.06250  0.908 0.384\nProlasius.1               0.00460 0.01285 0.35820 0.06250 0.06250  0.915 0.582\nMyrmecia.2                0.00338 0.01323 0.25520 0.00000 0.06250  0.920 0.221\nDoleromyrma.2             0.00336 0.01335 0.25140 0.06250 0.00000  0.925 0.426\nPolyrachis.1              0.00327 0.01298 0.25170 0.06250 0.00000  0.930 0.411\nRhopalomastix.2           0.00322 0.01277 0.25190 0.06250 0.00000  0.935 0.658\nMayriella.1               0.00314 0.01228 0.25550 0.00000 0.06250  0.939 0.297\nOligomyrmex.1             0.00314 0.01228 0.25550 0.00000 0.06250  0.944 0.197\nMelophorus.1              0.00310 0.01228 0.25230 0.06250 0.00000  0.949 0.428\nStigmacros.4              0.00296 0.01157 0.25570 0.00000 0.06250  0.953 0.556\nIridomyrmex.6             0.00292 0.01143 0.25580 0.00000 0.06250  0.957 0.197\nFroggattella.1            0.00283 0.01118 0.25310 0.06250 0.00000  0.961 0.429\nStrumigenys.1             0.00283 0.01118 0.25310 0.06250 0.00000  0.966 0.680\nMeranoplus.2              0.00275 0.01072 0.25600 0.00000 0.06250  0.970 0.180\nMyrmecia.3                0.00275 0.01072 0.25600 0.00000 0.06250  0.974 0.857\nColobostruma.2            0.00263 0.01036 0.25370 0.06250 0.00000  0.978 0.442\nMachomyrma.6              0.00262 0.01033 0.25370 0.06250 0.00000  0.981 0.431\nPheidole.6.1              0.00238 0.00929 0.25640 0.00000 0.06250  0.985 0.200\nIridomyrmex.8             0.00204 0.00794 0.25670 0.00000 0.06250  0.988 0.194\nMyrmecia.1                0.00204 0.00794 0.25670 0.00000 0.06250  0.991 0.721\nMyrmecia.4                0.00204 0.00794 0.25670 0.00000 0.06250  0.994 0.194\nRhopalomastix.5           0.00204 0.00794 0.25670 0.00000 0.06250  0.997 0.194\nTetramorium.4             0.00204 0.00794 0.25670 0.00000 0.06250  1.000 0.904\nAnonychromyrma.2          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nAnonychromyrma.3          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotis.26             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nColobostruma.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCrematogaster.2           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDolichonderus.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nEpopostruma.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nLeptomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecorhrynchus.1        0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.3                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPachychondyla.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.5            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.6            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPlagiolepis.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.7              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPristomyrmex.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProbolomyrmex.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.2               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.3               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.4               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.7               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nSolenopsis.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStrumigenys.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTechnomyrmex.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.3             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                             \nPheidole.5                   \nTapinoma.1                   \nMeranoplus.1                 \nIridomyrmex.purpureus     *  \nParatrechina.1               \nNotoncus.1                   \nMachomyrma.1                 \nPheidole.7                   \nMesostruma.1              *  \nMonomorium.1                 \nIridomyrmex.5             ***\nOchetellus.1                 \nCamponotus.consobrinus       \nIridomyrmex.2                \nRhytidiponera..metallica.    \nAnonychromyrma.1             \nCamponotus.12             .  \nDoleromyrma.1                \nParatrechina.2               \nPheidole.6                   \nPapyrius.2                   \nProlasius.6                  \nMayriella.2                  \nIridomyrmex.7             *  \nCrematogaster.1              \nPristomyrmex.2               \nHeteroponera.1               \nStigmacros.3              *  \nPheidole.7.1                 \nPolyrachis.5              .  \nRhopalomastix.1              \nMachomyrma.3                 \nCrematogaster.3              \nParatrechina.4               \nIridomyrmex.4                \nPolyrachis.3                 \nPolyrachis.2                 \nNotoncus.4                   \nPolyrachis.8              *  \nPachycondyla.2               \nDoleromyrma.3                \nMelophorus.2                 \nPapyrius.1                   \nProlasius.1                  \nMyrmecia.2                   \nDoleromyrma.2                \nPolyrachis.1                 \nRhopalomastix.2              \nMayriella.1                  \nOligomyrmex.1                \nMelophorus.1                 \nStigmacros.4                 \nIridomyrmex.6                \nFroggattella.1               \nStrumigenys.1                \nMeranoplus.2                 \nMyrmecia.3                   \nColobostruma.2               \nMachomyrma.6                 \nPheidole.6.1                 \nIridomyrmex.8                \nMyrmecia.1                   \nMyrmecia.4                   \nRhopalomastix.5              \nTetramorium.4                \nAnonychromyrma.2             \nAnonychromyrma.3             \nCamponotis.26                \nCamponotus.1                 \nColobostruma.1               \nCrematogaster.2              \nDisturbed.lost               \nDolichonderus.1              \nEpopostruma.1                \nLeptomyrmex.1                \nMachomyrma.4                 \nMyrmecorhrynchus.1           \nNotoncus.3                   \nOligomyrmex.2                \nPachychondyla.1              \nParatrechina.5               \nParatrechina.6               \nPheidole.1                   \nPheidole.2                   \nPlagiolepis.1                \nPolyrachis.7                 \nPristomyrmex.1               \nProbolomyrmex.1              \nProlasius.2                  \nProlasius.3                  \nProlasius.4                  \nProlasius.7                  \nRhopalomastix.3              \nSolenopsis.1                 \nStigmacros.1                 \nStigmacros.2                 \nStrumigenys.2                \nTechnomyrmex.1               \nTetramorium.3                \nTetramorium.5                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: CCIF_SRW \n\n                          average      sd   ratio     ava     avb cumsum     p\nRhytidiponera..metallica. 0.04257 0.03775 1.12770 1.30250 0.79590  0.057 0.017\nPheidole.5                0.04182 0.03620 1.15540 1.22750 0.75730  0.113 0.144\nTapinoma.1                0.03669 0.03424 1.07160 0.65120 0.18750  0.163 0.022\nNotoncus.1                0.03334 0.03491 0.95500 0.37000 0.48480  0.207 0.024\nAnonychromyrma.1          0.03050 0.04018 0.75920 0.25380 0.35530  0.248 0.403\nPheidole.7                0.02855 0.03471 0.82260 0.38970 0.26180  0.287 0.429\nMeranoplus.1              0.02722 0.03452 0.78840 0.33620 0.24430  0.323 0.500\nCrematogaster.2           0.02652 0.03561 0.74470 0.00000 0.43640  0.359 0.006\nMonomorium.1              0.02564 0.03936 0.65160 0.29450 0.27240  0.393 0.348\nParatrechina.1            0.02503 0.03087 0.81080 0.25000 0.34410  0.427 0.380\nDoleromyrma.3             0.02279 0.04130 0.55180 0.00000 0.33670  0.458 0.023\nMachomyrma.1              0.02166 0.03247 0.66710 0.33620 0.06250  0.487 0.336\nParatrechina.2            0.02106 0.02861 0.73610 0.12500 0.33230  0.515 0.336\nIridomyrmex.purpureus     0.01723 0.02873 0.59990 0.26180 0.06250  0.538 0.639\nHeteroponera.1            0.01655 0.02894 0.57170 0.18750 0.12500  0.560 0.046\nMachomyrma.3              0.01645 0.02882 0.57080 0.18750 0.12500  0.583 0.004\nTetramorium.3             0.01577 0.02844 0.55440 0.00000 0.29710  0.604 0.924\nIridomyrmex.7             0.01509 0.02700 0.55880 0.26180 0.00000  0.624 0.001\nParatrechina.4            0.01444 0.02783 0.51880 0.14480 0.12500  0.643 0.367\nDoleromyrma.1             0.01419 0.02703 0.52470 0.18750 0.06250  0.662 0.348\nIridomyrmex.2             0.01410 0.02789 0.50560 0.12500 0.12500  0.681 0.242\nPristomyrmex.2            0.01367 0.02639 0.51790 0.07430 0.21340  0.700 0.561\nPolyrachis.5              0.01242 0.02652 0.46830 0.18750 0.00000  0.716 0.001\nRhopalomastix.1           0.01228 0.02825 0.43460 0.14480 0.06250  0.733 0.026\nPheidole.6                0.01166 0.02200 0.52990 0.19930 0.06250  0.749 0.580\nCrematogaster.3           0.01062 0.02951 0.35990 0.16760 0.00000  0.763 0.001\nCamponotus.consobrinus    0.00998 0.02307 0.43250 0.12500 0.06250  0.776 0.711\nCamponotus.12             0.00976 0.02087 0.46790 0.19930 0.00000  0.789 0.537\nOchetellus.1              0.00876 0.01988 0.44060 0.12500 0.06250  0.801 0.781\nNotoncus.4                0.00824 0.02220 0.37100 0.13680 0.00000  0.812 0.001\nTechnomyrmex.1            0.00806 0.02172 0.37120 0.00000 0.12500  0.823 0.034\nMyrmecia.3                0.00804 0.02185 0.36780 0.00000 0.12500  0.834 0.360\nPachycondyla.2            0.00793 0.02127 0.37260 0.00000 0.13680  0.844 0.213\nProlasius.3               0.00733 0.01982 0.36990 0.00000 0.13680  0.854 0.247\nIridomyrmex.4             0.00717 0.02019 0.35530 0.06250 0.06250  0.864 0.371\nDolichonderus.1           0.00707 0.01899 0.37250 0.00000 0.12500  0.873 0.034\nStigmacros.1              0.00679 0.01819 0.37320 0.00000 0.12500  0.882 0.158\nStrumigenys.1             0.00657 0.01830 0.35900 0.06250 0.06250  0.891 0.113\nProlasius.1               0.00650 0.01816 0.35820 0.06250 0.06250  0.900 0.333\nMayriella.2               0.00630 0.01755 0.35920 0.06250 0.06250  0.909 0.792\nDoleromyrma.2             0.00434 0.01721 0.25250 0.06250 0.00000  0.914 0.006\nPolyrachis.1              0.00420 0.01660 0.25280 0.06250 0.00000  0.920 0.007\nRhopalomastix.2           0.00412 0.01626 0.25300 0.06250 0.00000  0.926 0.416\nMelophorus.1              0.00392 0.01548 0.25350 0.06250 0.00000  0.931 0.008\nCrematogaster.1           0.00390 0.01533 0.25440 0.00000 0.06250  0.936 0.998\nFroggattella.1            0.00351 0.01380 0.25430 0.06250 0.00000  0.941 0.006\nPapyrius.2                0.00346 0.01356 0.25510 0.00000 0.06250  0.945 0.953\nSolenopsis.1              0.00344 0.01348 0.25510 0.00000 0.06250  0.950 0.712\nPachychondyla.1           0.00339 0.01327 0.25520 0.00000 0.06250  0.955 0.182\nPolyrachis.2              0.00328 0.01289 0.25480 0.06250 0.00000  0.959 0.646\nColobostruma.2            0.00320 0.01257 0.25490 0.06250 0.00000  0.963 0.005\nPheidole.7.1              0.00320 0.01253 0.25540 0.00000 0.06250  0.968 0.966\nProlasius.2               0.00320 0.01253 0.25540 0.00000 0.06250  0.972 0.565\nStigmacros.4              0.00320 0.01253 0.25540 0.00000 0.06250  0.976 0.313\nMachomyrma.6              0.00320 0.01253 0.25490 0.06250 0.00000  0.981 0.008\nMelophorus.2              0.00320 0.01253 0.25490 0.06250 0.00000  0.985 0.171\nPolyrachis.3              0.00320 0.01253 0.25490 0.06250 0.00000  0.989 0.592\nOligomyrmex.2             0.00278 0.01087 0.25590 0.00000 0.06250  0.993 0.474\nPapyrius.1                0.00267 0.01044 0.25580 0.06250 0.00000  0.996 0.630\nStigmacros.3              0.00267 0.01044 0.25580 0.06250 0.00000  1.000 0.808\nAnonychromyrma.2          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nAnonychromyrma.3          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotis.26             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nColobostruma.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nEpopostruma.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.6             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.8             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nLeptomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMayriella.1               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMeranoplus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMesostruma.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecorhrynchus.1        0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.3                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.5            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.6            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.6.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPlagiolepis.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.7              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.8              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPristomyrmex.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProbolomyrmex.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.4               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.6               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.7               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.5           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStrumigenys.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.4             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                             \nRhytidiponera..metallica. *  \nPheidole.5                   \nTapinoma.1                *  \nNotoncus.1                *  \nAnonychromyrma.1             \nPheidole.7                   \nMeranoplus.1                 \nCrematogaster.2           ** \nMonomorium.1                 \nParatrechina.1               \nDoleromyrma.3             *  \nMachomyrma.1                 \nParatrechina.2               \nIridomyrmex.purpureus        \nHeteroponera.1            *  \nMachomyrma.3              ** \nTetramorium.3                \nIridomyrmex.7             ***\nParatrechina.4               \nDoleromyrma.1                \nIridomyrmex.2                \nPristomyrmex.2               \nPolyrachis.5              ***\nRhopalomastix.1           *  \nPheidole.6                   \nCrematogaster.3           ***\nCamponotus.consobrinus       \nCamponotus.12                \nOchetellus.1                 \nNotoncus.4                ***\nTechnomyrmex.1            *  \nMyrmecia.3                   \nPachycondyla.2               \nProlasius.3                  \nIridomyrmex.4                \nDolichonderus.1           *  \nStigmacros.1                 \nStrumigenys.1                \nProlasius.1                  \nMayriella.2                  \nDoleromyrma.2             ** \nPolyrachis.1              ** \nRhopalomastix.2              \nMelophorus.1              ** \nCrematogaster.1              \nFroggattella.1            ** \nPapyrius.2                   \nSolenopsis.1                 \nPachychondyla.1              \nPolyrachis.2                 \nColobostruma.2            ** \nPheidole.7.1                 \nProlasius.2                  \nStigmacros.4                 \nMachomyrma.6              ** \nMelophorus.2                 \nPolyrachis.3                 \nOligomyrmex.2                \nPapyrius.1                   \nStigmacros.3                 \nAnonychromyrma.2             \nAnonychromyrma.3             \nCamponotis.26                \nCamponotus.1                 \nColobostruma.1               \nDisturbed.lost               \nEpopostruma.1                \nIridomyrmex.5                \nIridomyrmex.6                \nIridomyrmex.8                \nLeptomyrmex.1                \nMachomyrma.4                 \nMayriella.1                  \nMeranoplus.2                 \nMesostruma.1                 \nMyrmecia.1                   \nMyrmecia.2                   \nMyrmecia.4                   \nMyrmecorhrynchus.1           \nNotoncus.3                   \nOligomyrmex.1                \nParatrechina.5               \nParatrechina.6               \nPheidole.1                   \nPheidole.2                   \nPheidole.6.1                 \nPlagiolepis.1                \nPolyrachis.7                 \nPolyrachis.8                 \nPristomyrmex.1               \nProbolomyrmex.1              \nProlasius.4                  \nProlasius.6                  \nProlasius.7                  \nRhopalomastix.3              \nRhopalomastix.5              \nStigmacros.2                 \nStrumigenys.2                \nTetramorium.4                \nTetramorium.5                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: CCIF_SSTF \n\n                          average      sd   ratio     ava     avb cumsum     p\nTapinoma.1                0.03262 0.03060 1.06610 0.65120 0.38460  0.048 0.271\nAnonychromyrma.1          0.03136 0.03725 0.84210 0.25380 0.49420  0.094 0.344\nCrematogaster.1           0.02882 0.03195 0.90180 0.00000 0.56960  0.136 0.008\nPheidole.5                0.02692 0.03286 0.81920 1.22750 1.08140  0.175 0.994\nMachomyrma.1              0.02656 0.02988 0.88910 0.33620 0.41370  0.214 0.058\nPheidole.7                0.02651 0.03231 0.82060 0.38970 0.24530  0.253 0.590\nNotoncus.1                0.02578 0.03264 0.78980 0.37000 0.24530  0.290 0.637\nRhytidiponera..metallica. 0.02298 0.03038 0.75630 1.30250 1.22640  0.324 0.910\nMeranoplus.1              0.02210 0.02966 0.74510 0.33620 0.15380  0.356 0.857\nIridomyrmex.purpureus     0.02041 0.03009 0.67830 0.26180 0.20030  0.386 0.396\nPheidole.6                0.02040 0.02656 0.76820 0.19930 0.32220  0.416 0.015\nMonomorium.1              0.02023 0.03295 0.61410 0.29450 0.20030  0.446 0.725\nParatrechina.2            0.01989 0.02774 0.71710 0.12500 0.32220  0.474 0.457\nDoleromyrma.1             0.01899 0.02775 0.68420 0.18750 0.23080  0.502 0.060\nParatrechina.1            0.01753 0.02640 0.66420 0.25000 0.15380  0.528 0.933\nCamponotus.consobrinus    0.01541 0.02442 0.63080 0.12500 0.24530  0.550 0.211\nTetramorium.3             0.01462 0.02929 0.49920 0.00000 0.25510  0.572 0.934\nCamponotus.12             0.01447 0.02343 0.61770 0.19930 0.15380  0.593 0.108\nOchetellus.1              0.01412 0.02598 0.54340 0.12500 0.16840  0.614 0.273\nIridomyrmex.7             0.01378 0.02463 0.55940 0.26180 0.00000  0.634 0.008\nPolyrachis.5              0.01120 0.02387 0.46900 0.18750 0.00000  0.650 0.034\nHeteroponera.1            0.01100 0.02391 0.45990 0.18750 0.00000  0.666 0.440\nMachomyrma.3              0.01099 0.02394 0.45900 0.18750 0.00000  0.682 0.171\nParatrechina.4            0.01052 0.02267 0.46410 0.14480 0.09150  0.698 0.689\nMachomyrma.4              0.01022 0.02464 0.41490 0.00000 0.15380  0.713 0.015\nPristomyrmex.2            0.00983 0.02038 0.48230 0.07430 0.15380  0.727 0.841\nCrematogaster.3           0.00962 0.02673 0.35990 0.16760 0.00000  0.741 0.115\nRhopalomastix.1           0.00884 0.02426 0.36460 0.14480 0.00000  0.754 0.257\nPapyrius.2                0.00861 0.02074 0.41500 0.00000 0.15380  0.767 0.617\nProlasius.6               0.00777 0.01909 0.40710 0.00000 0.16840  0.778 0.595\nNotoncus.4                0.00749 0.02019 0.37110 0.13680 0.00000  0.789 0.105\nIridomyrmex.4             0.00682 0.01819 0.37510 0.06250 0.07690  0.799 0.446\nIridomyrmex.2             0.00664 0.01802 0.36860 0.12500 0.00000  0.809 0.834\nPolyrachis.3              0.00624 0.01635 0.38150 0.06250 0.07690  0.818 0.269\nMayriella.2               0.00607 0.01591 0.38170 0.06250 0.07690  0.827 0.826\nCrematogaster.2           0.00570 0.02016 0.28280 0.00000 0.07690  0.835 0.911\nMesostruma.1              0.00549 0.01929 0.28450 0.00000 0.09150  0.843 0.821\nStigmacros.3              0.00513 0.01339 0.38320 0.06250 0.07690  0.850 0.556\nProlasius.7               0.00492 0.01732 0.28400 0.00000 0.07690  0.858 0.084\nStigmacros.2              0.00485 0.01699 0.28520 0.00000 0.09150  0.865 0.068\nAnonychromyrma.2          0.00461 0.01622 0.28450 0.00000 0.07690  0.871 0.069\nPolyrachis.7              0.00435 0.01529 0.28490 0.00000 0.07690  0.878 0.086\nStigmacros.1              0.00435 0.01529 0.28490 0.00000 0.07690  0.884 0.409\nPheidole.1                0.00433 0.01518 0.28490 0.00000 0.07690  0.890 0.089\nPristomyrmex.1            0.00407 0.01429 0.28520 0.00000 0.07690  0.896 0.625\nDoleromyrma.2             0.00390 0.01542 0.25280 0.06250 0.00000  0.902 0.272\nPolyrachis.1              0.00378 0.01493 0.25310 0.06250 0.00000  0.908 0.257\nRhopalomastix.2           0.00371 0.01466 0.25320 0.06250 0.00000  0.913 0.558\nCamponotis.26             0.00369 0.01292 0.28570 0.00000 0.07690  0.918 0.066\nCamponotus.1              0.00369 0.01292 0.28570 0.00000 0.07690  0.924 0.066\nEpopostruma.1             0.00369 0.01292 0.28570 0.00000 0.07690  0.929 0.066\nMelophorus.1              0.00356 0.01402 0.25360 0.06250 0.00000  0.934 0.279\nAnonychromyrma.3          0.00351 0.01228 0.28590 0.00000 0.07690  0.940 0.077\nMyrmecorhrynchus.1        0.00351 0.01228 0.28590 0.00000 0.07690  0.945 0.077\nParatrechina.5            0.00351 0.01228 0.28590 0.00000 0.07690  0.950 0.077\nProlasius.3               0.00351 0.01228 0.28590 0.00000 0.07690  0.955 0.689\nOligomyrmex.2             0.00351 0.01227 0.28590 0.00000 0.07690  0.960 0.213\nProlasius.4               0.00351 0.01227 0.28590 0.00000 0.07690  0.965 0.084\nFroggattella.1            0.00321 0.01264 0.25430 0.06250 0.00000  0.970 0.255\nStrumigenys.1             0.00321 0.01264 0.25430 0.06250 0.00000  0.975 0.592\nPolyrachis.2              0.00302 0.01187 0.25460 0.06250 0.00000  0.979 0.652\nColobostruma.2            0.00296 0.01160 0.25470 0.06250 0.00000  0.983 0.280\nProlasius.1               0.00296 0.01160 0.25470 0.06250 0.00000  0.988 0.756\nMachomyrma.6              0.00295 0.01157 0.25470 0.06250 0.00000  0.992 0.280\nMelophorus.2              0.00295 0.01157 0.25470 0.06250 0.00000  0.996 0.398\nPapyrius.1                0.00250 0.00977 0.25550 0.06250 0.00000  1.000 0.749\nColobostruma.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDoleromyrma.3             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDolichonderus.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.6             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.8             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nLeptomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMayriella.1               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMeranoplus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.3                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.3                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPachychondyla.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPachycondyla.2            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.6            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.6.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.7.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPlagiolepis.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.8              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProbolomyrmex.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.2               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.5           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nSolenopsis.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStrumigenys.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTechnomyrmex.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.4             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                            \nTapinoma.1                  \nAnonychromyrma.1            \nCrematogaster.1           **\nPheidole.5                  \nMachomyrma.1              . \nPheidole.7                  \nNotoncus.1                  \nRhytidiponera..metallica.   \nMeranoplus.1                \nIridomyrmex.purpureus       \nPheidole.6                * \nMonomorium.1                \nParatrechina.2              \nDoleromyrma.1             . \nParatrechina.1              \nCamponotus.consobrinus      \nTetramorium.3               \nCamponotus.12               \nOchetellus.1                \nIridomyrmex.7             **\nPolyrachis.5              * \nHeteroponera.1              \nMachomyrma.3                \nParatrechina.4              \nMachomyrma.4              * \nPristomyrmex.2              \nCrematogaster.3             \nRhopalomastix.1             \nPapyrius.2                  \nProlasius.6                 \nNotoncus.4                  \nIridomyrmex.4               \nIridomyrmex.2               \nPolyrachis.3                \nMayriella.2                 \nCrematogaster.2             \nMesostruma.1                \nStigmacros.3                \nProlasius.7               . \nStigmacros.2              . \nAnonychromyrma.2          . \nPolyrachis.7              . \nStigmacros.1                \nPheidole.1                . \nPristomyrmex.1              \nDoleromyrma.2               \nPolyrachis.1                \nRhopalomastix.2             \nCamponotis.26             . \nCamponotus.1              . \nEpopostruma.1             . \nMelophorus.1                \nAnonychromyrma.3          . \nMyrmecorhrynchus.1        . \nParatrechina.5            . \nProlasius.3                 \nOligomyrmex.2               \nProlasius.4               . \nFroggattella.1              \nStrumigenys.1               \nPolyrachis.2                \nColobostruma.2              \nProlasius.1                 \nMachomyrma.6                \nMelophorus.2                \nPapyrius.1                  \nColobostruma.1              \nDisturbed.lost              \nDoleromyrma.3               \nDolichonderus.1             \nIridomyrmex.5               \nIridomyrmex.6               \nIridomyrmex.8               \nLeptomyrmex.1               \nMayriella.1                 \nMeranoplus.2                \nMyrmecia.1                  \nMyrmecia.2                  \nMyrmecia.3                  \nMyrmecia.4                  \nNotoncus.3                  \nOligomyrmex.1               \nPachychondyla.1             \nPachycondyla.2              \nParatrechina.6              \nPheidole.2                  \nPheidole.6.1                \nPheidole.7.1                \nPlagiolepis.1               \nPolyrachis.8                \nProbolomyrmex.1             \nProlasius.2                 \nRhopalomastix.3             \nRhopalomastix.5             \nSolenopsis.1                \nStigmacros.4                \nStrumigenys.2               \nTechnomyrmex.1              \nTetramorium.4               \nTetramorium.5               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: CPW_SRW \n\n                          average      sd   ratio     ava     avb cumsum     p\nRhytidiponera..metallica. 0.03822 0.03415 1.11900 1.45860 0.79590  0.050 0.088\nPheidole.5                0.03590 0.03273 1.09680 0.92190 0.75730  0.097 0.695\nTapinoma.1                0.03476 0.03181 1.09270 0.73330 0.18750  0.142 0.063\nMeranoplus.1              0.03390 0.03020 1.12240 0.65660 0.24430  0.186 0.023\nParatrechina.1            0.03007 0.02943 1.02200 0.57810 0.34410  0.225 0.031\nNotoncus.1                0.02899 0.02993 0.96880 0.42060 0.48480  0.263 0.289\nIridomyrmex.purpureus     0.02813 0.03357 0.83820 0.54280 0.06250  0.300 0.016\nMachomyrma.1              0.02380 0.02847 0.83590 0.43750 0.06250  0.331 0.167\nCrematogaster.2           0.02281 0.03108 0.73380 0.00000 0.43640  0.361 0.015\nDoleromyrma.3             0.02183 0.03467 0.62960 0.12500 0.33670  0.389 0.042\nPheidole.7                0.02160 0.02632 0.82050 0.34410 0.26180  0.417 0.920\nParatrechina.2            0.02071 0.02575 0.80440 0.25000 0.33230  0.444 0.369\nAnonychromyrma.1          0.01960 0.02853 0.68720 0.08230 0.35530  0.470 0.978\nMesostruma.1              0.01824 0.02777 0.65690 0.39440 0.00000  0.494 0.004\nIridomyrmex.5             0.01798 0.02888 0.62260 0.34800 0.00000  0.517 0.001\nIridomyrmex.2             0.01790 0.02762 0.64800 0.27370 0.12500  0.540 0.035\nPristomyrmex.2            0.01765 0.02843 0.62100 0.22520 0.21340  0.563 0.238\nMonomorium.1              0.01651 0.02851 0.57900 0.16760 0.27240  0.585 0.889\nPapyrius.2                0.01607 0.03135 0.51270 0.21120 0.06250  0.606 0.070\nCamponotus.consobrinus    0.01606 0.02662 0.60330 0.25000 0.06250  0.627 0.154\nOchetellus.1              0.01592 0.02311 0.68860 0.32430 0.06250  0.648 0.138\nCrematogaster.1           0.01460 0.02391 0.61060 0.25000 0.06250  0.667 0.756\nMayriella.2               0.01391 0.02317 0.60040 0.25000 0.06250  0.685 0.095\nTetramorium.3             0.01377 0.02512 0.54840 0.00000 0.29710  0.703 0.961\nProlasius.6               0.01375 0.02477 0.55500 0.31540 0.00000  0.721 0.097\nPheidole.7.1              0.01245 0.02412 0.51630 0.19930 0.06250  0.737 0.351\nCamponotus.12             0.01152 0.02073 0.55600 0.25000 0.00000  0.752 0.369\nPachycondyla.2            0.01123 0.02188 0.51330 0.12500 0.13680  0.767 0.039\nDoleromyrma.1             0.01093 0.02069 0.52810 0.19930 0.06250  0.781 0.657\nPheidole.6                0.01029 0.01974 0.52150 0.18750 0.06250  0.794 0.690\nStigmacros.3              0.00964 0.02077 0.46410 0.21910 0.00000  0.807 0.082\nMyrmecia.3                0.00905 0.02061 0.43880 0.06250 0.12500  0.819 0.277\nParatrechina.4            0.00870 0.02024 0.42990 0.06250 0.12500  0.830 0.819\nHeteroponera.1            0.00839 0.01875 0.44750 0.07430 0.12500  0.841 0.722\nIridomyrmex.4             0.00740 0.02104 0.35190 0.09350 0.06250  0.851 0.321\nTechnomyrmex.1            0.00689 0.01879 0.36650 0.00000 0.12500  0.860 0.179\nPolyrachis.8              0.00678 0.01822 0.37230 0.12500 0.00000  0.868 0.002\nProlasius.3               0.00640 0.01748 0.36640 0.00000 0.13680  0.877 0.390\nDolichonderus.1           0.00614 0.01666 0.36850 0.00000 0.12500  0.885 0.184\nMachomyrma.3              0.00604 0.01635 0.36910 0.00000 0.12500  0.893 0.684\nStigmacros.1              0.00592 0.01603 0.36940 0.00000 0.12500  0.900 0.355\nStigmacros.4              0.00566 0.01584 0.35730 0.06250 0.06250  0.908 0.160\nPolyrachis.3              0.00561 0.01525 0.36800 0.12500 0.00000  0.915 0.356\nRhopalomastix.1           0.00545 0.01512 0.36070 0.07430 0.06250  0.922 0.569\nProlasius.1               0.00525 0.01488 0.35280 0.06250 0.06250  0.929 0.538\nPolyrachis.2              0.00488 0.01303 0.37440 0.12500 0.00000  0.935 0.506\nMyrmecia.2                0.00369 0.01451 0.25400 0.06250 0.00000  0.940 0.006\nMayriella.1               0.00340 0.01337 0.25450 0.06250 0.00000  0.945 0.175\nOligomyrmex.1             0.00340 0.01337 0.25450 0.06250 0.00000  0.949 0.004\nIridomyrmex.6             0.00315 0.01236 0.25500 0.06250 0.00000  0.953 0.004\nStrumigenys.1             0.00301 0.01192 0.25260 0.00000 0.06250  0.957 0.647\nSolenopsis.1              0.00299 0.01185 0.25260 0.00000 0.06250  0.961 0.862\nPachychondyla.1           0.00295 0.01169 0.25270 0.00000 0.06250  0.965 0.427\nMeranoplus.2              0.00294 0.01153 0.25530 0.06250 0.00000  0.969 0.005\nPapyrius.1                0.00294 0.01153 0.25530 0.06250 0.00000  0.973 0.512\nProlasius.2               0.00281 0.01110 0.25320 0.00000 0.06250  0.976 0.675\nMelophorus.2              0.00255 0.00994 0.25650 0.07430 0.00000  0.980 0.431\nPheidole.6.1              0.00253 0.00988 0.25600 0.06250 0.00000  0.983 0.005\nOligomyrmex.2             0.00248 0.00975 0.25410 0.00000 0.06250  0.986 0.655\nIridomyrmex.8             0.00214 0.00836 0.25650 0.06250 0.00000  0.989 0.003\nMyrmecia.1                0.00214 0.00836 0.25650 0.06250 0.00000  0.992 0.685\nMyrmecia.4                0.00214 0.00836 0.25650 0.06250 0.00000  0.994 0.003\nRhopalomastix.5           0.00214 0.00836 0.25650 0.06250 0.00000  0.997 0.003\nTetramorium.4             0.00214 0.00836 0.25650 0.06250 0.00000  1.000 0.887\nAnonychromyrma.2          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nAnonychromyrma.3          0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotis.26             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCamponotus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nColobostruma.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nColobostruma.2            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCrematogaster.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDoleromyrma.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nEpopostruma.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nFroggattella.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.7             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nLeptomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.4              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.6              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMelophorus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecorhrynchus.1        0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.3                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.5            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.6            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPlagiolepis.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.5              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.7              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPristomyrmex.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProbolomyrmex.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.4               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProlasius.7               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.2           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStigmacros.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStrumigenys.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                             \nRhytidiponera..metallica. .  \nPheidole.5                   \nTapinoma.1                .  \nMeranoplus.1              *  \nParatrechina.1            *  \nNotoncus.1                   \nIridomyrmex.purpureus     *  \nMachomyrma.1                 \nCrematogaster.2           *  \nDoleromyrma.3             *  \nPheidole.7                   \nParatrechina.2               \nAnonychromyrma.1             \nMesostruma.1              ** \nIridomyrmex.5             ***\nIridomyrmex.2             *  \nPristomyrmex.2               \nMonomorium.1                 \nPapyrius.2                .  \nCamponotus.consobrinus       \nOchetellus.1                 \nCrematogaster.1              \nMayriella.2               .  \nTetramorium.3                \nProlasius.6               .  \nPheidole.7.1                 \nCamponotus.12                \nPachycondyla.2            *  \nDoleromyrma.1                \nPheidole.6                   \nStigmacros.3              .  \nMyrmecia.3                   \nParatrechina.4               \nHeteroponera.1               \nIridomyrmex.4                \nTechnomyrmex.1               \nPolyrachis.8              ** \nProlasius.3                  \nDolichonderus.1              \nMachomyrma.3                 \nStigmacros.1                 \nStigmacros.4                 \nPolyrachis.3                 \nRhopalomastix.1              \nProlasius.1                  \nPolyrachis.2                 \nMyrmecia.2                ** \nMayriella.1                  \nOligomyrmex.1             ** \nIridomyrmex.6             ** \nStrumigenys.1                \nSolenopsis.1                 \nPachychondyla.1              \nMeranoplus.2              ** \nPapyrius.1                   \nProlasius.2                  \nMelophorus.2                 \nPheidole.6.1              ** \nOligomyrmex.2                \nIridomyrmex.8             ** \nMyrmecia.1                   \nMyrmecia.4                ** \nRhopalomastix.5           ** \nTetramorium.4                \nAnonychromyrma.2             \nAnonychromyrma.3             \nCamponotis.26                \nCamponotus.1                 \nColobostruma.1               \nColobostruma.2               \nCrematogaster.3              \nDisturbed.lost               \nDoleromyrma.2                \nEpopostruma.1                \nFroggattella.1               \nIridomyrmex.7                \nLeptomyrmex.1                \nMachomyrma.4                 \nMachomyrma.6                 \nMelophorus.1                 \nMyrmecorhrynchus.1           \nNotoncus.3                   \nNotoncus.4                   \nParatrechina.5               \nParatrechina.6               \nPheidole.1                   \nPheidole.2                   \nPlagiolepis.1                \nPolyrachis.1                 \nPolyrachis.5                 \nPolyrachis.7                 \nPristomyrmex.1               \nProbolomyrmex.1              \nProlasius.4                  \nProlasius.7                  \nRhopalomastix.2              \nRhopalomastix.3              \nStigmacros.2                 \nStrumigenys.2                \nTetramorium.5                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: CPW_SSTF \n\n                           average       sd    ratio      ava      avb cumsum\nTapinoma.1                0.030263 0.028420 1.065000 0.733300 0.384600  0.044\nPheidole.5                0.029517 0.030650 0.963000 0.921900 1.081400  0.086\nMeranoplus.1              0.029385 0.027290 1.076900 0.656600 0.153800  0.128\nIridomyrmex.purpureus     0.027624 0.031640 0.873200 0.542800 0.200300  0.168\nCrematogaster.1           0.026790 0.027370 0.979000 0.250000 0.569600  0.207\nParatrechina.1            0.026602 0.027150 0.979800 0.578100 0.153800  0.245\nMachomyrma.1              0.024562 0.025890 0.948600 0.437500 0.413700  0.281\nNotoncus.1                0.023488 0.027970 0.839900 0.420600 0.245300  0.315\nAnonychromyrma.1          0.022670 0.028920 0.783900 0.082300 0.494200  0.347\nPheidole.7                0.020393 0.025500 0.799600 0.344100 0.245300  0.377\nParatrechina.2            0.019403 0.024620 0.788000 0.250000 0.322200  0.405\nMesostruma.1              0.019143 0.026980 0.709500 0.394400 0.091500  0.432\nCamponotus.consobrinus    0.018233 0.024860 0.733500 0.250000 0.245300  0.458\nOchetellus.1              0.018070 0.024570 0.735500 0.324300 0.168400  0.485\nPheidole.6                0.017694 0.023450 0.754400 0.187500 0.322200  0.510\nPapyrius.2                0.016933 0.028240 0.599700 0.211200 0.153800  0.534\nProlasius.6               0.016813 0.024780 0.678600 0.315400 0.168400  0.559\nRhytidiponera..metallica. 0.016750 0.022810 0.734300 1.458600 1.226400  0.583\nIridomyrmex.5             0.016495 0.026310 0.627000 0.348000 0.000000  0.607\nDoleromyrma.1             0.016024 0.023380 0.685200 0.199300 0.230800  0.630\nCamponotus.12             0.014520 0.021640 0.670900 0.250000 0.153800  0.651\nPristomyrmex.2            0.014348 0.024190 0.593200 0.225200 0.153800  0.671\nMayriella.2               0.012995 0.021210 0.612800 0.250000 0.076900  0.690\nIridomyrmex.2             0.012878 0.023190 0.555400 0.273700 0.000000  0.709\nTetramorium.3             0.012610 0.025380 0.496800 0.000000 0.255100  0.727\nMonomorium.1              0.011954 0.021280 0.561800 0.167600 0.200300  0.744\nStigmacros.3              0.010737 0.019980 0.537400 0.219100 0.076900  0.760\nPheidole.7.1              0.009809 0.021290 0.460700 0.199300 0.000000  0.774\nMachomyrma.4              0.008692 0.021210 0.409900 0.000000 0.153800  0.786\nPolyrachis.3              0.007763 0.016830 0.461200 0.125000 0.076900  0.797\nIridomyrmex.4             0.007139 0.019360 0.368800 0.093500 0.076900  0.808\nPolyrachis.8              0.006228 0.016740 0.372100 0.125000 0.000000  0.817\nParatrechina.4            0.005458 0.014550 0.375000 0.062500 0.091500  0.825\nPachycondyla.2            0.005335 0.014540 0.366800 0.125000 0.000000  0.832\nDoleromyrma.3             0.005103 0.013700 0.372500 0.125000 0.000000  0.840\nCrematogaster.2           0.004783 0.017160 0.278700 0.000000 0.076900  0.847\nPolyrachis.2              0.004583 0.012250 0.374000 0.125000 0.000000  0.853\nStigmacros.2              0.004238 0.015000 0.282500 0.000000 0.091500  0.859\nProlasius.7               0.004208 0.014990 0.280600 0.000000 0.076900  0.865\nAnonychromyrma.2          0.003978 0.014140 0.281300 0.000000 0.076900  0.871\nPolyrachis.7              0.003780 0.013410 0.281900 0.000000 0.076900  0.876\nStigmacros.1              0.003780 0.013410 0.281900 0.000000 0.076900  0.882\nPheidole.1                0.003758 0.013330 0.282000 0.000000 0.076900  0.887\nPristomyrmex.1            0.003564 0.012610 0.282500 0.000000 0.076900  0.892\nMyrmecia.2                0.003360 0.013230 0.254000 0.062500 0.000000  0.897\nCamponotis.26             0.003262 0.011510 0.283400 0.000000 0.076900  0.902\nCamponotus.1              0.003262 0.011510 0.283400 0.000000 0.076900  0.907\nEpopostruma.1             0.003262 0.011510 0.283400 0.000000 0.076900  0.911\nMayriella.1               0.003123 0.012280 0.254400 0.062500 0.000000  0.916\nOligomyrmex.1             0.003123 0.012280 0.254400 0.062500 0.000000  0.920\nAnonychromyrma.3          0.003118 0.010990 0.283700 0.000000 0.076900  0.925\nMyrmecorhrynchus.1        0.003118 0.010990 0.283700 0.000000 0.076900  0.929\nParatrechina.5            0.003118 0.010990 0.283700 0.000000 0.076900  0.934\nProlasius.3               0.003118 0.010990 0.283700 0.000000 0.076900  0.938\nOligomyrmex.2             0.003117 0.010990 0.283700 0.000000 0.076900  0.943\nProlasius.4               0.003117 0.010990 0.283700 0.000000 0.076900  0.947\nStigmacros.4              0.002945 0.011560 0.254800 0.062500 0.000000  0.952\nIridomyrmex.6             0.002910 0.011420 0.254800 0.062500 0.000000  0.956\nMeranoplus.2              0.002732 0.010710 0.255100 0.062500 0.000000  0.960\nMyrmecia.3                0.002732 0.010710 0.255100 0.062500 0.000000  0.964\nPapyrius.1                0.002732 0.010710 0.255100 0.062500 0.000000  0.968\nHeteroponera.1            0.002634 0.010290 0.255900 0.074300 0.000000  0.972\nRhopalomastix.1           0.002634 0.010290 0.255900 0.074300 0.000000  0.975\nMelophorus.2              0.002413 0.009420 0.256200 0.074300 0.000000  0.979\nPheidole.6.1              0.002371 0.009270 0.255700 0.062500 0.000000  0.982\nProlasius.1               0.002215 0.008660 0.255900 0.062500 0.000000  0.985\nIridomyrmex.8             0.002029 0.007920 0.256200 0.062500 0.000000  0.988\nMyrmecia.1                0.002029 0.007920 0.256200 0.062500 0.000000  0.991\nMyrmecia.4                0.002029 0.007920 0.256200 0.062500 0.000000  0.994\nRhopalomastix.5           0.002029 0.007920 0.256200 0.062500 0.000000  0.997\nTetramorium.4             0.002029 0.007920 0.256200 0.062500 0.000000  1.000\nColobostruma.1            0.000000 0.000000      NaN 0.000000 0.000000  1.000\nColobostruma.2            0.000000 0.000000      NaN 0.000000 0.000000  1.000\nCrematogaster.3           0.000000 0.000000      NaN 0.000000 0.000000  1.000\nDisturbed.lost            0.000000 0.000000      NaN 0.000000 0.000000  1.000\nDoleromyrma.2             0.000000 0.000000      NaN 0.000000 0.000000  1.000\nDolichonderus.1           0.000000 0.000000      NaN 0.000000 0.000000  1.000\nFroggattella.1            0.000000 0.000000      NaN 0.000000 0.000000  1.000\nIridomyrmex.7             0.000000 0.000000      NaN 0.000000 0.000000  1.000\nLeptomyrmex.1             0.000000 0.000000      NaN 0.000000 0.000000  1.000\nMachomyrma.3              0.000000 0.000000      NaN 0.000000 0.000000  1.000\nMachomyrma.6              0.000000 0.000000      NaN 0.000000 0.000000  1.000\nMelophorus.1              0.000000 0.000000      NaN 0.000000 0.000000  1.000\nNotoncus.3                0.000000 0.000000      NaN 0.000000 0.000000  1.000\nNotoncus.4                0.000000 0.000000      NaN 0.000000 0.000000  1.000\nPachychondyla.1           0.000000 0.000000      NaN 0.000000 0.000000  1.000\nParatrechina.6            0.000000 0.000000      NaN 0.000000 0.000000  1.000\nPheidole.2                0.000000 0.000000      NaN 0.000000 0.000000  1.000\nPlagiolepis.1             0.000000 0.000000      NaN 0.000000 0.000000  1.000\nPolyrachis.1              0.000000 0.000000      NaN 0.000000 0.000000  1.000\nPolyrachis.5              0.000000 0.000000      NaN 0.000000 0.000000  1.000\nProbolomyrmex.1           0.000000 0.000000      NaN 0.000000 0.000000  1.000\nProlasius.2               0.000000 0.000000      NaN 0.000000 0.000000  1.000\nRhopalomastix.2           0.000000 0.000000      NaN 0.000000 0.000000  1.000\nRhopalomastix.3           0.000000 0.000000      NaN 0.000000 0.000000  1.000\nSolenopsis.1              0.000000 0.000000      NaN 0.000000 0.000000  1.000\nStrumigenys.1             0.000000 0.000000      NaN 0.000000 0.000000  1.000\nStrumigenys.2             0.000000 0.000000      NaN 0.000000 0.000000  1.000\nTechnomyrmex.1            0.000000 0.000000      NaN 0.000000 0.000000  1.000\nTetramorium.5             0.000000 0.000000      NaN 0.000000 0.000000  1.000\n                              p   \nTapinoma.1                0.578   \nPheidole.5                0.971   \nMeranoplus.1              0.278   \nIridomyrmex.purpureus     0.039 * \nCrematogaster.1           0.020 * \nParatrechina.1            0.276   \nMachomyrma.1              0.148   \nNotoncus.1                0.796   \nAnonychromyrma.1          0.911   \nPheidole.7                0.939   \nParatrechina.2            0.513   \nMesostruma.1              0.008 **\nCamponotus.consobrinus    0.067 . \nOchetellus.1              0.041 * \nPheidole.6                0.092 . \nPapyrius.2                0.069 . \nProlasius.6               0.026 * \nRhytidiponera..metallica. 0.997   \nIridomyrmex.5             0.003 **\nDoleromyrma.1             0.211   \nCamponotus.12             0.103   \nPristomyrmex.2            0.525   \nMayriella.2               0.177   \nIridomyrmex.2             0.357   \nTetramorium.3             0.971   \nMonomorium.1              0.968   \nStigmacros.3              0.059 . \nPheidole.7.1              0.585   \nMachomyrma.4              0.063 . \nPolyrachis.3              0.146   \nIridomyrmex.4             0.382   \nPolyrachis.8              0.130   \nParatrechina.4            0.940   \nPachycondyla.2            0.491   \nDoleromyrma.3             0.921   \nCrematogaster.2           0.936   \nPolyrachis.2              0.469   \nStigmacros.2              0.169   \nProlasius.7               0.202   \nAnonychromyrma.2          0.207   \nPolyrachis.7              0.192   \nStigmacros.1              0.537   \nPheidole.1                0.199   \nPristomyrmex.1            0.675   \nMyrmecia.2                0.284   \nCamponotis.26             0.158   \nCamponotus.1              0.158   \nEpopostruma.1             0.158   \nMayriella.1               0.432   \nOligomyrmex.1             0.306   \nAnonychromyrma.3          0.164   \nMyrmecorhrynchus.1        0.164   \nParatrechina.5            0.164   \nProlasius.3               0.770   \nOligomyrmex.2             0.354   \nProlasius.4               0.173   \nStigmacros.4              0.556   \nIridomyrmex.6             0.259   \nMeranoplus.2              0.276   \nMyrmecia.3                0.874   \nPapyrius.1                0.640   \nHeteroponera.1            0.973   \nRhopalomastix.1           0.823   \nMelophorus.2              0.589   \nPheidole.6.1              0.271   \nProlasius.1               0.835   \nIridomyrmex.8             0.287   \nMyrmecia.1                0.719   \nMyrmecia.4                0.287   \nRhopalomastix.5           0.287   \nTetramorium.4             0.898   \nColobostruma.1               NA   \nColobostruma.2               NA   \nCrematogaster.3              NA   \nDisturbed.lost               NA   \nDoleromyrma.2                NA   \nDolichonderus.1              NA   \nFroggattella.1               NA   \nIridomyrmex.7                NA   \nLeptomyrmex.1                NA   \nMachomyrma.3                 NA   \nMachomyrma.6                 NA   \nMelophorus.1                 NA   \nNotoncus.3                   NA   \nNotoncus.4                   NA   \nPachychondyla.1              NA   \nParatrechina.6               NA   \nPheidole.2                   NA   \nPlagiolepis.1                NA   \nPolyrachis.1                 NA   \nPolyrachis.5                 NA   \nProbolomyrmex.1              NA   \nProlasius.2                  NA   \nRhopalomastix.2              NA   \nRhopalomastix.3              NA   \nSolenopsis.1                 NA   \nStrumigenys.1                NA   \nStrumigenys.2                NA   \nTechnomyrmex.1               NA   \nTetramorium.5                NA   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nContrast: SRW_SSTF \n\n                          average      sd   ratio     ava     avb cumsum     p\nRhytidiponera..metallica. 0.04104 0.03832 1.07100 0.79590 1.22640  0.055 0.042\nPheidole.5                0.03769 0.03488 1.08070 0.75730 1.08140  0.105 0.535\nAnonychromyrma.1          0.03414 0.03631 0.94040 0.35530 0.49420  0.151 0.135\nCrematogaster.1           0.03197 0.03498 0.91390 0.06250 0.56960  0.194 0.002\nNotoncus.1                0.03025 0.03397 0.89040 0.48480 0.24530  0.234 0.203\nCrematogaster.2           0.02819 0.03616 0.77980 0.43640 0.07690  0.272 0.001\nParatrechina.2            0.02653 0.03156 0.84050 0.33230 0.32220  0.307 0.030\nTetramorium.3             0.02548 0.03594 0.70910 0.29710 0.25510  0.341 0.361\nTapinoma.1                0.02482 0.02975 0.83420 0.18750 0.38460  0.375 0.976\nPheidole.7                0.02373 0.03238 0.73280 0.26180 0.24530  0.406 0.803\nDoleromyrma.3             0.02273 0.04161 0.54640 0.33670 0.00000  0.437 0.045\nMachomyrma.1              0.02239 0.02844 0.78740 0.06250 0.41370  0.467 0.300\nParatrechina.1            0.02126 0.02826 0.75220 0.34410 0.15380  0.495 0.759\nMonomorium.1              0.02010 0.03262 0.61610 0.27240 0.20030  0.522 0.710\nMeranoplus.1              0.01999 0.03189 0.62690 0.24430 0.15380  0.549 0.936\nPheidole.6                0.01891 0.02798 0.67590 0.06250 0.32220  0.574 0.048\nPristomyrmex.2            0.01695 0.02842 0.59640 0.21340 0.15380  0.597 0.314\nDoleromyrma.1             0.01634 0.02805 0.58260 0.06250 0.23080  0.619 0.191\nIridomyrmex.purpureus     0.01478 0.03071 0.48120 0.06250 0.20030  0.638 0.799\nCamponotus.consobrinus    0.01432 0.02443 0.58630 0.06250 0.24530  0.657 0.316\nOchetellus.1              0.01295 0.02735 0.47330 0.06250 0.16840  0.675 0.391\nPapyrius.2                0.01185 0.02480 0.47770 0.06250 0.15380  0.691 0.351\nParatrechina.4            0.01184 0.02555 0.46330 0.12500 0.09150  0.707 0.573\nMachomyrma.4              0.01147 0.02801 0.40940 0.00000 0.15380  0.722 0.002\nStigmacros.1              0.01049 0.02273 0.46140 0.12500 0.07690  0.736 0.022\nProlasius.3               0.01023 0.02214 0.46200 0.13680 0.07690  0.750 0.094\nIridomyrmex.2             0.00886 0.02459 0.36050 0.12500 0.00000  0.761 0.678\nCamponotus.12             0.00855 0.02058 0.41560 0.00000 0.15380  0.773 0.658\nProlasius.6               0.00839 0.02076 0.40400 0.00000 0.16840  0.784 0.532\nTechnomyrmex.1            0.00803 0.02179 0.36870 0.12500 0.00000  0.795 0.104\nMyrmecia.3                0.00801 0.02194 0.36520 0.12500 0.00000  0.805 0.378\nPachycondyla.2            0.00789 0.02131 0.37050 0.13680 0.00000  0.816 0.220\nHeteroponera.1            0.00707 0.01918 0.36850 0.12500 0.00000  0.826 0.769\nDolichonderus.1           0.00704 0.01901 0.37050 0.12500 0.00000  0.835 0.117\nMachomyrma.3              0.00691 0.01861 0.37110 0.12500 0.00000  0.844 0.530\nMayriella.2               0.00680 0.01792 0.37930 0.06250 0.07690  0.853 0.754\nIridomyrmex.4             0.00676 0.01782 0.37960 0.06250 0.07690  0.862 0.447\nOligomyrmex.2             0.00618 0.01621 0.38110 0.06250 0.07690  0.871 0.032\nMesostruma.1              0.00606 0.02144 0.28260 0.00000 0.09150  0.879 0.800\nProlasius.7               0.00548 0.01943 0.28180 0.00000 0.07690  0.886 0.004\nStigmacros.2              0.00528 0.01860 0.28390 0.00000 0.09150  0.893 0.003\nAnonychromyrma.2          0.00510 0.01803 0.28260 0.00000 0.07690  0.900 0.003\nPolyrachis.7              0.00478 0.01687 0.28330 0.00000 0.07690  0.906 0.003\nPheidole.1                0.00474 0.01674 0.28340 0.00000 0.07690  0.913 0.003\nPristomyrmex.1            0.00444 0.01564 0.28390 0.00000 0.07690  0.918 0.565\nCamponotis.26             0.00398 0.01399 0.28470 0.00000 0.07690  0.924 0.001\nCamponotus.1              0.00398 0.01399 0.28470 0.00000 0.07690  0.929 0.001\nEpopostruma.1             0.00398 0.01399 0.28470 0.00000 0.07690  0.934 0.001\nPolyrachis.3              0.00398 0.01399 0.28470 0.00000 0.07690  0.940 0.511\nAnonychromyrma.3          0.00377 0.01324 0.28510 0.00000 0.07690  0.945 0.004\nMyrmecorhrynchus.1        0.00377 0.01324 0.28510 0.00000 0.07690  0.950 0.004\nParatrechina.5            0.00377 0.01324 0.28510 0.00000 0.07690  0.955 0.004\nProlasius.4               0.00377 0.01323 0.28510 0.00000 0.07690  0.960 0.002\nProlasius.1               0.00367 0.01449 0.25330 0.06250 0.00000  0.965 0.683\nStrumigenys.1             0.00344 0.01357 0.25380 0.06250 0.00000  0.970 0.507\nSolenopsis.1              0.00342 0.01348 0.25380 0.06250 0.00000  0.974 0.762\nPachychondyla.1           0.00337 0.01328 0.25400 0.06250 0.00000  0.979 0.253\nRhopalomastix.1           0.00334 0.01313 0.25400 0.06250 0.00000  0.983 0.771\nPheidole.7.1              0.00318 0.01252 0.25430 0.06250 0.00000  0.987 0.952\nProlasius.2               0.00318 0.01252 0.25430 0.06250 0.00000  0.992 0.549\nStigmacros.4              0.00318 0.01252 0.25430 0.06250 0.00000  0.996 0.430\nStigmacros.3              0.00311 0.01086 0.28600 0.00000 0.07690  1.000 0.727\nColobostruma.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nColobostruma.2            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nCrematogaster.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDisturbed.lost            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nDoleromyrma.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nFroggattella.1            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.6             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.7             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nIridomyrmex.8             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nLeptomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMachomyrma.6              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMayriella.1               0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMelophorus.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMelophorus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMeranoplus.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nMyrmecia.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.3                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nNotoncus.4                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nOligomyrmex.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPapyrius.1                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nParatrechina.6            0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.2                0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPheidole.6.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPlagiolepis.1             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.1              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.2              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.5              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nPolyrachis.8              0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nProbolomyrmex.1           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.2           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.3           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nRhopalomastix.5           0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nStrumigenys.2             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.4             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\nTetramorium.5             0.00000 0.00000     NaN 0.00000 0.00000  1.000    NA\n                             \nRhytidiponera..metallica. *  \nPheidole.5                   \nAnonychromyrma.1             \nCrematogaster.1           ** \nNotoncus.1                   \nCrematogaster.2           ***\nParatrechina.2            *  \nTetramorium.3                \nTapinoma.1                   \nPheidole.7                   \nDoleromyrma.3             *  \nMachomyrma.1                 \nParatrechina.1               \nMonomorium.1                 \nMeranoplus.1                 \nPheidole.6                *  \nPristomyrmex.2               \nDoleromyrma.1                \nIridomyrmex.purpureus        \nCamponotus.consobrinus       \nOchetellus.1                 \nPapyrius.2                   \nParatrechina.4               \nMachomyrma.4              ** \nStigmacros.1              *  \nProlasius.3               .  \nIridomyrmex.2                \nCamponotus.12                \nProlasius.6                  \nTechnomyrmex.1               \nMyrmecia.3                   \nPachycondyla.2               \nHeteroponera.1               \nDolichonderus.1              \nMachomyrma.3                 \nMayriella.2                  \nIridomyrmex.4                \nOligomyrmex.2             *  \nMesostruma.1                 \nProlasius.7               ** \nStigmacros.2              ** \nAnonychromyrma.2          ** \nPolyrachis.7              ** \nPheidole.1                ** \nPristomyrmex.1               \nCamponotis.26             ***\nCamponotus.1              ***\nEpopostruma.1             ***\nPolyrachis.3                 \nAnonychromyrma.3          ** \nMyrmecorhrynchus.1        ** \nParatrechina.5            ** \nProlasius.4               ** \nProlasius.1                  \nStrumigenys.1                \nSolenopsis.1                 \nPachychondyla.1              \nRhopalomastix.1              \nPheidole.7.1                 \nProlasius.2                  \nStigmacros.4                 \nStigmacros.3                 \nColobostruma.1               \nColobostruma.2               \nCrematogaster.3              \nDisturbed.lost               \nDoleromyrma.2                \nFroggattella.1               \nIridomyrmex.5                \nIridomyrmex.6                \nIridomyrmex.7                \nIridomyrmex.8                \nLeptomyrmex.1                \nMachomyrma.6                 \nMayriella.1                  \nMelophorus.1                 \nMelophorus.2                 \nMeranoplus.2                 \nMyrmecia.1                   \nMyrmecia.2                   \nMyrmecia.4                   \nNotoncus.3                   \nNotoncus.4                   \nOligomyrmex.1                \nPapyrius.1                   \nParatrechina.6               \nPheidole.2                   \nPheidole.6.1                 \nPlagiolepis.1                \nPolyrachis.1                 \nPolyrachis.2                 \nPolyrachis.5                 \nPolyrachis.8                 \nProbolomyrmex.1              \nRhopalomastix.2              \nRhopalomastix.3              \nRhopalomastix.5              \nStrumigenys.2                \nTetramorium.4                \nTetramorium.5                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 999\n\n\nExercise: Which species of ants are the most important for the separation of communities?"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#assumed-knowledge",
    "href": "lectures/L01/Lecture-01b.html#assumed-knowledge",
    "title": "Lecture 01b – Revision",
    "section": "Assumed knowledge",
    "text": "Assumed knowledge\n\nSamples, populations and statistical inference\nProbability distributions\nParameter estimation\n\ncentral tendency\nspread or variability\n\nSampling distribution of the mean\n\nStandard error\nConfidence intervals\nCentral Limit Theorem"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#why-is-this-important",
    "href": "lectures/L01/Lecture-01b.html#why-is-this-important",
    "title": "Lecture 01b – Revision",
    "section": "Why is this important?",
    "text": "Why is this important?\n\nUnderstanding sampling informs experimental design (Week 4 onward). How many samples do we need and are our samples representative?\nRecognising sample (not sampling) distributions helps us choose the right statistical model – e.g. t-test to compare two means that are normally distributed.\nMost statistical techniques use sample statistics for interpretation, e.g. the t-test can be explained using confidence intervals, and the ANOVA test can be interpreted in part using means and standard errors.\n\nAll of these concepts will make more sense as we go through the course, but if you do not try to understand them now, you will struggle."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#populations-and-samples",
    "href": "lectures/L01/Lecture-01b.html#populations-and-samples",
    "title": "Lecture 01b – Revision",
    "section": "Populations and samples",
    "text": "Populations and samples\nPopulations\n\nAll the possible units and their associated observations of interest\nScientists are often interested in making inferences about populations, but measuring every unit is impractical\n\nSamples\n\nA collection of observations from any population is a sample, and the number of observations in it is the sample size\nWe assume samples that we collect can be used to make inferences about the population\nNEW: Samples need to be representative of the population"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#statistics-vs-parameters",
    "href": "lectures/L01/Lecture-01b.html#statistics-vs-parameters",
    "title": "Lecture 01b – Revision",
    "section": "Statistics vs parameters",
    "text": "Statistics vs parameters\n\nCharacteristics of the population are called parameters (e.g. population mean or population regression slope)\nCharacteristics of the sample are called statistics (e.g. sample mean or sample regression slope) – they are used to estimate the population parameters\nStatistics are what we use to help us understand the population\nFormal statistical methods can help us make inferences about the population based on the sample – statistical inference\nNot all statistical techniques are inferential, but many are"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sample-data",
    "href": "lectures/L01/Lecture-01b.html#sample-data",
    "title": "Lecture 01b – Revision",
    "section": "Sample data",
    "text": "Sample data\nSample data are usually collected as variables, which are the characteristics we measure or record from each object.\n\nVariables can be:\nCategorical Variables\n\nNominal: categories without a natural order (e.g. colors, names)\nOrdinal: categories with a natural order (e.g. ratings, rankings)\n\nNumerical Variables\n\nContinuous: can take any value within a range (e.g. height, weight)\nDiscrete: can take only specific values (e.g. counts, presence/absence)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#you-decide-on-what-a-variable-represents",
    "href": "lectures/L01/Lecture-01b.html#you-decide-on-what-a-variable-represents",
    "title": "Lecture 01b – Revision",
    "section": "YOU decide on what a variable represents",
    "text": "YOU decide on what a variable represents\nA numerical, continuous variable can be treated as a categorical variable if you decide to categorise it.\nExamples\n\nheight (in cm) – a numerical, continuous variable, can be treated as a categorical variable if you group it into categories (short, medium, tall)\nage (in years) – a numerical, discrete variable, can be treated as a continuous variable (if we allow for certain issues)\ntreatment (A, B, C) – a categorical variable, can be treated as a numerical variable if we assign numbers to the treatments (1, 2, 3) and assume they are ordered e.g. effect of 1 &lt; 2 &lt; 3 – the basis of non-parametric tests"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#types-of-probability-distributions",
    "href": "lectures/L01/Lecture-01b.html#types-of-probability-distributions",
    "title": "Lecture 01b – Revision",
    "section": "Types of probability distributions",
    "text": "Types of probability distributions\nPopulations can be described by probability distributions, and by now, you should be familiar with these distributions and their properties\n\nNormal Distribution: Bell-shaped curve, symmetric around the mean. Data is continuous\nBinomial Distribution: Models success/failure outcomes in a fixed number of trials. Data is discrete\nPoisson Distribution: Models count data when events occur at a constant rate. Data is discrete\n\n\nKnowing the distribution of your data is important for choosing the right statistical model – although it is not always necessary."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#section",
    "href": "lectures/L01/Lecture-01b.html#section",
    "title": "Lecture 01b – Revision",
    "section": "",
    "text": "library(tidyverse)\n\nset.seed(908)\nnormal_data &lt;- data.frame(x = rnorm(10000, mean = 0, sd = 1))\nbinomial_data &lt;- data.frame(x = rbinom(10000, size = 10, prob = 0.5))\npoisson_data &lt;- data.frame(x = rpois(500, lambda = 3))\n\n# normal\np1 &lt;- ggplot(normal_data, aes(x = x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  ggtitle(\"Normal Distribution\")\n\n# binomial\np2 &lt;- ggplot(binomial_data, aes(x = x)) +\n  geom_bar(fill = \"lightgreen\", color = \"black\") +\n  ggtitle(\"Binomial Distribution\")\n\n# poisson\np3 &lt;- ggplot(poisson_data, aes(x = x)) +\n  geom_bar(fill = \"lightpink\", color = \"black\") +\n  ggtitle(\"Poisson Distribution\")\n\n# Arrange plots\nlibrary(patchwork)\np1 | p2 / p3"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-central-tendency",
    "href": "lectures/L01/Lecture-01b.html#measures-of-central-tendency",
    "title": "Lecture 01b – Revision",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nMean \\(\\bar{x}\\)\n\nThe arithmetic average of all values in a dataset\nSum of all values divided by number of observations\nSensitive to extreme values (outliers)\n\n\n\n\n\n\n\n\nFormula\n\n\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\]\n\nwhere \\(n\\) is the number of observations\n\\(x_i\\) represents each individual value\n\\(\\sum\\) means we add up all values from \\(i=1\\) to \\(n\\)\nExample: for data {2,4,6,8}, \\(n=4\\) and \\(\\bar{x} = \\frac{2+4+6+8}{4} = 5\\)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#section-1",
    "href": "lectures/L01/Lecture-01b.html#section-1",
    "title": "Lecture 01b – Revision",
    "section": "",
    "text": "Median\n\nMiddle value when data is ordered\n50th percentile of the data\nMore robust to outliers than mean\nFor even n, average of two middle values\n\n\nMode\n\nMost frequently occurring value\nCan have multiple modes\nOnly measure of central tendency for categorical data\nNot always meaningful for continuous data"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#how-they-compare",
    "href": "lectures/L01/Lecture-01b.html#how-they-compare",
    "title": "Lecture 01b – Revision",
    "section": "How they compare",
    "text": "How they compare\n\nDepending on the distribution of the data, the mean and median can be different, and this can tell you something about the data."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nVariance \\(s^2\\)\n\nMeasures how spread out the data is from the mean\nCalculated as average squared deviations from the mean\nSquared units make it harder to interpret\nSensitive to outliers (squares large deviations)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion-1",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion-1",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\n\n\n\n\n\n\nFormula\n\n\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\]\n\nwhere \\(n\\) is the number of observations\n\\(x_i\\) represents each individual value\n\\(\\bar{x}\\) is the sample mean\nFor data {2,4,6,8}:\n\nmean = 5\ndifferences = (-3,-1,1,3), squares = (9,1,1,9), sum = 20\nTherefore, \\(s^2 = \\frac{20}{3} \\approx 6.67\\)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#measures-of-dispersion-2",
    "href": "lectures/L01/Lecture-01b.html#measures-of-dispersion-2",
    "title": "Lecture 01b – Revision",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nStandard Deviation \\(s\\)\n\nSquare root of variance\nSame units as original data\nMore interpretable than variance\nEmpirical rule for normal distributions:\n\n≈68% of data within ±1 SD\n≈95% of data within ±2 SD\n≈99.7% of data within ±3 SD\n\n\n\n\n\n\n\n\n\nFormula\n\n\n\\[s = \\sqrt{s^2}\\]\n\nSimply the square root of variance\nFor our example data {2,4,6,8}:\n\n\\(s = \\sqrt{6.67} \\approx 2.58\\)\n\nInterpretation: On average, values deviate about 2.58 units from the mean"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#visualising-standard-deviation",
    "href": "lectures/L01/Lecture-01b.html#visualising-standard-deviation",
    "title": "Lecture 01b – Revision",
    "section": "Visualising standard deviation",
    "text": "Visualising standard deviation"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#population-parameters-vs-sample-statistics",
    "href": "lectures/L01/Lecture-01b.html#population-parameters-vs-sample-statistics",
    "title": "Lecture 01b – Revision",
    "section": "Population parameters vs sample statistics",
    "text": "Population parameters vs sample statistics\nFor those of you interested:\nMean\n\n\n\n\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\(\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i\\)\n\\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\n\n\n\nVariance\n\n\n\n\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\(\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2\\)\n\\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\)\n\n\n\nStandard Deviation\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\\(s = \\sqrt{s^2}\\)\n\n\n\n\n\n\n\n\n\nNote\n\n\nNotice the use of \\(n-1\\) in sample variance and standard deviation"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#why-n-1",
    "href": "lectures/L01/Lecture-01b.html#why-n-1",
    "title": "Lecture 01b – Revision",
    "section": "Why n-1?",
    "text": "Why n-1?\n\nWhen calculating sample variance, we use \\(n-1\\) instead of \\(n\\) in the denominator\nThis is called “Bessel’s correction”\nWhy? Because we lose one “degree of freedom” when we estimate the mean:\n\nIf you know the sample mean (\\(\\bar{x}\\))\nAnd you know all but one value in your sample\nThe last value is constrained - it must make the mean equal \\(\\bar{x}\\)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#what-is-a-sampling-distribution",
    "href": "lectures/L01/Lecture-01b.html#what-is-a-sampling-distribution",
    "title": "Lecture 01b – Revision",
    "section": "What is a sampling distribution?",
    "text": "What is a sampling distribution?\n\nDistribution of a statistic (e.g., mean) calculated from repeated samples\nShows how sample statistics vary from sample to sample\nImportant for understanding sampling variability and making inferences"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sampling-distribution-of-the-mean",
    "href": "lectures/L01/Lecture-01b.html#sampling-distribution-of-the-mean",
    "title": "Lecture 01b – Revision",
    "section": "Sampling distribution of the mean",
    "text": "Sampling distribution of the mean"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#central-limit-theorem",
    "href": "lectures/L01/Lecture-01b.html#central-limit-theorem",
    "title": "Lecture 01b – Revision",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nI know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the Central Limit Theorem. The law would have been personified by the Greeks and deified, if they had known of it.”\n\n– Sir Francis Galton, 1889, Natural Inheritance\nThe Central Limit Theorem (CLT) states that for sufficiently large samples:\n\nThe sampling distribution of the mean follows a normal distribution\nThe mean of the sampling distribution equals the population mean\nThe standard deviation of the sampling distribution (standard error) = \\(\\frac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#clt-in-action",
    "href": "lectures/L01/Lecture-01b.html#clt-in-action",
    "title": "Lecture 01b – Revision",
    "section": "CLT in action",
    "text": "CLT in action\n\n# Create a skewed population\nset.seed(456)\nskewed_pop &lt;- exp(rnorm(10000, mean = 0, sd = 0.5))\n\n# Sample means for different sample sizes (ordered small to large)\nsample_sizes &lt;- c(5, 30, 100)\nsample_labels &lt;- factor(paste(\"n =\", sample_sizes),\n  levels = paste(\"n =\", sample_sizes)\n) # preserve order\nsample_dist_data &lt;- lapply(sample_sizes, function(n) {\n  means &lt;- replicate(1000, mean(sample(skewed_pop, size = n)))\n  data.frame(means = means, size = factor(paste(\"n =\", n), levels = levels(sample_labels)))\n})\nsample_dist_df &lt;- do.call(rbind, sample_dist_data)\n\n# Plot\nggplot() +\n  geom_histogram(aes(x = means, y = ..density..),\n    data = sample_dist_df,\n    bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  geom_density(aes(x = means), data = sample_dist_df, color = \"blue\") +\n  facet_wrap(~size, scales = \"free_x\") +\n  ggtitle(\"Sampling distributions for different sample sizes\")"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#example",
    "href": "lectures/L01/Lecture-01b.html#example",
    "title": "Lecture 01b – Revision",
    "section": "Example",
    "text": "Example\n\nset.seed(239)\n# Generate a skewed distribution\nskewed &lt;- tibble(\n  x = rgamma(1000, shape = 2, scale = 1)\n)\n\n# plot in ggplot2\nggplot(data = skewed, aes(x = x)) +\n  geom_histogram(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlab(\"Height (m)\")\n\n\n\nSkewed population distribution for tree heights.\nWe want to estimate the mean height of the trees in the forest."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#sample-no-summary-statistic",
    "href": "lectures/L01/Lecture-01b.html#sample-no-summary-statistic",
    "title": "Lecture 01b – Revision",
    "section": "1 sample (no summary statistic)",
    "text": "1 sample (no summary statistic)\n\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 1,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\nWith only one sample, we are not really seeing a sampling distribution – we are just replicating the same population distribution. A sampling distribution emerges when we take multiple samples and calculate their means."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-1",
    "href": "lectures/L01/Lecture-01b.html#samples-1",
    "title": "Lecture 01b – Revision",
    "section": "2 samples",
    "text": "2 samples\n\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 2,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\nWe sample 2 trees and calculate the mean height, and repeat this 1000 times.\nThe distribution of sample means is starting to look more like a normal distribution."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-2",
    "href": "lectures/L01/Lecture-01b.html#samples-2",
    "title": "Lecture 01b – Revision",
    "section": "5 samples",
    "text": "5 samples\n\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 5,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\nFive random samples per calculated mean, repeated 1000 times.\nThe distribution is becoming more normal, and the spread is decreasing: estimate is getting more precise."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-3",
    "href": "lectures/L01/Lecture-01b.html#samples-3",
    "title": "Lecture 01b – Revision",
    "section": "30 samples",
    "text": "30 samples\n\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 30,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\nThirty random samples per calculated mean, repeated 1000 times.\nThe distribution of sample means is very close to a normal distribution."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#samples-4",
    "href": "lectures/L01/Lecture-01b.html#samples-4",
    "title": "Lecture 01b – Revision",
    "section": "50 samples",
    "text": "50 samples\n\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  xlim(0, 10) +\n  xlab(\"Mean height (m)\")\n\n\n\nFifty random samples per calculated mean, repeated 1000 times.\nHow many samples is enough?"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#are-50-samples-normal-enough",
    "href": "lectures/L01/Lecture-01b.html#are-50-samples-normal-enough",
    "title": "Lecture 01b – Revision",
    "section": "Are 50 samples “normal” enough?",
    "text": "Are 50 samples “normal” enough?\n\nskewed |&gt;\n  infer::rep_sample_n(\n    size = 50,\n    reps = 1000\n  ) |&gt;\n  group_by(replicate) |&gt;\n  summarise(xbar = mean(x)) |&gt;\n  ggplot(aes(x = xbar)) +\n  geom_density(\n    fill = \"orangered\",\n    alpha = 0.5, bins = 50\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = 2, # population mean for gamma(2,1)\n      sd = sqrt(2) / sqrt(50) # theoretical SE for gamma(2,1)\n    ),\n    linewidth = 1,\n    color = \"blue\",\n    linetype = \"dashed\"\n  ) +\n  xlab(\"Mean height (m)\")\n\n\n\nFifty random samples per calculated mean, repeated 1000 times.\nHow many samples is enough?"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#effect-of-sample-size",
    "href": "lectures/L01/Lecture-01b.html#effect-of-sample-size",
    "title": "Lecture 01b – Revision",
    "section": "Effect of sample size",
    "text": "Effect of sample size\n\n\nCode\nlibrary(tidymodels)\nlibrary(patchwork)\nset.seed(642)\n\nheights &lt;- tibble(heights = rnorm(1000, 1.99, 1))\npopmean &lt;- mean(heights$heights)\nsample_sizes &lt;- c(2, 5, 25, 100)\nn &lt;- length(sample_sizes)\n\nheights &lt;- tibble(heights = rgamma(1000, shape = 2, scale = 1))\nsample_sizes &lt;- c(2, 5, 25, 100)\nn &lt;- length(sample_sizes)\n\nplots &lt;- lapply(sample_sizes, function(size) {\n  df &lt;- heights |&gt;\n    rep_sample_n(size = size, reps = 2000) |&gt;\n    group_by(replicate) |&gt;\n    summarise(xbar = mean(heights))\n\n  mean_xbar &lt;- mean(df$xbar)\n\n  ggplot(df, aes(x = xbar)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    geom_vline(aes(xintercept = mean_xbar), color = \"blue\", linetype = \"dashed\") +\n    geom_text(aes(x = mean_xbar, label = sprintf(\"%.2f\", mean_xbar), y = Inf), hjust = -0.1, vjust = 2, color = \"blue\") +\n    ggtitle(paste0(\"Sample Size: \", size)) +\n    xlab(\"Mean height (m)\") +\n    xlim(-3, 8)\n})\nwrap_plots(plots)\n\n\n\nIncreased sample size leads to a more accurate estimate of the population mean, reflected by the narrower distribution of the sample mean, which is captured by the standard error."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#effect-of-variability",
    "href": "lectures/L01/Lecture-01b.html#effect-of-variability",
    "title": "Lecture 01b – Revision",
    "section": "Effect of variability",
    "text": "Effect of variability\n\n\nCode\nset.seed(1221)\n\n# Define a function to generate ggplot objects\ngenerate_plot &lt;- function(sd) {\n  data &lt;- rnorm(500, 1.99, sd)\n  p &lt;- ggplot(data = tibble(x = data), aes(x = x)) +\n    geom_histogram(fill = \"orangered\", alpha = 0.5, bins = 50) +\n    ggtitle(paste(\"SD =\", sd)) +\n    xlim(-100, 100)\n  return(p)\n}\n\n# Apply the function to a list of standard deviations\nsds &lt;- c(3, 6, 15, 25)\nplots &lt;- lapply(sds, generate_plot)\n\n# Wrap the plots\nwrap_plots(plots)\n\n\n\nIncreased variability (i.e. wide range of tree heights) leads to a wider distribution of the sample mean (i.e. less precision), which is also reflected by the standard error."
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#clt-drives-statistical-inference",
    "href": "lectures/L01/Lecture-01b.html#clt-drives-statistical-inference",
    "title": "Lecture 01b – Revision",
    "section": "CLT drives statistical inference",
    "text": "CLT drives statistical inference\nBecause of how predictable the CLT applies to sample means, we can use this to make reasonably accurate inferences about the population mean, even if we do not know the population distribution.\n\nA sampling distribution of the mean will be normally distributed for sufficiently large samples – how large is “sufficient” depends on the population distribution\nThe mean of the sampling distribution trends towards the population mean with increasing sample size\nTo determine how well the sample mean estimates the population mean, we use the standard error of the mean – basically a standard deviation of the sampling distribution"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#standard-error-of-the-mean",
    "href": "lectures/L01/Lecture-01b.html#standard-error-of-the-mean",
    "title": "Lecture 01b – Revision",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean\n\nMeasures the precision of a sample mean\nDescribes variation in sample means – around the true population mean\nDecreases as sample size increases, because we become more “confident” in our estimate\n\n\n\n\n\n\n\n\nFormula\n\n\n\\[SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\]\n\nwhere \\(s\\) is the sample standard deviation\n\\(n\\) is the sample size"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#when-to-report-sd-or-se",
    "href": "lectures/L01/Lecture-01b.html#when-to-report-sd-or-se",
    "title": "Lecture 01b – Revision",
    "section": "When to report SD or SE",
    "text": "When to report SD or SE\nStandard Deviation (SD)\n\nDescribes variability in your data\nStays constant regardless of sample size\n\nStandard Error (SE)\n\nDescribes precision of your mean estimate\nDecreases with larger sample size (\\(SE = \\frac{SD}{\\sqrt{n}}\\))\n\nWhen reporting statistics:\n\nUse mean ± SE to show precision of your estimate\nUse mean ± SD to show spread of your raw data\nSE can appear deceptively small with large sample sizes – always report sample size!"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#confidence-intervals",
    "href": "lectures/L01/Lecture-01b.html#confidence-intervals",
    "title": "Lecture 01b – Revision",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWhat is a confidence interval?\n\nRange of values likely to contain the true population parameter\nLevel of confidence (usually 95%) indicates reliability\nWider intervals = less precise estimates\n\n\n\n\n\n\n\n\nFormula for 95% CI\n\n\n\\[\\bar{x} \\pm (t_{n-1} \\times SE_{\\bar{x}})\\]"
  },
  {
    "objectID": "lectures/L01/Lecture-01b.html#visualising-confidence-intervals",
    "href": "lectures/L01/Lecture-01b.html#visualising-confidence-intervals",
    "title": "Lecture 01b – Revision",
    "section": "Visualising confidence intervals",
    "text": "Visualising confidence intervals\n\n# Generate sample data\nset.seed(253)\nsample_data &lt;- data.frame(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 30),\n  value = c(\n    rnorm(30, 100, 15),\n    rnorm(30, 110, 15),\n    rnorm(30, 105, 15)\n  )\n)\n\n# Calculate means and CIs\nci_data &lt;- sample_data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    mean = mean(value),\n    se = sd(value) / sqrt(n()),\n    ci_lower = mean - qt(0.975, n() - 1) * se,\n    ci_upper = mean + qt(0.975, n() - 1) * se\n  )\n\n# Plot\nggplot(ci_data, aes(x = group, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +\n  ggtitle(\"Means with 95% Confidence Intervals\")\n\n\nWe will learn more about confidence intervals in the next lecture."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#key-concepts-we-covered",
    "href": "lectures/L02/Lecture-02a.html#key-concepts-we-covered",
    "title": "Lecture 02a – Sampling designs",
    "section": "Key concepts we covered",
    "text": "Key concepts we covered\n\nPopulation vs. sample\n\nPopulation: The complete set of all items we’re studying\nSample: A subset of the population we actually measure\n\nParameters (population) and statistics (sample)\n\nCentral tendency: how data clusters around a middle value\n\nmean (average), median (middle value), mode (most common)\n\nSpread/dispersion: how data points vary from each other\n\nvariance (average squared deviation), standard deviation (square root of variance)\n\n\nConfidence intervals – we’ll explore this further today!"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#overview",
    "href": "lectures/L02/Lecture-02a.html#overview",
    "title": "Lecture 02a – Sampling designs",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\nAspect\nObservational study\nControlled experiment\n\n\n\n\nControl\nNo control over the variables of interest: mensurative and absolute\nControl over the variables of interest: comparative and manipulative\n\n\nCausation\nCannot establish causation, but perhaps association\nCan establish causation\n\n\nFeasibility\nCan be done in many cases\nMay be destructive and thus cannot always be done\n\n\nExamples\nSurveys, monitoring studies, correlational studies, case-control studies, cohort studies\nClinical trials, A/B testing, laboratory experiments, field experiments\n\n\nStatistical Tests\nCorrelation, regression, chi-squared tests, t-tests, one-way ANOVA, time series analysis\nT-tests, one-way ANOVA, factorial ANOVA, regression\n\n\n\n\n\n\n\nWe will focus on the fundamentals behind observational studies this week.\n\n\n\n\n\n\nTip\n\n\nMensurative studies involve measuring without manipulating variables.\nAbsolute studies measure actual values rather than comparing between treatments."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#observational-studies-two-common-types",
    "href": "lectures/L02/Lecture-02a.html#observational-studies-two-common-types",
    "title": "Lecture 02a – Sampling designs",
    "section": "Observational studies: two common types",
    "text": "Observational studies: two common types\n\nSurveys\n\nEstimate a statistic (e.g. mean, variance), but\nno temporal change during estimate.\nE.g. measuring species richness in a forest.\nThink of it as a snapshot at one point in time.\n\n\n\nMonitoring studies\n\nEstimate a change in statistic (same as above), and\ntemporal change across observations, i.e. before and after.\nE.g. measuring species richness in a forest before and after a fire.\nThink of it as taking multiple snapshots over time to see changes."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#sampling-designs",
    "href": "lectures/L02/Lecture-02a.html#sampling-designs",
    "title": "Lecture 02a – Sampling designs",
    "section": "Sampling designs",
    "text": "Sampling designs\n\nSimple random sampling:\n\nEach unit has an equal chance of being selected.\nRandomly sample units from the entire population.\nLike putting all names in a hat and drawing some out randomly.\n\n\n\n\n\n\n\n\nStratified random sampling\n\nThe population is first divided into strata (groups with similar characteristics).\nRandomly sample units within each strata by simple random sampling.\nLike separating students by year level, then randomly selecting some from each year."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#what-is-random-sampling",
    "href": "lectures/L02/Lecture-02a.html#what-is-random-sampling",
    "title": "Lecture 02a – Sampling designs",
    "section": "What is “random” sampling?",
    "text": "What is “random” sampling?\n\nRandom selection of finite or infinite population units.\n\n\n\nWhat does random mean?\n\n\n\nWithin a population, all units have a &gt; 0 probability of being selected i.e. everything has a chance to be selected.\n\nThis chance is called the inclusion probability (\\(\\pi_i\\)):\n\n\\(\\pi_i\\) is equal within a population unit – i.e. all units have the same chance of being selected.\n\\(\\pi_i\\) not necessarily equal between different population units – i.e. units from different groups may have different chances of being selected - more on this later.\n\n\n\n\nHow do we perform random sampling in real life?\n\nRandom number generator (RNG) – e.g. R’s sample() function.\nRandom number table – e.g. Random number table by the National Institute of Standards and Technology (NIST).\nThink of it like rolling dice or drawing names from a hat, but using mathematics to ensure true randomness."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#we-know-that",
    "href": "lectures/L02/Lecture-02a.html#we-know-that",
    "title": "Lecture 02a – Sampling designs",
    "section": "We know that…",
    "text": "We know that…\nFrom the previous lecture\n\nSample mean is a good measure of central tendency (the “middle” of our data).\nSample variance is a good measure of dispersion (how spread out our data is).\nSample size affects the precision of the sample mean (more samples = more precise).\n\n\nCan we combine all of the above in a single statistic?\nYes! That’s where confidence intervals come in."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#combining-an-estimate-with-its-precision",
    "href": "lectures/L02/Lecture-02a.html#combining-an-estimate-with-its-precision",
    "title": "Lecture 02a – Sampling designs",
    "section": "Combining an estimate with its precision",
    "text": "Combining an estimate with its precision\nA confidence interval (CI) is:\n\nA range of values that likely contains the true population value\nLike saying “We’re 95% confident that out of 100 samples, 95 of them will be within this range”\nCrucial for hypothesis testing and estimation, the basis of statistical inference\n\nYou will often see CIs in scientific papers, reports, and news articles."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#calculating-confidence-intervals",
    "href": "lectures/L02/Lecture-02a.html#calculating-confidence-intervals",
    "title": "Lecture 02a – Sampling designs",
    "section": "Calculating confidence intervals",
    "text": "Calculating confidence intervals\nWhat we need\n\nEstimate of the population parameter, i.e. the sample mean (\\(\\bar{x}\\)) - our best guess of the true value\nCritical value (\\(t_{n-1}\\)) - a number that represents our chosen confidence level (like 95%)\nStandard error of the estimate, SE of the mean (\\(SE_{\\bar{x}}\\)) - tells us how precise our mean is\n\n\n\n\n\n\n\nNote\n\n\nThink of standard error as “how much our sample means would vary if we took many different samples and calculated the mean each time”\n\n\n\nAll these (mean, standard error and critical value) are combined to form the confidence interval."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#breakdown",
    "href": "lectures/L02/Lecture-02a.html#breakdown",
    "title": "Lecture 02a – Sampling designs",
    "section": "Breakdown",
    "text": "Breakdown\nIn general, a CI has the form: \\[\\text{estimate} \\pm \\text{margin of error}\\]\nwhere the margin of error is a function of the standard error of the estimate:\n\\[\\text{estimate} \\pm (\\text{critical value} \\times \\text{standard error (estimate)})\\]\nwhere the critical value is based on the sampling distribution of the estimate i.e. the \\(t\\)-distribution.\n\n\n\n\n\n\nTip\n\n\nThink of margin of error like the “plus or minus” value you often see in survey results (±3%)"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#formula-for-95-confidence-interval-ci",
    "href": "lectures/L02/Lecture-02a.html#formula-for-95-confidence-interval-ci",
    "title": "Lecture 02a – Sampling designs",
    "section": "Formula for 95% Confidence Interval (CI)",
    "text": "Formula for 95% Confidence Interval (CI)\n\\[\n\\bar{x} \\pm \\left(t_{n-1} \\times \\frac{s}{\\sqrt{n}}\\right)\n\\]\nStep-by-step calculation by hand\n\nCalculate the sample mean, \\(\\bar{x}\\) (add all values and divide by number of samples)\nCalculate the sample standard deviation, \\(s\\) (measure of spread in your data)\nDetermine the standard error of the mean, \\(SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\) (how precise your mean estimate is)\nLook up the t-value, \\(t_{n-1}\\), from the t-distribution table for the 95% confidence level and \\(n-1\\) degrees of freedom.\n\nThis is a specific number based on how many samples you have\n\nCompute the margin of error: \\[\\text{Margin of Error} = t_{n-1} \\times SE_{\\bar{x}}\\]\nFinally, the 95% CI is: \\[\\bar{x} \\pm (t_{n-1} \\times SE_{\\bar{x}})\\]\n\nYou need to be able to calculate this by hand/calculator."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions",
    "href": "lectures/L02/Lecture-02a.html#more-definitions",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nDegrees of freedom (df)\nThe number of values in a sample that are free to vary while still maintaining the same statistic.\n\\[\n\\text{df} = n - 1\n\\]\nwhere \\(n\\) is the number of samples.\n\nExample\nImagine you have 4 numbers with a mean of 5:\n\nYou can choose the first three numbers freely: 3, 10, and 7\nBut the fourth number MUST be 0 to make the mean = 5\n\n(3 + 10 + 7 + 0) ÷ 4 = 5\n\nSo only 3 numbers (n-1) can be freely chosen = 3 degrees of freedom"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions-1",
    "href": "lectures/L02/Lecture-02a.html#more-definitions-1",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nt-critical value\nA number that helps determine how wide to make your confidence interval.\n\nBased on your confidence level (e.g. 95%) and sample size\nLarger t-values = wider intervals = more confidence but less precision\nSmaller sample sizes = larger t-values (because we’re less certain)"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#more-definitions-2",
    "href": "lectures/L02/Lecture-02a.html#more-definitions-2",
    "title": "Lecture 02a – Sampling designs",
    "section": "More definitions",
    "text": "More definitions\nt-distribution\nA probability distribution that accounts for the uncertainty when estimating from small samples.\n\nSimilar to the normal “bell curve” distribution, but with heavier tails.\nWith few samples (small n), the t-distribution is wider, reflecting greater uncertainty.\nAs sample size increases, the t-distribution gets closer to the normal distribution.\n\n\nlibrary(ggplot2)\nlibrary(gganimate)\n\n# Variable to control animation speed (higher values = slower transitions)\nanim_speed &lt;- 1\n\n# Create a sequence of x values for the plot\nx &lt;- seq(-4, 4, length.out = 400)\n\n# Degrees of freedom: 1 through 5 then even numbers from 6 to 30\ndfs &lt;- c(1:5, seq(6, 30, by = 2))\n\n# Prepare data for t-distribution curves with a new \"df\" column\nt_curves &lt;- do.call(rbind, lapply(dfs, function(df) {\n    data.frame(\n        x = x,\n        density = dt(x, df),\n        df = df\n    )\n}))\n\n# Create data for the standard normal distribution (static)\nnormal_curve &lt;- data.frame(\n    x = x,\n    density = dnorm(x)\n)\n\n# Plot with gganimate: animate t-distribution curves (each frame shows one df)\np &lt;- ggplot() +\n    # Static normal distribution curve\n    geom_line(\n        data = normal_curve, aes(x = x, y = density),\n        color = \"black\", linetype = \"dashed\", size = 1\n    ) +\n    # t-distribution curve that will animate\n    geom_line(\n        data = t_curves, aes(x = x, y = density, color = factor(df)),\n        size = 1\n    ) +\n    labs(\n        title = \"Degrees of Freedom: {closest_state}\",\n        x = \"x\",\n        y = \"Density\",\n        subtitle = \"Dashed line = Normal distribution; Solid line = t-distribution\"\n    ) +\n    theme(legend.position = \"none\") +\n    transition_states(states = df, transition_length = anim_speed, state_length = anim_speed)\n\np\n\n\n\n\n\n\n\n\nNote\n\n\nNotice how the t-distribution (solid line) gets closer to the normal distribution (dashed line) as the degrees of freedom increase!"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#interpreting-confidence-intervals",
    "href": "lectures/L02/Lecture-02a.html#interpreting-confidence-intervals",
    "title": "Lecture 02a – Sampling designs",
    "section": "Interpreting confidence intervals",
    "text": "Interpreting confidence intervals\n\nConfidence intervals depend on a specified confidence level (e.g. 95%, 99%) with higher confidence levels producing wider intervals (i.e. more conservative).\nAnother way to think of it: a range of values that we are fairly sure contains the true value of the population parameter.\n\n\nFishing analogy\nA confidence interval is like a fishing net:\n\nA wider net (interval) is more likely to catch the fish (true value)\nA spear (single point estimate) is less likely to catch the fish\nThe net width represents our uncertainty about the true value\n\n\n\n\n\n\n\nImportant\n\n\nCommon misunderstanding: A 95% CI does NOT mean there’s a 95% chance the true value is inside the interval. It means if you took 100 different samples and made 100 different CIs, about 95 of them would contain the true value."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#soil-carbon",
    "href": "lectures/L02/Lecture-02a.html#soil-carbon",
    "title": "Lecture 02a – Sampling designs",
    "section": "Soil carbon",
    "text": "Soil carbon\n\nSoil carbon content was measured at 7 locations across the area. The amount at each location was 48, 56, 90, 78, 86, 71, 42 tonnes per hectare (t/ha).\n\nsoil &lt;- c(48, 56, 90, 78, 86, 71, 42)\nsoil\n\n[1] 48 56 90 78 86 71 42\n\n\nWhat is the mean soil carbon content and how confident are we in this estimate? How this is calculated depends on whether we used simple random sampling or stratified random sampling."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#mean-and-95-ci",
    "href": "lectures/L02/Lecture-02a.html#mean-and-95-ci",
    "title": "Lecture 02a – Sampling designs",
    "section": "Mean and 95% CI",
    "text": "Mean and 95% CI\nStep-by-step calculation\n\nMean: \\(\\bar{x} = \\frac{48+56+90+78+86+71+42}{7} \\approx 67.3\\) t/ha\nStandard deviation: \\(s \\approx 18.84\\) t/ha\n\nThis tells us how much individual measurements vary from the mean\n\nStandard error: \\(SE = \\frac{s}{\\sqrt{7}} \\approx 7.12\\) t/ha\n\nThis tells us how precise our estimate of the mean is\n\nt-value (95% CI, df = 6): \\(t_{0.975,6} \\approx 2.447\\)\n\nThis is the critical value for 95% confidence with 6 degrees of freedom\n\nMargin of error: \\(t_{0.975,6} \\times SE \\approx 17.43\\) t/ha\n\nThis is the “plus or minus” value for our interval\n\nWhich gives: \\((67.3 - 17.43, 67.3 + 17.43) = (49.87, 84.73)\\) t/ha\n\n\nAnd so we report the mean soil carbon content as 67.3 t/ha with a 95% CI of (49.87, 84.73) t/ha or 67.3 ± 17.43 t/ha.\nNote: Our confidence interval is quite wide relative to our mean (about ±26% of the mean value), suggesting moderate uncertainty in our estimate."
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#implementation-in-r",
    "href": "lectures/L02/Lecture-02a.html#implementation-in-r",
    "title": "Lecture 02a – Sampling designs",
    "section": "Implementation in R",
    "text": "Implementation in R\nManual calculation\n\n# Step 1: Calculate the sample mean of soil carbon content\nmean_soil &lt;- mean(soil)\n\n# Step 2: Calculate the sample standard deviation of soil carbon content\nsd_soil &lt;- sd(soil)\n\n# Step 3: Calculate the standard error of the mean (SE)\nse_soil &lt;- sd_soil / sqrt(length(soil))\n\n# Step 4: Calculate the t-critical value for a 95% confidence interval\nt_crit &lt;- qt(0.975, df = length(soil) - 1)\n\n# Step 5: Calculate the margin of error (t_crit * SE)\n# and then determine the lower and upper bounds of the confidence interval\nci &lt;- mean_soil + c(-1, 1) * (t_crit * se_soil)\n\n# Step 6: View the calculated 95% confidence interval\nci\n\n[1] 49.84627 84.72516\n\n\nThere are ways to calculate this in R quickly, but it is important to understand the manual calculation.\n\n\n\n\n\n\nUnderstanding the R code\n\n\n\nmean() calculates the average value\nsd() calculates the standard deviation\nqt(0.975, df=6) gives the t-critical value (we use 0.975 because we want 95% in the middle, with 2.5% in each tail)\nc(-1, 1) creates a vector to subtract and add the margin of error"
  },
  {
    "objectID": "lectures/L02/Lecture-02a.html#questions",
    "href": "lectures/L02/Lecture-02a.html#questions",
    "title": "Lecture 02a – Sampling designs",
    "section": "Questions",
    "text": "Questions\n\nHow precise is our estimate?\n\nOur margin of error is about 17.43 t/ha, which is roughly 26% of our mean value.\n\nHow big a change must there be to estimate a statistically significant change?\n\nChanges larger than about 17.43 t/ha would likely be statistically significant.\n\nCan we sample more efficiently?\n\nYes! This is where stratified sampling becomes valuable.\n\n\nTo answer these questions, we need to compare simple random sampling with a hypothetical stratified random sampling design (i.e. what if we had considered stratification before sampling?)"
  },
  {
    "objectID": "lectures/L02/index.html",
    "href": "lectures/L02/index.html",
    "title": "Lecture 02",
    "section": "",
    "text": "Lecture 02a – Sampling designs Full Screen | PDF\n\nLecture 02a – Sampling designs II Full Screen | PDF"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#the-problem-with-multiple-t-tests",
    "href": "lectures/L03/Lecture-03b.html#the-problem-with-multiple-t-tests",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "The problem with multiple \\(t\\)-tests",
    "text": "The problem with multiple \\(t\\)-tests\nSuppose we want to compare weight gain of chicks fed on 4 different diets.\n\nWith \\(t\\)-tests, we’d need \\(\\binom{4}{2} = 6\\) pairwise comparisons:\n1-2, 1-3, 1-4, 2-3, 2-4, 3-4\n\n\nEven if there are no true differences, each test has a 5% chance of incorrectly finding significance. How does that add up?\n\nprob_all_correct &lt;- 0.95^6\nprob_all_correct\n\n[1] 0.7350919\n\n\nWe have a 26.5% chance that at least one test result will be incorrect!"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#we-need-a-better-method",
    "href": "lectures/L03/Lecture-03b.html#we-need-a-better-method",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "We need a better method",
    "text": "We need a better method\nThe problem of multiple comparisons requires a technique that considers all treatments at once.\n\nANOVA – Analysis of Variance\nBefore deciding which treatments differ, we first consider:\n\nDifferences between the treatment groups – between-treatment effects\nDifferences within each treatment group – within-treatment effects\n\ndue to random environmental fluctuations, genetics, experimental error"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#the-experiment",
    "href": "lectures/L03/Lecture-03b.html#the-experiment",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "The experiment",
    "text": "The experiment\n\n\n\n\n20 chicks\n4 diets\n5 replicates per diet\nResponse: weight gain (g)\n\n\n\n\nchicks_wide &lt;- read.csv(\"data/chicks.csv\")\nchicks_wide\n\n  Diet.1 Diet.2 Diet.3 Diet.4\n1     99     61     42    169\n2     88    112     97    137\n3     76     30     81    169\n4     38     89     95     85\n5     94     63     92    154"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#reshaping-the-data",
    "href": "lectures/L03/Lecture-03b.html#reshaping-the-data",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Reshaping the data",
    "text": "Reshaping the data\n\nchicks &lt;- read.csv(\"data/chicks.csv\") |&gt;\n  pivot_longer(\n    cols = starts_with(\"Diet\"),\n    names_to = \"diet\",\n    values_to = \"weight\"\n  ) |&gt;\n  mutate(diet = as.factor(diet))\n\nchicks\n\n# A tibble: 20 × 2\n   diet   weight\n   &lt;fct&gt;   &lt;int&gt;\n 1 Diet.1     99\n 2 Diet.2     61\n 3 Diet.3     42\n 4 Diet.4    169\n 5 Diet.1     88\n 6 Diet.2    112\n 7 Diet.3     97\n 8 Diet.4    137\n 9 Diet.1     76\n10 Diet.2     30\n11 Diet.3     81\n12 Diet.4    169\n13 Diet.1     38\n14 Diet.2     89\n15 Diet.3     95\n16 Diet.4     85\n17 Diet.1     94\n18 Diet.2     63\n19 Diet.3     92\n20 Diet.4    154"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#terminology",
    "href": "lectures/L03/Lecture-03b.html#terminology",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Terminology",
    "text": "Terminology\n\nFactor (or treatment): the categorical variable of interest (here: diet)\nLevels: the categories within a factor (here: Diet 1, Diet 2, Diet 3, Diet 4)\nReplicates: the number of observations per level (here: \\(r = 5\\))\n\\(t\\) = number of treatments; \\(N\\) = total observations = \\(r \\times t = 20\\)\n\n\nThis is a one-way (or one-factor) ANOVA because there is only one factor (diet)."
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#which-model-best-describes-the-data",
    "href": "lectures/L03/Lecture-03b.html#which-model-best-describes-the-data",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Which model best describes the data?",
    "text": "Which model best describes the data?\n\n\nCode\noverall_mean &lt;- mean(chicks$weight)\ngroup_means &lt;- chicks |&gt;\n  group_by(diet) |&gt;\n  summarise(mean_wt = mean(weight))\n\np1 &lt;- ggplot(chicks, aes(diet, weight)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_point(data = group_means, aes(diet, mean_wt),\n    shape = 18, size = 5, colour = \"red\") +\n  labs(title = \"Group means (red)\", x = \"Diet\", y = \"Weight (g)\")\n\np2 &lt;- ggplot(chicks, aes(diet, weight)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_hline(yintercept = overall_mean, colour = \"blue\",\n    linewidth = 1, linetype = \"dashed\") +\n  labs(title = \"Overall mean (blue)\", x = \"Diet\", y = \"Weight (g)\")\n\nlibrary(patchwork)\np1 + p2\n\n\n\nDoes the group means model (left) explain the data better than the overall mean model (right)?"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#same-form-as-the-t-test",
    "href": "lectures/L03/Lecture-03b.html#same-form-as-the-t-test",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Same form as the \\(t\\)-test",
    "text": "Same form as the \\(t\\)-test\n\\[y_{ij} = \\mu_i + \\varepsilon_{ij}\\]\n\n\\(i = 1, 2, \\ldots, t\\) (treatment); \\(j = 1, 2, \\ldots, n_i\\) (replicate)\n\nIn the chick example:\n\n\\(y_{ij}\\) = observed weight gain for the \\(j\\)th chick on Diet \\(i\\)\n\\(\\mu_i\\) = mean weight gain for chicks on Diet \\(i\\)\n\\(\\varepsilon_{ij}\\) = random error (residual)"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#checking-normality",
    "href": "lectures/L03/Lecture-03b.html#checking-normality",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Checking normality",
    "text": "Checking normality\n\n\n\nggplot(chicks, aes(diet, weight)) +\n  geom_boxplot() +\n  labs(x = \"Diet\", y = \"Weight (g)\")\n\n\n\n\n\n\n\n\n\n\nshapiro.test(chicks$weight)\n\n\n    Shapiro-Wilk normality test\n\ndata:  chicks$weight\nW = 0.93272, p-value = 0.1742\n\n\n\\(p &gt; 0.05\\): we can assume normality."
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#checking-equal-variances",
    "href": "lectures/L03/Lecture-03b.html#checking-equal-variances",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Checking equal variances",
    "text": "Checking equal variances\n\n\\(\\sigma_1^2 = \\sigma_2^2 = \\ldots = \\sigma_t^2\\)\nGeneral guide: \\(\\frac{\\text{largest SD}}{\\text{smallest SD}} &lt; 2.0\\)\nAlternatively, use Bartlett’s test of homogeneity of variance:\n\n\nbartlett.test(weight ~ diet, data = chicks)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by diet\nBartlett's K-squared = 0.85164, df = 3, p-value = 0.8371\n\n\n\n\n\n\n\n\nWarning\n\n\nBartlett’s test is sensitive to non-normality. Only use it when normality is reasonable."
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#hypotheses",
    "href": "lectures/L03/Lecture-03b.html#hypotheses",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nNull hypothesis: \\(H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_t\\)\nAlternative hypothesis: \\(H_1\\): not all \\(\\mu_i\\) are equal\n\n\n\n\n\n\n\nImportant\n\n\nANOVA only tells us that at least two group means are different – not which ones."
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#the-concept-partition-variability",
    "href": "lectures/L03/Lecture-03b.html#the-concept-partition-variability",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "The concept: partition variability",
    "text": "The concept: partition variability\nPartition the total variability into components:\n\\[SS_{\\text{total}} = SS_{\\text{treatment}} + SS_{\\text{residual}}\\]\n\n\\(SS_{\\text{treatment}}\\): variation between groups (due to diet)\n\\(SS_{\\text{residual}}\\): variation within groups (random error)\n\n\nIf the treatment has a real effect, \\(SS_{\\text{treatment}}\\) should be large relative to \\(SS_{\\text{residual}}\\)."
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#total-sum-of-squares-ss_texttotal",
    "href": "lectures/L03/Lecture-03b.html#total-sum-of-squares-ss_texttotal",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Total sum of squares: \\(SS_{\\text{total}}\\)",
    "text": "Total sum of squares: \\(SS_{\\text{total}}\\)\n\\[SS_{\\text{total}} = \\sum(y_{ij} - \\bar{y})^2\\]\n\noverall_mean &lt;- mean(chicks$weight)\nss_total &lt;- sum((chicks$weight - overall_mean)^2)\nss_total\n\n[1] 29678.95"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#treatment-sum-of-squares-ss_texttreatment",
    "href": "lectures/L03/Lecture-03b.html#treatment-sum-of-squares-ss_texttreatment",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Treatment sum of squares: \\(SS_{\\text{treatment}}\\)",
    "text": "Treatment sum of squares: \\(SS_{\\text{treatment}}\\)\n\\[SS_{\\text{treatment}} = \\sum n_i \\times (\\bar{y}_i - \\bar{y})^2\\]\n\ngroup_means &lt;- chicks |&gt;\n  group_by(diet) |&gt;\n  summarise(grp_mean = mean(weight))\n\nss_trt &lt;- sum(5 * (group_means$grp_mean - overall_mean)^2)\nss_trt\n\n[1] 16466.95"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#residual-sum-of-squares-ss_textresidual",
    "href": "lectures/L03/Lecture-03b.html#residual-sum-of-squares-ss_textresidual",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Residual sum of squares: \\(SS_{\\text{residual}}\\)",
    "text": "Residual sum of squares: \\(SS_{\\text{residual}}\\)\n\\[SS_{\\text{residual}} = \\sum(y_{ij} - \\bar{y}_i)^2\\]\n\nchicks_with_means &lt;- chicks |&gt;\n  left_join(group_means, by = \"diet\")\n\nss_res &lt;- sum((chicks_with_means$weight - chicks_with_means$grp_mean)^2)\nss_res\n\n[1] 13212\n\n\n\nCheck: \\(SS_{\\text{total}} = SS_{\\text{treatment}} + SS_{\\text{residual}}\\)\n\nc(ss_total, ss_trt + ss_res)\n\n[1] 29678.95 29678.95"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#structure",
    "href": "lectures/L03/Lecture-03b.html#structure",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Structure",
    "text": "Structure\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\nF\n\n\n\n\nTreatment\n\\(t - 1\\)\n\\(SS_{\\text{trt}}\\)\n\\(SS_{\\text{trt}} / (t-1)\\)\n\\(MS_{\\text{trt}} / MS_{\\text{res}}\\)\n\n\nResidual\n\\(N - t\\)\n\\(SS_{\\text{res}}\\)\n\\(SS_{\\text{res}} / (N-t)\\)\n\n\n\nTotal\n\\(N - 1\\)\n\\(SS_{\\text{total}}\\)\n\n\n\n\n\n\n\\(N\\) = total observations, \\(t\\) = number of treatment levels\nMean Squares (MS) standardise SS so they are comparable\nThe larger the ratio \\(MS_{\\text{trt}} / MS_{\\text{res}}\\), the stronger the treatment effect"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#using-aov-in-r",
    "href": "lectures/L03/Lecture-03b.html#using-aov-in-r",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Using aov() in R",
    "text": "Using aov() in R\n\nmodel &lt;- aov(weight ~ diet, data = chicks)\nsummary(model)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)   \ndiet         3  16467    5489   6.647  0.004 **\nResiduals   16  13212     826                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#interpreting-the-results",
    "href": "lectures/L03/Lecture-03b.html#interpreting-the-results",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Interpreting the results",
    "text": "Interpreting the results\n\n\\(F = 6.65\\) with \\(df = 3, 16\\)\n\\(p = 0.004\\) which is \\(&lt; 0.05\\), so we reject \\(H_0\\)\nThere are significant differences in mean weight gain amongst the 4 diets\n\n\nHow much variability is explained?\n\\[\\frac{SS_{\\text{treatment}}}{SS_{\\text{total}}} = \\frac{1.6467\\times 10^{4}}{2.9679\\times 10^{4}} = 55.5\\%\\]\nThe diets explain about 55% of the total variability in chick weight gain."
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#confidence-intervals-for-group-means",
    "href": "lectures/L03/Lecture-03b.html#confidence-intervals-for-group-means",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Confidence intervals for group means",
    "text": "Confidence intervals for group means\nWe can examine 95% CIs for each treatment mean:\n\\[95\\% \\text{ CI} = \\bar{y}_i \\pm t^{0.025}_{N-t} \\times SE(\\bar{y}_i)\\]\nwhere \\(SE(\\bar{y}_i) = \\sqrt{MS_{\\text{res}} / n_i}\\)"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#using-emmeans",
    "href": "lectures/L03/Lecture-03b.html#using-emmeans",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Using emmeans",
    "text": "Using emmeans\n\nlibrary(emmeans)\nemm &lt;- emmeans(model, \"diet\")\nemm\n\n diet   emmean   SE df lower.CL upper.CL\n Diet.1   79.0 12.9 16     51.8    106.2\n Diet.2   71.0 12.9 16     43.8     98.2\n Diet.3   81.4 12.9 16     54.2    108.6\n Diet.4  142.8 12.9 16    115.6    170.0\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#visualising-group-comparisons",
    "href": "lectures/L03/Lecture-03b.html#visualising-group-comparisons",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Visualising group comparisons",
    "text": "Visualising group comparisons\n\nplot(emm, comparisons = TRUE) +\n  labs(x = \"Weight (g)\", y = \"Diet\")\n\n\nGroups with overlapping arrows are not significantly different from each other."
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#t-test-vs-anova",
    "href": "lectures/L03/Lecture-03b.html#t-test-vs-anova",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "\\(t\\)-test vs ANOVA",
    "text": "\\(t\\)-test vs ANOVA\n\n\\(t\\)-test: compares means of 2 groups\nANOVA: compares means of 2 or more groups simultaneously\nANOVA avoids the inflated Type I error rate from multiple \\(t\\)-tests\nThe ANOVA \\(F\\)-test tells us if differences exist, but not where\nPost-hoc methods (e.g. emmeans) identify which pairs differ"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#next-week",
    "href": "lectures/L03/Lecture-03b.html#next-week",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Next week",
    "text": "Next week\n\nHow to better identify which pairs are significantly different\nHow to test and interpret model assumptions using residual diagnostics"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#references",
    "href": "lectures/L03/Lecture-03b.html#references",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "References",
    "text": "References\n\nQuinn & Keough (2002), Chapter 7: Section 7.1\nMead et al. (2002), Chapter 18: Sections 18.1–18.3"
  },
  {
    "objectID": "lectures/L03/Lecture-03b.html#thanks",
    "href": "lectures/L03/Lecture-03b.html#thanks",
    "title": "Lecture 03b – One-way ANOVA",
    "section": "Thanks!",
    "text": "Thanks!\nQuestions?\nThis presentation is based on the SOLES Quarto reveal.js template and is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "lectures/L04/index.html",
    "href": "lectures/L04/index.html",
    "title": "Lecture 04",
    "section": "",
    "text": "Lecture 04 – Residual diagnostics & post hoc tests\nDownload PDF file.Download PDF\nDownload Lecture Notes"
  },
  {
    "objectID": "lectures/L06/index.html",
    "href": "lectures/L06/index.html",
    "title": "Lecture 06",
    "section": "",
    "text": "Lecture 06 – ANOVA with blocking\nDownload PDF file.Download PDF\nDownload Lecture Notes"
  },
  {
    "objectID": "lectures/L07/index.html",
    "href": "lectures/L07/index.html",
    "title": "Lecture 07",
    "section": "",
    "text": "Lecture 07 – Regression modelling: Full screen | PDF"
  },
  {
    "objectID": "lectures/L08/index.html",
    "href": "lectures/L08/index.html",
    "title": "Lecture 08",
    "section": "",
    "text": "Lecture 08 – Regression model development Full screen | PDF"
  },
  {
    "objectID": "lectures/L09/index.html",
    "href": "lectures/L09/index.html",
    "title": "Lecture 09",
    "section": "",
    "text": "Lecture 09 – Predictive modelling Full screen | PDF"
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Lab Solutions",
    "section": "",
    "text": "Lab worksheets with answers included. Each lab is available as HTML (for viewing online) or PDF (for download).\n\n\n\n\n\n\n\n\n\n\nWeek\nLab\nHTML\nPDF\n\n\n\n\n1\nIntroduction\nLab 01\n\n\n\n2\nSample Designs\nLab 02\n\n\n\n3\nOne-way ANOVA\nLab 03\n\n\n\n4\nAssumptions & Multiple Comparisons\nLab 04\n\n\n\n5\nExperimental Design\nLab 05\n\n\n\n6\nFactorial Designs\nLab 06\n\n\n\n7\nRegression Model Development\nLab 07\n\n\n\n8\nModel Selection\nLab 08\n\n\n\n9\nPredictive Modelling\nLab 09\n\n\n\n10\nPCA\nLab 10\n\n\n\n11\nClustering\nLab 11\n\n\n\n12\nnMDS and PERMANOVA\nLab 12"
  },
  {
    "objectID": "tutorials/tutorial02.html",
    "href": "tutorials/tutorial02.html",
    "title": "Tutorial 02",
    "section": "",
    "text": "Welcome to Tutorial 2. In this tutorial we will show you how to:\n\nTake a random sample from a finite population; and\ntake a random sample from an infinite population."
  },
  {
    "objectID": "tutorials/tutorial02.html#welcome",
    "href": "tutorials/tutorial02.html#welcome",
    "title": "Tutorial 02",
    "section": "",
    "text": "Welcome to Tutorial 2. In this tutorial we will show you how to:\n\nTake a random sample from a finite population; and\ntake a random sample from an infinite population."
  },
  {
    "objectID": "tutorials/tutorial02.html#getting-started",
    "href": "tutorials/tutorial02.html#getting-started",
    "title": "Tutorial 02",
    "section": "0.1 Getting started",
    "text": "0.1 Getting started\n\n\nIn some instances you will see notes like this one in the margin, which often contain useful tips or reminders.\n\nThese tutorials are designed to be comprehensive yet accessible. I recommend following along and grasping the statistical concepts as much as you can.\nYou will need to create your own Quarto (recommended) or R Markdown document to complete the exercises. In RStudio, go to\n\nFile -&gt; New File -&gt; Quarto Document... or\nFile -&gt; New File -&gt; R Markdown... and follow the intuitive prompts.\n\nFor those of you who are interested in furthering your data science skills and are keen to learn more, I have included optional sections. These techniques are not necessary for you to get through ENVX2001 – we do not cover code in the exams. Nevertheless these skills come handy in report writing and data analysis and can be a great addition to your projects.\nIf you have any suggestions for improvement, please let me know! Send me an email (you can find my details on Canvas)."
  },
  {
    "objectID": "tutorials/tutorial02.html#finite-populations",
    "href": "tutorials/tutorial02.html#finite-populations",
    "title": "Tutorial 02",
    "section": "1.1 Finite populations",
    "text": "1.1 Finite populations\nImagine a hypothetical situation where we know there are 100 farms in a catchment, and we wish to survey 10 of them about their management practices. To ensure unbiased results, the 10 farms should be randomly selected.\nR can do this for us quite easily.\nBelow is the full code to randomly select 10 farms from the 100 in the catchment. We will break it down step by step.\n\nset.seed(1038)\nfarms &lt;- 1:100\nsample(x = farms, size = 10)\n\n\nRandom, but reproducible\n\nset.seed(1038)\n\nThe set.seed() function is used to “fix the seed” of the random number generator. This is useful when you want to ensure that the random numbers generated are the same each time you run the code, which is useful for reproducibility. We don’t want to randomly select 10 farms, and then run the code again and get a different set of 10 farms!\nThe number in the brackets can be any number you choose1.\n1 It is good practice to be as unbiased as possible when picking a number for set.seed(). A good choice is the 4 digits for the current time of the day.\n\nIdentifying the population\n\nfarms &lt;- 1:100\n\nIf there are 100 farms, then we need to be able to select from them. This ensures that each farm is unique so that we can perform random sampling without replacement (i.e. we should not be sampling the same farm twice).\nThe colon operator (:) creates a sequence of numbers from the first number to the second number. For 100 farms we can create a sequence of numbers from 1 to 100 easily using this operator, and assign it to the object farms.\n\n\nUsing : is arguably much easier than typing out all the numbers from 1 to 100.\n\n\nSimple random sampling\n\nsites &lt;- sample(x = farms, size = 10)\n\nThe sample() function is a base R function – part of the core R language – and is used to take a random sample of a specified size from a set of values.\n\n\nNotice that no result is printed to the console when you assign (&lt;-) something to and object like sites. This is because R is not being asked to print anything, and so unless the function itself prints something, nothing will be presented to you. You can print the result by typing sites again in a new line, which instructs R to print the value of sites.\nTo use the sample() function, you need to specify the following arguments:\n\nx: the set of values from which to sample. In this case, the set of farms.\nsize: the number of values to sample. In this case, 10 farms.\n\nThe sample() function returns a vector of the randomly selected farms, which we have assigned to the object sites.\nWe now have our random sample of 10 farms from the 100 in the catchment.\n\nsites\n\n [1] 92 27 78 54 48 44 61 66  4 83\n\n\n\n\nIf you have set the same seed as we have, you should get the same results, which makes your experimental design reproducible."
  },
  {
    "objectID": "tutorials/tutorial02.html#bonus",
    "href": "tutorials/tutorial02.html#bonus",
    "title": "Tutorial 02",
    "section": "1.2 Bonus",
    "text": "1.2 Bonus\nA tidyverse approach to the same problem would be to use the dplyr package to create a sequence of numbers from 1 to 100, and then use the sample_n() function from the dplyr package to take a random sample of 10 farms from the 100 in the catchment.\nThe action can be performed using pipes (|&gt;) to pass the result of one function to the next function, which makes the code easier to read and understand (this is an opinion shared by many).\n\npipe approachNested approachStepped approach\n\n\n\nlibrary(dplyr)\nsites &lt;-\n    tibble(farm = 1:100) |&gt; # create a tibble of 100 farms, and...\n    # ...pass it to the next function:\n    sample_n(10) # take a random sample of 10 farms\n\n\n\n\nlibrary(dplyr)\nsites &lt;- sample_n(tibble(farm = 1:100), 10)\n\n\n\n\nlibrary(dplyr)\n\nfarms &lt;- tibble(farm = 1:100)\nsites &lt;- sample_n(farms, 10)\n\n\n\n\n\n\nPiping is a powerful feature of the tidyverse in R and is common in other programming languages. Here, the |&gt; operator is used to pass the result of the tibble() function to the sample_n() function.\nWithout piping, we would either have to nest the functions, or assign the result of the first function to an object and then pass that object in a stepped approach, which generates redundant “intermediate” objects before the final result is obtained.\nThe tibble() function is used to create a tibble, which is a modern version of a data frame. The sample_n() function is used to take a random sample of a specified size from a tibble.\n\nsites\n\n# A tibble: 10 × 1\n    farm\n   &lt;int&gt;\n 1    14\n 2    41\n 3    96\n 4    73\n 5    25\n 6    88\n 7    40\n 8    32\n 9    39\n10    21\n\n\nUnlike sample(), sample_n() returns a tibble object which shows data in a column rather than row. It may look clutered with a single variable but it is useful for working with data in the tidyverse, especially when you have multiple variables."
  },
  {
    "objectID": "tutorials/tutorial02.html#practice",
    "href": "tutorials/tutorial02.html#practice",
    "title": "Tutorial 02",
    "section": "1.3 Practice",
    "text": "1.3 Practice\nUse this section to practice taking random samples from finite populations. Code is provided, but hidden by default for you to make comparisons with your own. Click on the “Show the code” button to reveal the code.\n\nCreate a random sample of 15 cats from a population of 300 cats.\n\n\nbase Rdplyr approach\n\n\n\n\nShow the code\nset.seed(1202)\ncats &lt;- 1:300\nsubjects &lt;- sample(x = cats, size = 15)\n\n\n\n\n\n\nShow the code\nset.seed(1202)\nsubjects &lt;-\n    tibble(cat = 1:300) |&gt;\n    sample_n(15)\n\n\n\n\n\n\nCreate a random sample of 20 trees from a population of 500 trees.\n\n\nbase Rdplyr approach\n\n\n\n\nShow the code\nset.seed(1204)\ntrees &lt;- 1:500\nsubjects &lt;- sample(x = trees, size = 20)\n\n\n\n\n\n\nShow the code\nset.seed(1204)\nsubjects &lt;-\n    tibble(tree = 1:500) |&gt;\n    sample_n(20)"
  },
  {
    "objectID": "tutorials/tutorial02.html#practice-1",
    "href": "tutorials/tutorial02.html#practice-1",
    "title": "Tutorial 02",
    "section": "2.1 Practice",
    "text": "2.1 Practice\nUse this section to practice taking random samples from infinite populations. Code is provided, but hidden by default for you to make comparisons with your own. Click on the “Show the code” button to reveal the code.\n\nGenerate 20 random points within a 100m x 100m study area.\n\n\n\nShow the code\nset.seed(1242)\nxcoord &lt;- runif(n = 20, min = 0, max = 100)\nycoord &lt;- runif(n = 20, min = 0, max = 100)\nsites &lt;- data.frame(xcoord, ycoord)\n\n\n\nNow this combines sample() and runif() functions. Use runif() to generate 100 data points between 10 and 20, and then use sample() to sample 20 of these points.\n\n\n\nShow the code\nset.seed(1242)\ndata &lt;- runif(n = 100, min = 10, max = 20)\nsampled_data &lt;- sample(data, size = 20)\n\n\n\nUse ggplot2 to plot a histogram of the sampled data generated in the previous step.\n\n\n\nShow the code\nlibrary(ggplot2)\nggplot(\n    data = data.frame(data = sampled_data),\n    aes(x = data)\n) +\n    geom_histogram(\n        binwidth = 0.5,\n        color = \"black\"\n    ) +\n    labs(\n        title = \"Histogram of sampled data\",\n        x = \"Value\",\n        y = \"Frequency\"\n    ) +\n    theme_bw()"
  },
  {
    "objectID": "tutorials/tutorial04.html",
    "href": "tutorials/tutorial04.html",
    "title": "Tutorial 04: ANOVA Assumptions and Post-hoc Tests",
    "section": "",
    "text": "In this week’s lectures we covered how to check the assumptions of ANOVA using residual diagnostics, and how to determine which pairs of treatment means are significantly different using post-hoc tests. A solid understanding of these concepts is essential for conducting robust statistical analyses in R and before we move to more complex models in future weeks."
  },
  {
    "objectID": "tutorials/tutorial04.html#what-are-residuals",
    "href": "tutorials/tutorial04.html#what-are-residuals",
    "title": "Tutorial 04: ANOVA Assumptions and Post-hoc Tests",
    "section": "2.1 What are residuals?",
    "text": "2.1 What are residuals?\n\n\n\n\n\n\nImportantResiduals\n\n\n\nResiduals are the differences between the observed values and the values predicted by our statistical model. In the context of ANOVA, residuals help us assess how well our model fits the data.\n\n\nMathematically, the residual for each observation can be calculated as:\n\n\\text{Residual} = \\text{Observed Value} - \\text{Predicted Value}\n\nResiduals have key advantages over raw data for testing assumptions and you will be required to use the residuals of your model to test the assumptions of ANOVA and regression from now on.\nKey advantages of using residuals:\n\nThey isolate the unexplained variation in the data after accounting for the effects of the independent variables.\nThey allow us to assess the assumptions of the model more directly, as they should ideally be randomly distributed if the model is appropriate.\nThey are the best way to test for assumptions of more complex models where there are multiple factors, which we will introduce in following weeks."
  },
  {
    "objectID": "tutorials/tutorial04.html#exercise-1-why-use-residuals",
    "href": "tutorials/tutorial04.html#exercise-1-why-use-residuals",
    "title": "Tutorial 04: ANOVA Assumptions and Post-hoc Tests",
    "section": "2.2 Exercise 1: Why use residuals?",
    "text": "2.2 Exercise 1: Why use residuals?\n\n\n\n\n\n\nImportant\n\n\n\nThis exercise illustrates why it is not ideal to test the normality assumption using all of the observations irrespective of the treatments or the size of dataset.\n\n\nFirst we will create 2 synthetic datasets which we sample 50 times (n=50) from a normally distributed population. Both underlying populations have the same variation (sd=3) but have a different mean (mean=10, mean=40). We then plot the histograms for each individually, both groups combined and the combined residuals (observation minus group mean).\n\n\nShow the code\nset.seed(123)\ngroup1 &lt;- rnorm(n = 50, mean = 10, sd = 3)\ngroup2 &lt;- rnorm(n = 50, mean = 40, sd = 3)\n\npar(mfrow = c(2, 2))\nhist(group1, main = \"A: Group1\", xlab = \"\")\nhist(group2, main = \"B: Group2\", xlab = \"\")\nhist(c(group1, group2), main = \"C: Group1&2\", xlab = \"\")\nhist(c(group1 - mean(group1), group2 - mean(group2)), main = \"D: Residuals Group1&2\", xlab = \"\")\n\n\n\n\n\n\n\n\n\nShow the code\npar(mfrow = c(1, 1))\n\n\nWe can see that histogram of each group is normally distributed (A, B), however when we combine the data we have 2 distinct groupings centred on the mean of each group (C). Therefore, if we look at the raw data irrespective of the groups we would not see a normally distributed dataset. This is because the effect of individual treatments (or groups) is different so each observation is perturbed according to the treatment it receives or group it is in. If we examine the residuals (D), the treatment (or group) effects have been removed and we can then test if the data is normal or has constant variance. It requires fitting of a model to the data, in this case a 1-way ANOVA model. This is why we test the assumptions on the residuals. You could look at the distribution of each group separately but then for some experiments the replication is small so it is hard to assess normality, using residuals allows all of the observations to be pooled together.\n\n1.1) Testing assumptions using residuals\nNow we will fit a one-way ANOVA model to the data and extract the residuals to test the assumptions of normality and homogeneity of variance.\n\n\nShow the code\n# Create a data frame\ndata &lt;- data.frame(\n  value = c(group1, group2),\n  group = as.factor(rep(c(\"Group1\", \"Group2\"), each = 50))\n)\n# Fit a one-way ANOVA model\nanova_model &lt;- aov(value ~ group, data = data)\n# Extract residuals\nresiduals_anova &lt;- residuals(anova_model)\n# Plot histogram of residuals\nhist(residuals_anova, main = \"Histogram of Residuals\", xlab = \"Residuals\")\n\n\n\n\n\n\n\n\n\nWe can see that the histogram of the residuals appears to be normally distributed.\nLet’s compare this to the histogram of the raw data (also plot C above).\n\n\nShow the code\n# Plot histogram of raw data\nhist(data$value, main = \"Histogram of Raw Data\", xlab = \"Value\")\n\n\n\n\n\n\n\n\n\nThe histogram of the raw data shows two distinct peaks corresponding to the two groups, indicating that the data is not normally distributed when considering all observations together. This highlights the importance of using residuals to assess normality in the context of ANOVA.\nWe can also create a Q-Q plot to further assess normality.\n\n\nShow the code\n# Q-Q plot of residuals\nqqnorm(residuals_anova)\nqqline(residuals_anova, col = \"red\")\n\n\n\n\n\n\n\n\n\nThe Q-Q plot shows that the residuals closely follow the reference line, indicating that they are approximately normally distributed.\n\n\n\n\n\n\nImportantANOVA has several key assumptions:\n\n\n\n\nIndependence: The observations are independent of each other.\nNormality: The residuals of the model are normally distributed.\nHomogeneity of variances: The variances across the different groups are equal."
  },
  {
    "objectID": "tutorials/tutorial04.html#statistical-tests-and-graphical-approaches",
    "href": "tutorials/tutorial04.html#statistical-tests-and-graphical-approaches",
    "title": "Tutorial 04: ANOVA Assumptions and Post-hoc Tests",
    "section": "2.3 Statistical tests and graphical approaches",
    "text": "2.3 Statistical tests and graphical approaches"
  },
  {
    "objectID": "tutorials/tutorial04.html#exercise-2-diatoms-in-streams",
    "href": "tutorials/tutorial04.html#exercise-2-diatoms-in-streams",
    "title": "Tutorial 04: ANOVA Assumptions and Post-hoc Tests",
    "section": "2.4 Exercise 2: Diatoms in streams",
    "text": "2.4 Exercise 2: Diatoms in streams\nHere we will test the assumptions using residual diagnostics and finding significant differences using plots and Tukey’s test. The data is found in the Diatoms worksheet.\n\n\n\nImage credit: B. Caissie, https://diatoms.org/\n\n\nA researcher is interested in the effect of zinc pollution on the diversity of diatom communities in streams. They set up an experiment with four levels of zinc concentration: background (back), low, medium, and high. After a certain period, they measure the diversity of diatom species in each stream.\n\n2.1) Importing and processing data, then fitting an ANOVA model\n\n\nShow the code\nlibrary(readxl)\ndiatoms &lt;- read_excel(\"data/Data4.xlsx\", sheet = \"Diatoms\")\ndiatoms$Zinc &lt;- as.factor(diatoms$Zinc)\nstr(diatoms)\n\n\ntibble [34 × 4] (S3: tbl_df/tbl/data.frame)\n $ Stream   : chr [1:34] \"Eagle\" \"Blue\" \"Blue\" \"Blue\" ...\n $ Zinc     : Factor w/ 4 levels \"BACK\",\"HIGH\",..: 1 1 1 1 1 1 1 1 3 3 ...\n $ Diversity: num [1:34] 2.27 1.7 2.05 1.98 2.2 1.53 0.76 1.89 1.4 2.18 ...\n $ Group    : num [1:34] 1 1 1 1 1 1 1 1 2 2 ...\n\n\nShow the code\nanova.diatoms &lt;- aov(Diversity ~ Zinc, data = diatoms)\nsummary(anova.diatoms)\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nZinc         3  2.567  0.8555   3.939 0.0176 *\nResiduals   30  6.516  0.2172                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n2.2) Testing assumptions\nTo test the model assumptions we encourage you to produce 3 figures:\n\nhistogram of the residuals;\na QQ plot of the residuals;\nplot residuals against fitted values.\n\nIt is good practice to base this on the standardised residuals which can be extracted from a model object using the rstandard function. Standardised residuals are \\sim N(0,1), and make it easier to interpret the plots for outliers. Based on the normal distribution 95% of observations fall within ±2 SD’s of the mean or in the case of standardised residuals ±2.\nHistogram of standardised residuals:\nThe figure below presents the histogram of the standardised residuals. The majority of the observations plot as a bell-shaped (normal) distribution. The exception are 2 observations less than -2. Given there are 34 observations this is about 6% of the dataset so acceptable given we expect 95% observations to be in the interval of ~[-2, 2].\n\n\nShow the code\nhist(rstandard(anova.diatoms))\n\n\n\n\n\n\n\n\n\nQ-Q plot:\nThe QQ plot below shows that the observed quantiles match the theoretical quantiles (assuming normality) based on the observations reasonably following the 1:1 line. We can assume the data is normally distributed.\n\n\nShow the code\nqqnorm(rstandard(anova.diatoms))\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nAn alternative method for testing normality is to use the Shapiro-Wilk test. This test has the null hypothesis that the data is normally distributed. A p-value &gt; 0.05 indicates we fail to reject the null hypothesis and therefore the data is normally distributed.\n\n\nShow the code\nshapiro.test(rstandard(anova.diatoms))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rstandard(anova.diatoms)\nW = 0.96923, p-value = 0.4405\n\n\nThe p-value is &gt; 0.05 so we can assume the data is normally distributed.\n\n\nTesting the assumption of constant variance (homoscedasticity)\nThe plot below shows the standardised residuals plotted against the fitted values (the group means in this case). To test the assumption of constant variance we want to have the same spread of observations for increases in the fitted values. This is the case here. We don’t want to see fanning where the spread of residuals increases or decreases while the fitted values increase.\n\n\nShow the code\nplot(fitted(anova.diatoms), rstandard(anova.diatoms))\n\n\n\n\n\n\n\n\n\nAlternatively we can use the Bartlett test to test for homogeneity of variance using the residuals. The null hypothesis is that the variances are equal across groups. A p-value &gt; 0.05 indicates we fail to reject the null hypothesis and therefore the variances are equal.\n\n\nShow the code\nbartlett.test(rstandard(anova.diatoms) ~ diatoms$Zinc)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  rstandard(anova.diatoms) by diatoms$Zinc\nBartlett's K-squared = 0.25337, df = 3, p-value = 0.9685\n\n\nThe p-value is &gt; 0.05 so we can assume the variances are equal.\nWe can also use the built-in plot() function to produce the diagnostics plots.\n\n\nShow the code\npar(mfrow = c(2, 2))\nplot(anova.diatoms)\n\n\n\n\n\n\n\n\n\nShow the code\npar(mfrow = c(1, 1))\n\n\nor:\n\n\nShow the code\npar(mfrow = c(1, 2))\nplot(anova.diatoms, which = 1) # Residuals vs fitted values\nplot(anova.diatoms, which = 2) # Normal Q-Q plot\n\n\n\n\n\n\n\n\n\nShow the code\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "tutorials/tutorial04.html#exercise-3-family-wise-error-rate",
    "href": "tutorials/tutorial04.html#exercise-3-family-wise-error-rate",
    "title": "Tutorial 04: ANOVA Assumptions and Post-hoc Tests",
    "section": "4.1 Exercise 3: Family-wise error rate",
    "text": "4.1 Exercise 3: Family-wise error rate\nWhen conducting multiple pairwise comparisons, the family-wise error rate increases. To control the family-wise error rate, post-hoc tests like Tukey’s HSD are used, which adjust the significance levels to account for the number of comparisons being made.\n\n\n\n\n\n\nImportantFamily-wise error rate\n\n\n\nThe family-wise error rate is the probability of making at least one Type I error (false positive) among all the comparisons.\n\n\nTukey’s HSD test is designed to maintain the overall family-wise error rate at a specified level (commonly 0.05) while still providing sufficient power to detect true differences between group means.\nHowever, there is a trade-off between controlling the family-wise error rate and maintaining statistical power. As the number of comparisons increases, the adjustments made by post-hoc tests can lead to a reduction in power, making it more difficult to detect true differences.\nTherefore, it is important to carefully consider the number of comparisons being made and choose appropriate post-hoc tests that balance the need for controlling Type I errors with the desire to maintain adequate statistical power.\nLet’s look at an example of the trade-off between family-wise error rate and power using simulation.\n\nset.seed(123)\nn_groups &lt;- 5\nn_per_group &lt;- 10\nn_simulations &lt;- 1000\nalpha &lt;- 0.05\nfamily_wise_errors &lt;- 0\n\nfor (i in 1:n_simulations) {\n  data &lt;- data.frame(\n    value = rnorm(n_groups * n_per_group, mean = 0, sd = 1),\n    group = as.factor(rep(1:n_groups, each = n_per_group))\n  )\n  \n  anova_model &lt;- aov(value ~ group, data = data)\n  tukey_result &lt;- TukeyHSD(anova_model)\n  \n  if (any(tukey_result$group[, \"p adj\"] &lt; alpha)) {\n    family_wise_errors &lt;- family_wise_errors + 1\n  }\n}\nfamily_wise_error_rate &lt;- family_wise_errors / n_simulations\nfamily_wise_error_rate\n\n[1] 0.044\n\n\nIn this simulation, we create 5 groups with 10 observations each, and we run 1000 simulations. We fit a one-way ANOVA model to the data and perform Tukey’s HSD test for pairwise comparisons. We count how many times at least one comparison is significant (p-value &lt; 0.05) across all simulations to estimate the family-wise error rate (~0.04 or 4% error rate).\nThe resulting family-wise error rate should be close to the nominal level of 0.05, demonstrating that Tukey’s HSD test effectively controls the family-wise error rate while maintaining reasonable power to detect true differences between group means.\nIf we don’t use a post-hoc test and just do multiple t-tests we can see how the family-wise error rate increases. For example let’s not adjust the p-values for multiple comparisons:\n\nfamily_wise_errors_no_adjust &lt;- 0\nfor (i in 1:n_simulations) {\n  data &lt;- data.frame(\n    value = rnorm(n_groups * n_per_group, mean = 0, sd = 1),\n    group = as.factor(rep(1:n_groups, each = n_per_group))\n  )\n  \n  p_values &lt;- c()\n  for (j in 1:(n_groups - 1)) {\n    for (k in (j + 1):n_groups) {\n      t_test_result &lt;- t.test(value ~ group, data = subset(data, group %in% c(j, k)))\n      p_values &lt;- c(p_values, t_test_result$p.value)\n    }\n  }\n  \n  if (any(p_values &lt; alpha)) {\n    family_wise_errors_no_adjust &lt;- family_wise_errors_no_adjust + 1\n  }\n}\nfamily_wise_error_rate_no_adjust &lt;- family_wise_errors_no_adjust / n_simulations\nfamily_wise_error_rate_no_adjust\n\n[1] 0.286\n\n\nIn this simulation, we perform multiple t-tests without adjusting the p-values for multiple comparisons. We count how many times at least one comparison is significant (p-value &lt; 0.05) across all simulations to estimate the family-wise error rate.\nThe resulting family-wise error rate should be significantly higher than the nominal level of 0.05 (i.e. ~0.286 or 29% error rate!), demonstrating that not adjusting for multiple comparisons leads to an increased risk of Type I errors.\nThis example highlights the importance of using post-hoc tests like Tukey’s HSD to control the family-wise error rate when conducting multiple pairwise comparisons in ANOVA.\n\n\n\n\n\n\nTipVisual methods for multiple comparisons\n\n\n\nStatistical tests for multiple comparisons can sometimes be difficult to interpret, especially when there are many groups involved. Hence many propose graphical methods as a more powerful way to determine which groups are different, as we saw in the plot above."
  },
  {
    "objectID": "tutorials/tutorial04.html#power-trade-off",
    "href": "tutorials/tutorial04.html#power-trade-off",
    "title": "Tutorial 04: ANOVA Assumptions and Post-hoc Tests",
    "section": "4.2 Power trade-off",
    "text": "4.2 Power trade-off\nWhen conducting multiple comparisons, there is a trade-off between controlling the family-wise error rate and maintaining statistical power. As the number of comparisons increases, the adjustments made by post-hoc tests can lead to a reduction in power, making it more difficult to detect true differences.\n\n\n\n\n\n\nImportantStatistical power\n\n\n\nStatistical power is the probability of correctly rejecting the null hypothesis when it is false (i.e., detecting a true effect).\n\n\nTo illustrate this trade-off, we can simulate data with a known effect size and compare the power of Tukey’s HSD test to that of unadjusted t-tests.\n\nset.seed(123)\nn_groups &lt;- 5\nn_per_group &lt;- 10\nn_simulations &lt;- 1000\nalpha &lt;- 0.05\ntrue_effect_size &lt;- 1\npower_tukey &lt;- 0\npower_t_test &lt;- 0\nfor (i in 1:n_simulations) {\n  data &lt;- data.frame(\n    value = c(rnorm(n_per_group, mean = 0, sd = 1),\n              rnorm(n_per_group, mean = true_effect_size, sd = 1),\n              rnorm(n_per_group, mean = 0, sd = 1),\n              rnorm(n_per_group, mean = 0, sd = 1),\n              rnorm(n_per_group, mean = 0, sd = 1)),\n    group = as.factor(rep(1:n_groups, each = n_per_group))\n  )\n  \n  anova_model &lt;- aov(value ~ group, data = data)\n  tukey_result &lt;- TukeyHSD(anova_model)\n  \n  if (tukey_result$group[\"2-1\", \"p adj\"] &lt; alpha) {\n    power_tukey &lt;- power_tukey + 1\n  }\n  \n  t_test_result &lt;- t.test(value ~ group, data = subset(data, group %in% c(1, 2)))\n  if (t_test_result$p.value &lt; alpha) {\n    power_t_test &lt;- power_t_test + 1\n  }\n}\npower_tukey_rate &lt;- power_tukey / n_simulations\npower_t_test_rate &lt;- power_t_test / n_simulations\n\n\n# print results:\ncat(\"Adjusted for family-rate errors using Tukey's test. Proportion correctly detected:\", power_tukey_rate, \"\\n\")\n\nAdjusted for family-rate errors using Tukey's test. Proportion correctly detected: 0.299 \n\ncat(\"Unadjusted t-tests. Proportion correctly detected:\", power_t_test_rate, \"\\n\")\n\nUnadjusted t-tests. Proportion correctly detected: 0.568 \n\n\nIn this simulation, we create 5 groups with 10 observations each, where one group has a true effect size of 1. We run 1000 simulations and fit a one-way ANOVA model to the data. We then perform Tukey’s HSD test and unadjusted t-tests to compare the power of each method in detecting the true effect.\nThe resulting power rates indicate the proportion of simulations in which each method correctly detected the true effect. We expect the power of the unadjusted t-tests to be higher (~57%) than that of Tukey’s HSD test (~30%), demonstrating the trade-off between controlling the family-wise error rate and maintaining statistical power."
  },
  {
    "objectID": "tutorials/tutorial04.html#example-exam-questions",
    "href": "tutorials/tutorial04.html#example-exam-questions",
    "title": "Tutorial 04: ANOVA Assumptions and Post-hoc Tests",
    "section": "5.1 Example exam questions",
    "text": "5.1 Example exam questions\n\nExplain why residuals are used to assess the assumptions of ANOVA instead of the raw data. Provide an example to illustrate your explanation.\nDescribe the assumptions of ANOVA and outline the graphical and statistical methods used to test these assumptions using residuals.\nA researcher conducts a one-way ANOVA and finds a significant effect of treatment on a response variable. Explain when and why post-hoc tests are necessary, and describe how to interpret the results of Tukey’s Honest Significant Difference (HSD) test.\nDiscuss the trade-off between controlling the family-wise error rate and maintaining statistical power when conducting multiple comparisons. Provide an example to illustrate this trade-off."
  },
  {
    "objectID": "tutorials/tutorial06.html",
    "href": "tutorials/tutorial06.html",
    "title": "Tutorial 06: Factorial Designs",
    "section": "",
    "text": "In this week’s lectures we covered factorial designs, main effects and interactions. We applied these concepts by using analysis of variance (ANOVA) as a method to analyse data from these experimental designs."
  },
  {
    "objectID": "tutorials/tutorial06.html#main-effects-and-interactions",
    "href": "tutorials/tutorial06.html#main-effects-and-interactions",
    "title": "Tutorial 06: Factorial Designs",
    "section": "2.1 Main Effects and Interactions",
    "text": "2.1 Main Effects and Interactions\nIn a factorial design, the main effects refer to the individual effects of each treatment/factor on the response variable, while interactions refer to the combined effects of two or more treatments/factors on the response variable. An interaction occurs when the effect of one treatment/factor depends on the level of another treatment/factor.\nLet’s plot some examples of interaction plots:\n\nNo interaction\nPositive interaction\nNegative interaction\n\n\nlibrary(ggplot2)\n\n# No interaction\nno_interaction &lt;- data.frame(\n  FactorA = rep(c(\"A1\", \"A2\"), each = 3),\n  FactorB = rep(c(\"B1\", \"B2\", \"B3\"), times = 2),\n  Response = c(5, 10, 15, 7, 12, 17)\n)\n\nggplot(no_interaction, aes(x = FactorB, y = Response, color = FactorA, group = FactorA)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"No Interaction\")\n\n\n\n\n\n\n\n# Positive interaction\npositive_interaction &lt;- data.frame(\n  FactorA = rep(c(\"A1\", \"A2\"), each = 3),\n  FactorB = rep(c(\"B1\", \"B2\", \"B3\"), times = 2),\n  Response = c(5, 10, 15, 10, 20, 30)\n)\n\nggplot(positive_interaction, aes(x = FactorB, y = Response, color = FactorA, group = FactorA)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Positive Interaction\")\n\n\n\n\n\n\n\n# Negative interaction\nnegative_interaction &lt;- data.frame(\n  FactorA = rep(c(\"A1\", \"A2\"), each = 3),\n  FactorB = rep(c(\"B1\", \"B2\", \"B3\"), times = 2),\n  Response = c(15, 10, 5, 5, 10, 15)\n)\n\nggplot(negative_interaction, aes(x = FactorB, y = Response, color = FactorA, group = FactorA)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Negative Interaction\")\n\n\n\n\n\n\n\n\nThe plots above illustrate different types of interactions between two factors:\n\nIn the “No Interaction” plot, the lines are parallel, indicating that the effect of one factor does not depend on the level of the other factor.\nIn the “Positive Interaction” plot, the lines diverge, indicating that the effect of one factor increases with the level of the other factor.\nIn the “Negative Interaction” plot, the lines converge, indicating that the effect of one factor decreases with the level of the other factor.\n\n\n\n\n\n\n\nTipInterpreting Interaction Plots\n\n\n\n\nParallel lines indicate no interaction between factors.\nNon-parallel lines indicate an interaction between factors."
  },
  {
    "objectID": "tutorials/tutorial06.html#exercise-1-identifying-main-effects-and-interactions",
    "href": "tutorials/tutorial06.html#exercise-1-identifying-main-effects-and-interactions",
    "title": "Tutorial 06: Factorial Designs",
    "section": "2.2 Exercise 1: Identifying Main Effects and Interactions",
    "text": "2.2 Exercise 1: Identifying Main Effects and Interactions\nIn this ecological experiment, we investigated the influence of two factors: Fire and Rainfall on small mammal captures (using a 2-way factorial design).\nOur main effects are:\n\nEffect of Fire (Levels: Burnt, Unburnt)\nEffect of Rainfall (Levels: Low, Medium, High)\n\nOur interaction is:\n\nInteraction between Fire and Rainfall\n\nLet’s simulate some data and plot the main effects and interactions:\n\nset.seed(123)\nfire &lt;- rep(c(\"Burnt\", \"Unburnt\"), each = 30)\nrainfall &lt;- rep(c(\"Low\", \"Medium\", \"High\"), times = 20)\ncaptures &lt;- rnorm(60, mean = ifelse(fire == \"Burnt\",\n                                    ifelse(rainfall == \"Low\", 5,\n                                           ifelse(rainfall == \"Medium\", 10, 15)),\n                                    ifelse(rainfall == \"Low\", 10,\n                                           ifelse(rainfall == \"Medium\", 15, 20))), sd = 3)\n\ndata &lt;- data.frame(Fire = fire, Rainfall = rainfall, Captures = captures)\n\n# Add level order for plotting\ndata$Rainfall &lt;- factor(data$Rainfall, levels = c(\"Low\", \"Medium\", \"High\"))\n\nlibrary(ggplot2)\n\n# Plot main effect: Rainfall\nggplot(data, aes(x = Rainfall, y = Captures)) +\n  stat_summary(fun = mean, geom = \"bar\", aes(group = 1)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Main Effect of Rainfall on Small Mammal Captures\")\n\n\n\n\n\n\n\n# Plot main effect: Fire\nggplot(data, aes(x = Fire, y = Captures)) +\n  stat_summary(fun = mean, geom = \"bar\", aes(group = 1)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Main Effect of Fire on Small Mammal Captures\")\n\n\n\n\n\n\n\n# Plot interaction\nggplot(data, aes(x = Rainfall, y = Captures, color = Fire, group = Fire)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Interaction between Fire and Rainfall on Small Mammal Captures\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKey concepts\n\n\n\n\nMain effects represent the individual impact of each factor on the response variable.\nInteractions occur when the effect of one factor depends on the level of another factor.\nIf an interaction is significant, do not interpret the main effects on their own.\nRead a factorial ANOVA table from the bottom up: interaction terms first, then main effects.\n\nYou must understand these concepts to correctly interpret the results of factorial experiments."
  },
  {
    "objectID": "tutorials/tutorial06.html#exercise-2-anova-using-a-factorial-treatment-design",
    "href": "tutorials/tutorial06.html#exercise-2-anova-using-a-factorial-treatment-design",
    "title": "Tutorial 06: Factorial Designs",
    "section": "2.3 Exercise 2: ANOVA using a Factorial Treatment Design",
    "text": "2.3 Exercise 2: ANOVA using a Factorial Treatment Design\nNow that we have visualised the main effects and interaction, we can fit an ANOVA model to test for the significance of these effects.\n\n# Fit ANOVA model\ndata$Fire &lt;- as.factor(data$Fire)\ndata$Rainfall &lt;- as.factor(data$Rainfall) \n\nanova_model &lt;- aov(Captures ~ Fire * Rainfall, data = data)\n\n# Summary of ANOVA model\nsummary(anova_model)\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nFire           1  483.3   483.3  63.117 1.21e-10 ***\nRainfall       2 1030.1   515.0  67.259 2.19e-15 ***\nFire:Rainfall  2   16.5     8.3   1.078    0.347    \nResiduals     54  413.5     7.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWhich effects are significant? Are they main effects and/or interactions? Are they consistent with the plots above?\n\n\n# Add your answers here and check with teaching staff"
  },
  {
    "objectID": "tutorials/tutorial06.html#exercise-3-factorial-design-with-blocking",
    "href": "tutorials/tutorial06.html#exercise-3-factorial-design-with-blocking",
    "title": "Tutorial 06: Factorial Designs",
    "section": "3.1 Exercise 3: Factorial Design with Blocking",
    "text": "3.1 Exercise 3: Factorial Design with Blocking\n\n3.1) ANOVA with Blocking\nIn this exercise, we will simulate data for a factorial design with blocking. We will investigate the effects of two factors: Fertilizer (Levels: A, B) and Irrigation (Levels: Low, High) on crop yield, while blocking by Field (Levels: 1, 2, 3).\n\nset.seed(456)\nfield &lt;- rep(c(\"Field1\", \"Field2\", \"Field3\"), each = 20)\nfertilizer &lt;- rep(c(\"A\", \"B\"), times = 30)\nirrigation &lt;- rep(c(\"Low\", \"High\"), each = 10, times = 3)\nyield &lt;- rnorm(60, mean = ifelse(fertilizer == \"A\",\n                                 ifelse(irrigation == \"Low\", 30, 40),\n                                 ifelse(irrigation == \"Low\", 35, 45)) +\n                  ifelse(field == \"Field1\", 5,\n                         ifelse(field == \"Field2\", 0, -5)), sd = 4)\n\ndata_blocked &lt;- data.frame(Field = field, Fertilizer = fertilizer,\n                           Irrigation = irrigation, Yield = yield)\n\n# Add level order for plotting\ndata_blocked$Irrigation &lt;- factor(data_blocked$Irrigation, levels = c(\"Low\", \"High\"))\ndata_blocked$Field &lt;- as.factor(data_blocked$Field)\ndata_blocked$Fertilizer &lt;- as.factor(data_blocked$Fertilizer)\ndata_blocked$Irrigation &lt;- as.factor(data_blocked$Irrigation)\n\n# Fit ANOVA model with blocking\nanova_blocked_model &lt;- aov(Yield ~ Field + Fertilizer * Irrigation, data = data_blocked)\n\n# Summary of ANOVA model\nsummary(anova_blocked_model)\n\n                      Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nField                  2 1208.9   604.4  43.669 5.22e-12 ***\nFertilizer             1  631.5   631.5  45.625 1.03e-08 ***\nIrrigation             1 2003.0  2003.0 144.711  &lt; 2e-16 ***\nFertilizer:Irrigation  1   40.1    40.1   2.899   0.0944 .  \nResiduals             54  747.4    13.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWhich effects are significant? Are they main effects, interactions, or blocking effects?\n\n\n# Add your answers here and check with teaching staff\n\n\n\n3.2) Visualising Factorial Design with Blocking\nNow let’s visualise the effects in this blocked factorial design using the emmeans package and its plotting functions.\n\nlibrary(emmeans)\n\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\n\n# Calculate estimated marginal means\nemm &lt;- emmeans(anova_blocked_model, ~ Fertilizer * Irrigation)\n\n# Main effect plots using emmeans\nplot(emmeans(anova_blocked_model, ~ Fertilizer), \n     main = \"Main Effect of Fertilizer on Crop Yield\", comparisons = TRUE)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n\n\n\n\n\n\nplot(emmeans(anova_blocked_model, ~ Irrigation), \n     main = \"Main Effect of Irrigation on Crop Yield\", comparisons = TRUE)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n\n\n\n\n\n\n# Blocking effect plot using emmeans\nplot(emmeans(anova_blocked_model, ~ Field), \n     main = \"Blocking Effect of Field on Crop Yield\", comparisons = TRUE)\n\n\n\n\n\n\n\n# Interaction plot using emmeans\nemmip(anova_blocked_model, Fertilizer ~ Irrigation,\n      main = \"Interaction between Fertilizer and Irrigation on Crop Yield\", CIs = TRUE)\n\n\n\n\n\n\n\n\n\nDescribe the main effects and interaction effects observed in the plots above. How do these visualisations help in understanding the effects of Fertilizer and Irrigation on crop yield?\n\n\n# Add your answers here and check with teaching staff\n\nNote: We usually don’t plot blocking effects as they are not of primary interest, but it can be useful to check for large differences between blocks.\nDepending on our hypotheses, we may be interested in post-hoc tests for the main effects or interaction effects. If there is a significant interaction, we should focus on that rather than the main effects. We may not need to use all the plots. Always let your hypotheses guide what plots to produce.\n\n\n3.3) Calculate the variance explained by each effect\n\n# Calculate variance explained by each effect\nanova_summary &lt;- summary(anova_blocked_model)\nss_total &lt;- sum(anova_summary[[1]][, \"Sum Sq\"])\nss_effects &lt;- anova_summary[[1]][, \"Sum Sq\"]\n\nvariance_explained &lt;- (ss_effects / ss_total) * 100\n\nanova_summary # ANOVA table\n\n                      Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nField                  2 1208.9   604.4  43.669 5.22e-12 ***\nFertilizer             1  631.5   631.5  45.625 1.03e-08 ***\nIrrigation             1 2003.0  2003.0 144.711  &lt; 2e-16 ***\nFertilizer:Irrigation  1   40.1    40.1   2.899   0.0944 .  \nResiduals             54  747.4    13.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nvariance_explained # variation explained by each effect in order of the ANOVA table from top to bottom\n\n[1] 26.1042461 13.6367542 43.2524207  0.8665808 16.1399983\n\n\n\nWhich effect explains the most variance in crop yield? How does this information help in understanding the factors influencing crop yield?\n\n\n# Add your answers here and check with teaching staff"
  },
  {
    "objectID": "tutorials/tutorial06.html#example-exam-questions",
    "href": "tutorials/tutorial06.html#example-exam-questions",
    "title": "Tutorial 06: Factorial Designs",
    "section": "4.1 Example exam questions",
    "text": "4.1 Example exam questions\n\nIn a 2-way factorial design with factors A (2 levels) and B (3 levels), describe how you would identify and interpret main effects and interactions using ANOVA. Provide an example of how to visualise these effects.\nExplain the difference between a treatment design and an experimental design. Provide examples of each and discuss how they can be combined in a factorial design with blocking.\nIf you conducted a factorial experiment with blocking and found a significant interaction between two factors, how would you interpret this result? Would you still consider the main effects of each factor? Justify your answer with reference to ANOVA principles."
  },
  {
    "objectID": "tutorials/tutorial08.html",
    "href": "tutorials/tutorial08.html",
    "title": "Tutorial 08",
    "section": "",
    "text": "Data: Dippers spreadsheet\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks. The dataset in this exercise contains data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\nTwenty-two sites were included in the survey. Some of the variables have been transformed.\nThe variables measured were:\n\nAltitude site altitude\nHardness water hardness\nRiverSlope river-bed slope\nLogCadd the numbers of caddis fly larvae, transformed\nLogStone the numbers of stonefly larvae, transformed\nLogMay the numbers of mayfly larvae, transformed\nLogOther the numbers of all other invertebrates collected, transformed\nBr_Dens the number of breeding pairs of dippers per 10 km of river\n\nIn the analyses, the four invertebrate variables were transformed using a log(x+1) transformation.\n\nlibrary(readxl)\nDippers &lt;- read_xlsx(\"data/mlr.xlsx\" , sheet = \"Dippers\")\nglimpse(Dippers)\n\nRows: 22\nColumns: 8\n$ Altitude   &lt;dbl&gt; 259, 198, 251, 184, 145, 145, 198, 160, 251, 159, 160, 145,…\n$ Hardness   &lt;dbl&gt; 12.20, 22.00, 26.30, 22.50, 29.50, 39.90, 42.80, 59.60, 69.…\n$ RiverSlope &lt;dbl&gt; 10.90, 14.70, 6.90, 4.60, 1.91, 5.00, 6.20, 14.30, 4.60, 3.…\n$ Br_Dens    &lt;dbl&gt; 3.60, 4.30, 3.80, 3.40, 3.80, 4.50, 4.30, 5.00, 4.50, 3.40,…\n$ LogCadd    &lt;dbl&gt; 2.303, 2.890, 3.784, 4.419, 3.219, 3.932, 3.664, 4.431, 3.7…\n$ LogStone   &lt;dbl&gt; 5.242, 4.344, 5.231, 5.242, 3.829, 4.898, 4.357, 6.337, 5.4…\n$ LogMay     &lt;dbl&gt; 0.000, 3.401, 5.826, 5.749, 5.509, 5.749, 5.371, 0.000, 4.8…\n$ LogOther   &lt;dbl&gt; 1.386, 1.609, 1.386, 1.386, 1.099, 3.045, 1.386, 2.944, 2.5…\n\n\nYou may explore the data on your own (hint: look at last week’s exercise on histogram and scatterplot matrices).\nWhen ready, perform a backward elimination starting from the full model:\n\nFullMod &lt;- lm(Br_Dens ~ ., data=Dippers)\nRedMod &lt;- step(FullMod, direction = \"backward\")\nsummary(FullMod)\nsummary(RedMod)\nAIC(FullMod, RedMod)\n\n\n\nWhich model is chosen? Why?"
  },
  {
    "objectID": "tutorials/tutorial08.html#exercise-1-backward-elimination-in-r",
    "href": "tutorials/tutorial08.html#exercise-1-backward-elimination-in-r",
    "title": "Tutorial 08",
    "section": "",
    "text": "Data: Dippers spreadsheet\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks. The dataset in this exercise contains data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\nTwenty-two sites were included in the survey. Some of the variables have been transformed.\nThe variables measured were:\n\nAltitude site altitude\nHardness water hardness\nRiverSlope river-bed slope\nLogCadd the numbers of caddis fly larvae, transformed\nLogStone the numbers of stonefly larvae, transformed\nLogMay the numbers of mayfly larvae, transformed\nLogOther the numbers of all other invertebrates collected, transformed\nBr_Dens the number of breeding pairs of dippers per 10 km of river\n\nIn the analyses, the four invertebrate variables were transformed using a log(x+1) transformation.\n\nlibrary(readxl)\nDippers &lt;- read_xlsx(\"data/mlr.xlsx\" , sheet = \"Dippers\")\nglimpse(Dippers)\n\nRows: 22\nColumns: 8\n$ Altitude   &lt;dbl&gt; 259, 198, 251, 184, 145, 145, 198, 160, 251, 159, 160, 145,…\n$ Hardness   &lt;dbl&gt; 12.20, 22.00, 26.30, 22.50, 29.50, 39.90, 42.80, 59.60, 69.…\n$ RiverSlope &lt;dbl&gt; 10.90, 14.70, 6.90, 4.60, 1.91, 5.00, 6.20, 14.30, 4.60, 3.…\n$ Br_Dens    &lt;dbl&gt; 3.60, 4.30, 3.80, 3.40, 3.80, 4.50, 4.30, 5.00, 4.50, 3.40,…\n$ LogCadd    &lt;dbl&gt; 2.303, 2.890, 3.784, 4.419, 3.219, 3.932, 3.664, 4.431, 3.7…\n$ LogStone   &lt;dbl&gt; 5.242, 4.344, 5.231, 5.242, 3.829, 4.898, 4.357, 6.337, 5.4…\n$ LogMay     &lt;dbl&gt; 0.000, 3.401, 5.826, 5.749, 5.509, 5.749, 5.371, 0.000, 4.8…\n$ LogOther   &lt;dbl&gt; 1.386, 1.609, 1.386, 1.386, 1.099, 3.045, 1.386, 2.944, 2.5…\n\n\nYou may explore the data on your own (hint: look at last week’s exercise on histogram and scatterplot matrices).\nWhen ready, perform a backward elimination starting from the full model:\n\nFullMod &lt;- lm(Br_Dens ~ ., data=Dippers)\nRedMod &lt;- step(FullMod, direction = \"backward\")\nsummary(FullMod)\nsummary(RedMod)\nAIC(FullMod, RedMod)\n\n\n\nWhich model is chosen? Why?"
  },
  {
    "objectID": "tutorials/tutorial11.html",
    "href": "tutorials/tutorial11.html",
    "title": "Tutorial 11: Clustering",
    "section": "",
    "text": "We are going to analyse the bird assemblage data from the lecture.\n\n\nLoad the vegan package:\n\nlibrary(vegan)\n\n\n\n\nRead the data:\n\nmacnally &lt;- read.csv(\"data/macnally.csv\", row.names = 1)\nhead(macnally, 1)\n\n           HABITAT V1GST V2EYR V3GF V4BTH V5GWH V6WTTR V7WEHE V8WNHE V9SFW\nReedy Lake   Mixed   3.4     0    0     0     0      0      0   11.9   0.4\n           V10WBSW V11CR V12LK V13RWB V14AUR V15STTH V16LR V17WPHE V18YTH V19ER\nReedy Lake       0   1.1   3.8    9.7      0       0   4.8    27.3      0   5.1\n           V20PCU V21ESP V22SCR V23RBFT V24BFCS V25WAG V26WWCH V27NHHE V28VS\nReedy Lake      0      0      0       0     0.6    1.9       0       0     0\n           V29CST V30BTR V31AMAG V32SCC V33RWH V34WSW V35STP V36YFHE V37WHIP\nReedy Lake    1.7   12.5     8.6   12.5    0.6      0    4.8       0       0\n           V38GAL V39FHE V40BRTH V41SPP V42SIL V43GCU V44MUSK V45MGLK V46BHHE\nReedy Lake    4.8   26.2       0      0      0      0    13.1     1.7     1.1\n           V47RFC V48YTBC V49LYRE V50CHE V51OWH V52TRM V53MB V54STHR V55LHE\nReedy Lake      0       0       0      0      0     15     0       0      0\n           V56FTC V57PINK V58OBO V59YR V60LFB V61SPW V62RBTR V63DWS V64BELL\nReedy Lake      0       0      0     0    2.9      0       0    0.4       0\n           V65LWB V66CBW V67GGC V68PIL V69SKF V70RSL V71PDOV V72CRP V73JW\nReedy Lake      0      0      0      0    1.9    6.7       0      0     0\n           V74BCHE V75RCR V76GBB V77RRP V78LLOR V79YTHE V80RF V81SHBC V82AZKF\nReedy Lake       0      0      0    4.8       0       0     0       0       0\n           V83SFC V84YRTH V85ROSE V86BCOO V87LFC V88WG V89PCOO V90WTG V91NMIN\nReedy Lake      0       0       0       0      0     0     1.9      0     0.2\n           V92NFB V93DB V94RBEE V95HBC V96DF V97PCL V98FLAME V99WWT V100WBWS\nReedy Lake      0     0       0      0     0    9.1        0      0        0\n           V101LCOR V102KING\nReedy Lake        0        0\n\n\n\n\n\nCalculate the Bray-Curtis dissimilarity:\n\nBraydistance &lt;- vegdist(macnally[, 3:102])\n\n\n\n\nApply hierarchical clustering using the UPGMA method (“average”):\n\nhc &lt;- hclust(Braydistance, method = \"average\")\n\n\n\n\nPlot the dendrogram:\n\nplot(hc)\n\n\n\n\n\n\n\n\nLet’s add some labels, and look at the potential number of clusters:\n\nplot(hc, las = 1, \n     main = \"Cluster diagram of Bird Assemblages\",\n     xlab = \"Site\", \n     ylab = \"Bray-Curtis Dissimilarity\")\nrect.hclust(hc, 2, border = \"red\")\nrect.hclust(hc, 5, border = \"darkgreen\")\n\n\n\n\n\n\n\n\nThe rectangles show potential groupings at different levels of the dendrogram:\n\nRed rectangles: 2 clusters\nGreen rectangles: 5 clusters\n\nThe choice of how many clusters to use depends on your research question and the level of dissimilarity that is meaningful for your study."
  },
  {
    "objectID": "tutorials/tutorial11.html#setup",
    "href": "tutorials/tutorial11.html#setup",
    "title": "Tutorial 11: Clustering",
    "section": "",
    "text": "Load the vegan package:\n\nlibrary(vegan)"
  },
  {
    "objectID": "tutorials/tutorial11.html#load-the-data",
    "href": "tutorials/tutorial11.html#load-the-data",
    "title": "Tutorial 11: Clustering",
    "section": "",
    "text": "Read the data:\n\nmacnally &lt;- read.csv(\"data/macnally.csv\", row.names = 1)\nhead(macnally, 1)\n\n           HABITAT V1GST V2EYR V3GF V4BTH V5GWH V6WTTR V7WEHE V8WNHE V9SFW\nReedy Lake   Mixed   3.4     0    0     0     0      0      0   11.9   0.4\n           V10WBSW V11CR V12LK V13RWB V14AUR V15STTH V16LR V17WPHE V18YTH V19ER\nReedy Lake       0   1.1   3.8    9.7      0       0   4.8    27.3      0   5.1\n           V20PCU V21ESP V22SCR V23RBFT V24BFCS V25WAG V26WWCH V27NHHE V28VS\nReedy Lake      0      0      0       0     0.6    1.9       0       0     0\n           V29CST V30BTR V31AMAG V32SCC V33RWH V34WSW V35STP V36YFHE V37WHIP\nReedy Lake    1.7   12.5     8.6   12.5    0.6      0    4.8       0       0\n           V38GAL V39FHE V40BRTH V41SPP V42SIL V43GCU V44MUSK V45MGLK V46BHHE\nReedy Lake    4.8   26.2       0      0      0      0    13.1     1.7     1.1\n           V47RFC V48YTBC V49LYRE V50CHE V51OWH V52TRM V53MB V54STHR V55LHE\nReedy Lake      0       0       0      0      0     15     0       0      0\n           V56FTC V57PINK V58OBO V59YR V60LFB V61SPW V62RBTR V63DWS V64BELL\nReedy Lake      0       0      0     0    2.9      0       0    0.4       0\n           V65LWB V66CBW V67GGC V68PIL V69SKF V70RSL V71PDOV V72CRP V73JW\nReedy Lake      0      0      0      0    1.9    6.7       0      0     0\n           V74BCHE V75RCR V76GBB V77RRP V78LLOR V79YTHE V80RF V81SHBC V82AZKF\nReedy Lake       0      0      0    4.8       0       0     0       0       0\n           V83SFC V84YRTH V85ROSE V86BCOO V87LFC V88WG V89PCOO V90WTG V91NMIN\nReedy Lake      0       0       0       0      0     0     1.9      0     0.2\n           V92NFB V93DB V94RBEE V95HBC V96DF V97PCL V98FLAME V99WWT V100WBWS\nReedy Lake      0     0       0      0     0    9.1        0      0        0\n           V101LCOR V102KING\nReedy Lake        0        0"
  },
  {
    "objectID": "tutorials/tutorial11.html#calculate-bray-curtis-dissimilarity",
    "href": "tutorials/tutorial11.html#calculate-bray-curtis-dissimilarity",
    "title": "Tutorial 11: Clustering",
    "section": "",
    "text": "Calculate the Bray-Curtis dissimilarity:\n\nBraydistance &lt;- vegdist(macnally[, 3:102])"
  },
  {
    "objectID": "tutorials/tutorial11.html#hierarchical-clustering",
    "href": "tutorials/tutorial11.html#hierarchical-clustering",
    "title": "Tutorial 11: Clustering",
    "section": "",
    "text": "Apply hierarchical clustering using the UPGMA method (“average”):\n\nhc &lt;- hclust(Braydistance, method = \"average\")"
  },
  {
    "objectID": "tutorials/tutorial11.html#plot-the-dendrogram",
    "href": "tutorials/tutorial11.html#plot-the-dendrogram",
    "title": "Tutorial 11: Clustering",
    "section": "",
    "text": "Plot the dendrogram:\n\nplot(hc)\n\n\n\n\n\n\n\n\nLet’s add some labels, and look at the potential number of clusters:\n\nplot(hc, las = 1, \n     main = \"Cluster diagram of Bird Assemblages\",\n     xlab = \"Site\", \n     ylab = \"Bray-Curtis Dissimilarity\")\nrect.hclust(hc, 2, border = \"red\")\nrect.hclust(hc, 5, border = \"darkgreen\")\n\n\n\n\n\n\n\n\nThe rectangles show potential groupings at different levels of the dendrogram:\n\nRed rectangles: 2 clusters\nGreen rectangles: 5 clusters\n\nThe choice of how many clusters to use depends on your research question and the level of dissimilarity that is meaningful for your study."
  }
]